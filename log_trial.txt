Script started on Thu 14 Oct 2021 20:19:57 IST
bash: alias: NF} $1 | sort -nu | tail -n 1 : not found
(base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ source ~/venv/bin/activate
(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ python cv_grid_all_ml.py 2 nahnah /home/ciaran/completed_big_matrix_binary_new_snps_ids_first_10k. raw top FT16cv5k 10006       ^C
(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ vi cv_grid_all_ml.py
[?1049h[?1h=[1;44r[34l[34h[?25h[23m[24m[0m[H[J[?25l[44;1H"cv_grid_all_ml.py" 319L, 18738C[1;1H[34m#Warning : best model selected by NMAE and R2 might not be the same
#performs linear regression, linear regression, neural network, svm and random forest, LASSO, RIDGE, CNN
#source ~/venv/bin/activate #in python 3.5.2
#print a log to a .txt file!
#model = pickle.load(open('FILEPATH', 'rb')) 
#dependencies = {'coeff_determination':coeff_determination}
#model = tf.keras.models.load_model('FILEPATH', custom_objects=dependencies)
#

#import tensorflow
#import numpy as np; import scipy #need to do this before path insert
#import sys
#sys.path.insert(1, '/external_storage/ciaran/Library/Python/3.7/python/site-packages/')
#import dill as pickle
#sys.path.insert(1, '/external_storage/ciaran/Library/Python/3.7/python/site-packages/nested_cv')
#from nested_cv import NestedCV
#with open('NCV_NN.pkl', 'rb') as f:
#     red = pickle.load(f)[0m

[35mimport[0m sys
stdoutOrigin=sys.stdout
sys.stdout = [36mopen[0m([31m"log.txt"[0m, [31m"w"[0m)


[36mprint[0m([31m"Please remember to set the right set size in the nested_cv code"[0m)
[35mimport[0m sys
sys.path.insert([31m1[0m, [31m'/external_storage/ciaran/Library/Python/3.7/python/site-packages/nested_cv'[0m)
num = sys.argv[[31m1[0m] [34m#script number for saving out[0m
spacer = [36mstr[0m(sys.argv[[31m2[0m]) [34m#nothing rn[0m
data = [36mstr[0m(sys.argv[[31m3[0m]) [34m#needs to be same size as set_size[0m
snps = [36mstr[0m(sys.argv[[31m4[0m]) [34m#top or shuf[0m
phenotype = [36mstr[0m(sys.argv[[31m5[0m]) [34m#make a directory for the results[0m
set_size = [36mint[0m(sys.argv[[31m6[0m]) [34m#how many SNPs[0m

[35mfrom[0m nested_cv [35mimport[0m NestedCV
[35mimport[0m statistics
[35mimport[0m numpy [33mas[0m np
[35mimport[0m sklearn
[35mimport[0m seaborn [33mas[0m sns
[35mfrom[0m sklearn [35mimport[0m preprocessing
[35mfrom[0m sklearn.pipeline [35mimport[0m make_pipeline, Pipeline
[35mfrom[0m sklearn.preprocessing [35mimport[0m StandardScaler
[35mimport[0m datetime[44;150H1,1[11CTop[1;1H[34h[?25h[?25l[44;150H2[2;1H[34h[?25h[?25l[44;150H3[3;1H[34h[?25h[?25l[44;150H4[4;1H[34h[?25h[?25l[44;150H5[5;1H[34h[?25h[?25l[44;150H6[6;1H[34h[?25h[?25l[44;150H7[7;1H[34h[?25h[?25l[44;150H8[8;1H[34h[?25h[?25l[44;150H9,0-1[9;1H[34h[?25h[?25l[44;150H10,1 [10;1H[34h[?25h[?25l[44;151H1[11;1H[34h[?25h[?25l[44;151H2[12;1H[34h[?25h[?25l[44;151H3[13;1H[34h[?25h[?25l[44;151H4[14;1H[34h[?25h[?25l[44;151H5[15;1H[34h[?25h[?25l[44;151H6[16;1H[34h[?25h[?25l[44;151H7[17;1H[34h[?25h[?25l[44;151H8[18;1H[34h[?25h[?25l[44;151H9,0-1[19;1H[34h[?25h[?25l[44;150H20,1  [20;1H[34h[?25h[?25l[44;151H1[21;1H[34h[?25h[?25l[21;43r[43;1H
[1;44r[43;1H[35mimport[0m time[44;1H[K[44;150H21,1[10CTop[21;1H[34h[?25h[?25l[21;43r[43;1H
[1;44r[43;1H[35mfrom[0m statistics [35mimport[0m mean[44;150H[K[44;150H21,0-1[8CTop[21;1H[34h[?25h[?25l[21;43r[43;1H
[1;44r[43;1H[35mimport[0m matplotlib.pyplot [33mas[0m plt[44;150H[K[44;150H21,0-1[8CTop[21;1H[34h[?25h[?25l[21;43r[43;1H
[1;44r[43;1H[35mfrom[0m matplotlib [35mimport[0m style[44;150H[K[44;150H21,1[10CTop[21;1H[34h[?25h[?25l[44;151H0[20;1H[34h[?25h[?25l[20;43r[43;1H
[1;44r[43;1H[35mfrom[0m sklearn.svm [35mimport[0m LinearSVR[44;150H[K[44;150H20,1[10CTop[20;1H[34h[?25h[?25l[44;150H19,0-1[19;1H[34h[?25h[?25l[44;150H[K[44;1H:[34h[?25hx[?25l[34h[?25h[?25l"cv_grid_all_ml.py" 314L, 18666C written
[?1l>[34h[?25h[?1049l(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ vi cv_grid_all_ml.pysource ~/venv/bin/activateexit[Kscript screen.logpython cv_grid_all_ml.py 2  nahnah /home/ciaran/completed_big_matrix_binary_new_snps_ids_first_10k..raw  top FT16cv5k 10006 &> rbg_try.log[KM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cscript screen.log[K
[KM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cexit[Ksource ~/venv/bin/activate[6Pvi cv_grid_all_ml.pysource ~/venv/bin/activateexit[Kscript screen.logpython cv_grid_all_ml.py 2  nahnah /home/ciaran/completed_big_matrix_binary_new_snps_ids_first_10k..raw  top FT16cv5k 10006 &> rbg_try.log[8@2>&1 | tee[C[C[C[C[C[C[C[C[C[C[C[C[K >> rbg_try.log[K
Please remember to set the right set size in the nested_cv code
WARNING THIS IS AN EDITED SCRIPT - Ciaran Kelly 2021 
 Edited with permission under liscence 
 Top version
Set size set to 10006
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
2
nahnah
/home/ciaran/completed_big_matrix_binary_new_snps_ids_first_10k.raw
top
FT16cv5k
10006
2021-10-14 20:21:00
Performing SVM
{'degree': [1, 2, 3], 'kernel': ['poly'], 'C': [1, 2], 'gamma': [0.1, 0.4, 0.7, 1.0]}
{'C': [1, 2], 'loss': ['epsilon_insensitive', 'squared_epsilon_insensitive']}
Performing RBG
ADBUCE
INNER COUNT NO.  1
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 24 is smaller than n_iter=50. Running 24 iterations. For exhaustive searches, use GridSearchCV.
  % (grid_size, self.n_iter, grid_size), UserWarning)
{'degree': 1, 'gamma': 0.1, 'C': 1, 'kernel': 'poly'}
Blue
Red
0.664951400220033
Typer SVR
{'degree': 1, 'gamma': 0.4, 'C': 1, 'kernel': 'poly'}
Blue
Red
0.6504421693872825
Typer SVR
{'degree': 1, 'gamma': 0.7, 'C': 1, 'kernel': 'poly'}
Blue
Red
0.6504421717916923
Typer SVR
{'degree': 1, 'gamma': 1.0, 'C': 1, 'kernel': 'poly'}
Blue
Red
0.6504415819129383
Typer SVR
{'degree': 2, 'gamma': 0.1, 'C': 1, 'kernel': 'poly'}
Blue
Red
0.6464131641399719
Typer SVR
{'degree': 2, 'gamma': 0.4, 'C': 1, 'kernel': 'poly'}
Blue
Red
0.6464131641399719
Typer SVR
{'degree': 2, 'gamma': 0.7, 'C': 1, 'kernel': 'poly'}
Blue
Red
0.6464130952573006
Typer SVR
{'degree': 2, 'gamma': 1.0, 'C': 1, 'kernel': 'poly'}
Blue
Red
0.6464131429453601
Typer SVR
{'degree': 3, 'gamma': 0.1, 'C': 1, 'kernel': 'poly'}
Blue
Red
0.61887449325136
Typer SVR
{'degree': 3, 'gamma': 0.4, 'C': 1, 'kernel': 'poly'}
Blue
Red
0.61887449325136
Typer SVR
{'degree': 3, 'gamma': 0.7, 'C': 1, 'kernel': 'poly'}
Blue
^Z
[1]+  Stopped                 python cv_grid_all_ml.py 2 nahnah /home/ciaran/completed_big_matrix_binary_new_snps_ids_first_10k.raw top FT16cv5k 10006
(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ python cv_grid_all_ml.py 2  nahnah /home/ciaran/completed_big_matrix_binary_new_snps_ids_first_10k..raw  top FT16cv5k 10006M[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cvi cv_grid_all_ml.py[K
[KM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C
[?1049h[?1h=[1;44r[34l[34h[?25h[23m[24m[0m[H[J[?25l[44;1H"cv_grid_all_ml.py" 314L, 18666C[1;1H[34m#Warning : best model selected by NMAE and R2 might not be the same
#performs linear regression, linear regression, neural network, svm and random forest, LASSO, RIDGE, CNN
#source ~/venv/bin/activate #in python 3.5.2
#print a log to a .txt file!
#model = pickle.load(open('FILEPATH', 'rb')) 
#dependencies = {'coeff_determination':coeff_determination}
#model = tf.keras.models.load_model('FILEPATH', custom_objects=dependencies)
#

#import tensorflow
#import numpy as np; import scipy #need to do this before path insert
#import sys
#sys.path.insert(1, '/external_storage/ciaran/Library/Python/3.7/python/site-packages/')
#import dill as pickle
#sys.path.insert(1, '/external_storage/ciaran/Library/Python/3.7/python/site-packages/nested_cv')
#from nested_cv import NestedCV
#with open('NCV_NN.pkl', 'rb') as f:
#     red = pickle.load(f)[0m

[36mprint[0m([31m"Please remember to set the right set size in the nested_cv code"[0m)
[35mimport[0m sys
sys.path.insert([31m1[0m, [31m'/external_storage/ciaran/Library/Python/3.7/python/site-packages/nested_cv'[0m)
num = sys.argv[[31m1[0m] [34m#script number for saving out[0m
spacer = [36mstr[0m(sys.argv[[31m2[0m]) [34m#nothing rn[0m
data = [36mstr[0m(sys.argv[[31m3[0m]) [34m#needs to be same size as set_size[0m
snps = [36mstr[0m(sys.argv[[31m4[0m]) [34m#top or shuf[0m
phenotype = [36mstr[0m(sys.argv[[31m5[0m]) [34m#make a directory for the results[0m
set_size = [36mint[0m(sys.argv[[31m6[0m]) [34m#how many SNPs[0m

[35mfrom[0m nested_cv [35mimport[0m NestedCV
[35mimport[0m statistics
[35mimport[0m numpy [33mas[0m np
[35mimport[0m sklearn
[35mimport[0m seaborn [33mas[0m sns
[35mfrom[0m sklearn [35mimport[0m preprocessing
[35mfrom[0m sklearn.pipeline [35mimport[0m make_pipeline, Pipeline
[35mfrom[0m sklearn.preprocessing [35mimport[0m StandardScaler
[35mimport[0m datetime
[35mimport[0m time
[35mfrom[0m statistics [35mimport[0m mean
[35mimport[0m matplotlib.pyplot [33mas[0m plt
[35mfrom[0m matplotlib [35mimport[0m style
[35mfrom[0m sklearn.svm [35mimport[0m LinearSVR[44;150H1,1[11CTop[1;1H[34h[?25h[?25l[44;150H2[2;1H[34h[?25h[?25l[44;150H3[3;1H[34h[?25h[?25l[44;150H4[4;1H[34h[?25h[?25l[44;150H5[5;1H[34h[?25h[?25l[44;150H6[6;1H[34h[?25h[?25l[44;150H7[7;1H[34h[?25h[?25l[44;150H8[8;1H[34h[?25h[?25l[44;150H9,0-1[9;1H[34h[?25h[?25l[44;150H10,1 [10;1H[34h[?25h[?25l[44;151H1[11;1H[34h[?25h[?25l[44;151H2[12;1H[34h[?25h[?25l[44;151H3[13;1H[34h[?25h[?25l[44;151H4[14;1H[34h[?25h[?25l[44;151H5[15;1H[34h[?25h[?25l[44;151H6[16;1H[34h[?25h[?25l[44;151H7[17;1H[34h[?25h[?25l[44;151H8[18;1H[34h[?25h[?25l[44;151H9,0-1[19;1H[34h[?25h[?25l[44;150H20,1  [20;1H[34h[?25h[?25l[44;151H1[21;1H[34h[?25h[?25l[44;151H2[22;1H[34h[?25h[?25l[44;151H3[23;1H[34h[?25h[?25l[44;151H4[24;1H[34h[?25h[?25l[44;151H5[25;1H[34h[?25h[?25l[44;151H6[26;1H[34h[?25h[?25l[44;151H7[27;1H[34h[?25h[?25l[44;151H8[28;1H[34h[?25h[?25l[44;151H9,0-1[29;1H[34h[?25h[?25l[44;150H30,1  [30;1H[34h[?25h[?25l[44;151H1[31;1H[34h[?25h[?25l[44;151H2[32;1H[34h[?25h[?25l[44;151H3[33;1H[34h[?25h[?25l[44;151H4[34;1H[34h[?25h[?25l[44;151H5[35;1H[34h[?25h[?25l[44;151H6[36;1H[34h[?25h[?25l[44;151H7[37;1H[34h[?25h[?25l[44;151H8[38;1H[34h[?25h[?25l[44;151H9[39;1H[34h[?25h[?25l[44;150H40[40;1H[34h[?25h[?25l[44;151H1[41;1H[34h[?25h[?25l[44;151H2[42;1H[34h[?25h[?25l[44;151H3[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[35mfrom[0m sklearn.svm [35mimport[0m SVR[44;1H[K[44;150H44,1[11C0%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[35mfrom[0m sklearn.model_selection [35mimport[0m train_test_split[44;150H[K[44;150H45,1[11C0%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[35mfrom[0m sklearn.model_selection [35mimport[0m RandomizedSearchCV[44;150H[K[44;150H46,1[11C1%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[35mfrom[0m sklearn.linear_model [35mimport[0m LinearRegression, Lasso, Ridge[44;150H[K[44;150H47,1[11C1%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[34m#import pickle #use dill instead below[0m[44;150H[K[44;150H48,1[11C1%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[35mfrom[0m sklearn.ensemble [35mimport[0m RandomForestRegressor [34m#based in part on https://towardsdatascience.com/random-forest-in-python-24d0893d51c0[0m[44;150H[K[44;150H49,1[11C2%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[35mfrom[0m statistics [35mimport[0m mean[44;150H[K[44;150H50,1[11C2%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[35mimport[0m os[44;150H[K[44;150H51,1[11C2%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[35mimport[0m pandas [33mas[0m pd [34m# data processing, CSV file I/O (e.g. pd.read_csv)[0m[44;150H[K[44;150H52,1[11C3%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[35mfrom[0m sklearn.metrics [35mimport[0m make_scorer[44;150H[K[44;150H53,1[11C3%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[35mimport[0m tensorflow [33mas[0m tf[44;150H[K[44;150H54,1[11C4%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[35mfrom[0m tensorflow.python.keras [35mimport[0m backend [33mas[0m K[44;150H[K[44;150H55,1[11C4%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[35mfrom[0m tensorflow.keras.layers [35mimport[0m Dense, Dropout, Flatten [34m#.core[0m[44;150H[K[44;150H56,1[11C4%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[35mfrom[0m tensorflow.keras.optimizers [35mimport[0m SGD[44;150H[K[44;150H57,1[11C5%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[35mfrom[0m tensorflow.keras.layers [35mimport[0m Dropout[44;150H[K[44;150H58,1[11C5%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[35mfrom[0m sklearn.externals [35mimport[0m joblib[44;150H[K[44;150H59,1[11C5%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[35mfrom[0m tensorflow.keras.models [35mimport[0m Sequential[44;150H[K[44;150H60,1[11C6%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[35mfrom[0m tensorflow.keras.layers [35mimport[0m Dense[44;150H[K[44;150H61,1[11C6%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[35mfrom[0m tensorflow.keras.wrappers.scikit_learn [35mimport[0m KerasRegressor [34m# or Classifier[0m[44;150H[K[44;150H62,1[11C7%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[35mfrom[0m sklearn.model_selection [35mimport[0m KFold[44;150H[K[44;150H63,1[11C7%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[35mfrom[0m tensorflow.keras.optimizers [35mimport[0m SGD[44;150H[K[44;150H64,1[11C7%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[35mfrom[0m tensorflow.keras.layers [35mimport[0m Dropout[44;150H[K[44;150H65,1[11C8%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[35mimport[0m random[44;150H[K[44;150H66,1[11C8%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[35mfrom[0m tensorboard.plugins.hparams [35mimport[0m api [33mas[0m hp[44;150H[K[44;150H67,1[11C8%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[34m#https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb[0m[44;150H[K[44;150H68,1[11C9%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[35mfrom[0m tensorboard.plugins.hparams [35mimport[0m api [33mas[0m hp[44;150H[K[44;150H69,1[11C9%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[35mimport[0m random[44;150H[K[44;150H70,1[11C9%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[33mif[0m snps == [31m'shuf'[0m :[44;150H[K[44;150H71,1[10C10%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9H[36mprint[0m([31m"Shuf nestedCV in usage"[0m)[44;150H[K[44;150H72,1-8[8C10%[43;8H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9H[35mfrom[0m nested_cv_shuf [35mimport[0m NestedCV[44;150H[K[44;150H73,1-8[8C11%[43;8H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[33melif[0m snps == [31m'top'[0m:[44;150H[K[44;150H74,1[10C11%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9H[35mfrom[0m nested_cv [35mimport[0m NestedCV[44;150H[K[44;150H75,1-8[8C11%[43;8H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[33melse[0m:[44;150H[K[44;150H76,1[10C12%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9H[36mprint[0m([31m"snnps must be top or shuf"[0m)[44;150H[K[44;150H77,1-8[8C12%[43;8H[34h[?25h[?25l[1;43r[43;1H
[1;44r[44;150H[K[44;150H78,1-8[8C12%[43;8H[34h[?25h[?25l[1;43r[43;1H
[1;44r[44;150H[K[44;150H79,0-1[8C13%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1Hsys.path.insert([31m1[0m, [31m'/external_storage/ciaran/Library/Python/3.7/python/site-packages/'[0m)[44;150H[K[44;150H80,1[10C13%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[35mimport[0m dill [33mas[0m pickle[44;150H[K[44;150H81,1[10C14%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[33mfor[0m i [33min[0m [36mrange[0m([31m1[0m,[36mlen[0m(sys.argv)):[44;150H[K[44;150H82,1[10C14%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9H[36mprint[0m(sys.argv[i])[44;150H[K[44;150H83,1-8[8C14%[43;8H[34h[?25h[?25l[1;43r[43;1H
[1;44r[44;150H[K[44;150H84,0-1[8C15%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[33mif[0m [33mnot[0m os.path.exists([31m'/external_storage/ciaran/arabadopsis/'[0m + phenotype):[44;150H[K[44;150H85,1[10C15%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;5Hos.makedirs([31m'/external_storage/ciaran/arabadopsis/'[0m + phenotype)[44;150H[K[44;150H86,1[10C15%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[44;150H[K[44;150H87,0-1[8C16%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1Hos.chdir([31m'/external_storage/ciaran/arabadopsis/'[0m + phenotype)[44;150H[K[44;150H88,1[10C16%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1Hdate_object = datetime.datetime.now().replace(second=[31m0[0m,microsecond=[31m0[0m)[44;150H[K[44;150H89,1[10C16%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[36mprint[0m(date_object)[44;150H[K[44;150H90,1[10C17%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[44;150H[K[44;150H91,0-1[8C17%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[33mdef[0m [36mcoeff_determination[0m(y_true, y_pred):[44;150H[K[44;150H92,1[10C18%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9HSS_res = K.sum(K.square( y_true-y_pred ))[44;150H[K[44;150H93,1[10C18%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9HSS_tot = K.sum(K.square( y_true - K.mean(y_true)))[44;150H[K[44;150H94,1[10C18%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9H[33mreturn[0m ([31m1[0m-SS_res/SS_tot)[44;150H[K[44;150H95,1[10C19%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[44;150H[K[44;150H96,0-1[8C19%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[44;150H[K[44;150H97,0-1[8C19%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[33mdef[0m [36mload_data[0m(data):[44;150H[K[44;150H98,1[10C20%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9Hdataset = np.loadtxt(data, skiprows=[31m1[0m)[44;150H[K[44;150H99,1[10C20%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9Hx = dataset[: , [31m6[0m:set_size]/[31m2[0m[44;150H[K[44;150H100,1[9C21%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9Hy = dataset[: , [31m5[0m ][44;150H[K[44;150H101,1[9C21%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9Hy = y.reshape(-[31m1[0m,[31m1[0m)[44;150H[K[44;150H102,1[9C21%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9H[34m#print("Performing split of raw data....")[0m[44;150H[K[44;150H103,1[9C22%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9H[34m#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.8, random_state=42)[0m[44;150H[K[44;150H104,1[9C22%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9H[33mreturn[0m x, y [34m#x_train, y_train, x_test, y_test[0m[44;150H[K[44;150H105,1[9C22%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[44;150H[K[44;150H106,0-1[7C23%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[44;150H[K[44;150H107,0-1[7C23%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[44;150H[K[44;150H108,0-1[7C23%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[33mwith[0m [36mopen[0m(([31m'validation_results_'[0m+ [36mstr[0m(snps) +[36mstr[0m(num) + phenotype + [36mstr[0m([31m"{:%Y_%m_%d}"[0m.format(datetime.datetime.now())) + [31m'.vallog'[0m ), [31m'a'[0m) [33mas[0m f:[44;150H[K[44;150H109,1[9C24%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9Horiginal_stdout = sys.stdout [34m# Save a reference to the original standard output[0m[44;150H[K[44;150H110,1[9C24%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9Hsys.stdout = f [34m# Change the standard output to the file we created.[0m[44;150H[K[44;150H111,1[9C25%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9H[36mprint[0m(datetime.datetime.now())[44;150H[K[44;150H112,1[9C25%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9Hsys.stdout = original_stdout[44;150H[K[44;150H113,1[9C25%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[44;150H[K[44;150H114,0-1[7C26%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[33mdef[0m [36mbaseline[0m(x, y):[44;150H[K[44;150H115,1[9C26%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9Hmodel = LinearRegression()[44;150H[K[44;150H116,1[9C26%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9Hmodel.fit(x, y)[44;150H[K[44;150H117,1[9C27%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9H[33mreturn[0m model[44;150H[K[44;150H118,1[9C27%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[44;150H[K[44;150H119,0-1[7C28%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[44;150H[K[44;150H120,0-1[7C28%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[33mdef[0m [36mavg_cv_result[0m(measure,cv_result):[44;150H[K[44;150H121,1[9C28%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9Hmy_var_name = [k [33mfor[0m k,v [33min[0m [36mlocals[0m().items() [33mif[0m v == measure][[31m0[0m] [34m#just to print out the name[0m[44;150H[K[44;150H122,1-8[7C29%[43;8H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9H[36mprint[0m(my_var_name)[44;150H[K[44;150H123,1-8[7C29%[43;8H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9Hn_combos = [36mlen[0m(cv_result.cv_results_[[31m'split0_test_neg_mean_absolute_error'[0m])[44;150H[K[44;150H124,1-8[7C29%[43;8H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9Hnamed_dict = {} [34m#dictonary will have a list of results PER grid combination across all CV results and this will return the average result. [0m[44;150H[K[44;150H125,1-8[7C30%[43;8H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9Havg_list = [][44;150H[K[44;150H126,1-8[7C30%[43;8H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9H[33mfor[0m combo [33min[0m [36mrange[0m([31m0[0m, n_combos):[44;150H[K[44;150H127,1-8[7C30%[43;8H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;17Hnamed_dict[[36mstr[0m(combo)] = [][44;150H[K[44;150H128,1-8[7C31%[43;8H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;17H[33mfor[0m split [33min[0m measure:[44;150H[K[44;150H129,1-8[7C31%[43;8H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;25Hnamed_dict[[36mstr[0m(combo)].append(cv_result.cv_results_[split][combo])[44;150H[K[44;150H130,1-8[7C32%[43;8H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;17Havg_list.append(statistics.mean(named_dict[[36mstr[0m(combo)]))[44;150H[K[44;150H131,1-8[7C32%[43;8H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;17H[36mprint[0m(combo, statistics.mean(named_dict[[36mstr[0m(combo)]))[44;150H[K[44;150H132,1-8[7C32%[43;8H[34h[?25h[?25l[1;43r[43;1H
[1;44r[44;150H[K[44;150H133,0-1[7C33%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9H[36mprint[0m([31m'Max'[0m, np.nanmax(avg_list), np.where(avg_list == np.nanmax(avg_list)))[44;150H[K[44;150H134,1-8[7C33%[43;8H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9H[36mprint[0m([31m'Min'[0m, np.nanmin(avg_list), np.where(avg_list == np.nanmin(avg_list)))[44;150H[K[44;150H135,1-8[7C33%[43;8H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9H[33mreturn[0m avg_list[44;150H[K[44;150H136,1-8[7C34%[43;8H[34h[?25h[?25l[1;43r[43;1H
[1;44r[44;150H[K[44;150H137,0-1[7C34%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1Hx_train, y_train = load_data(data)[44;150H[K[44;150H138,1[9C35%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1Hname_list = np.loadtxt(data, skiprows=[31m1[0m, usecols=([31m0[0m,), dtype=[31m'str'[0m)[44;150H[K[44;150H139,1[9C35%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[44;150H[K[44;150H140,0-1[7C35%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1Hscaler = preprocessing.StandardScaler().fit(y_train)[44;150H[K[44;150H141,1[9C36%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[34m#pickle.dump(scaler, open('scaler.pkl', 'wb'))[0m[44;150H[K[44;150H142,1[9C36%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[34m#scaler = pickle.load(open('scaler.pkl', 'rb'))[0m[44;150H[K[44;150H143,1[9C36%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[44;150H[K[44;150H144,0-1[7C37%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1Hy_train = scaler.transform(y_train)[44;150H[K[44;150H145,1[9C37%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[44;150H[K[44;150H146,0-1[7C38%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1Hn_snps = x_train.shape[[31m1[0m][44;150H[K[44;150H147,1[9C38%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1Hmy_cv = sklearn.model_selection.KFold(n_splits=[31m10[0m, shuffle=[36mTrue[0m, random_state=[31m42[0m)[44;150H[K[44;150H148,1[9C38%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[34m#################################################SVM####SVM#####SVM####################################################################[0m[44;150H[K[44;150H149,1[9C39%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[33mdef[0m [36mncv_results[0m(analysis, ncv_object):[44;150H[K[44;150H150,1[9C39%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9H[36mprint[0m([31m"Best Params of %s is %s "[0m % (analysis, ncv_object.best_params))[44;150H[K[44;150H151,1[9C39%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9H[36mprint[0m([31m"Outer scores of %s is %s "[0m % (analysis, ncv_object.outer_scores))[44;150H[K[44;150H152,1[9C40%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9H[36mprint[0m([31m"Variance of %s is %s "[0m % (analysis, ncv_object.variance))[44;150H[K[44;150H153,1[9C40%[43;1H[34h[?25h[?25l[1;43r[1;1H[2M[1;44r[42;9H[33mwith[0m [36mopen[0m([31m'NCV_'[0m + [36mstr[0m(analysis) + [31m'_'[0m +  [36mstr[0m(snps) + [31m'_'[0m + [36mstr[0m(phenotype) + [31m'_'[0m + [36mstr[0m(num) + [31m'.pkl'[0m, [31m'wb'[0m) [33mas[0m ncvfile: [34m#with open("fname.pkl", 'rb') as ncvfill[43;1He:[0m[44;150H[K[44;150H154,1[9C41%[42;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;17Hpickle.dump(ncv_object, ncvfile) [34m#ncv_object = pickle.load(ncvfile)[0m[44;150H[K[44;150H155,1[9C41%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[44;150H[K[44;150H156,0-1[7C41%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[33mdef[0m [36mnn_results[0m(analysis, ncv_object):[44;150H[K[44;150H157,1[9C42%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9H[36mprint[0m([31m"Best Params of %s is %s "[0m % (analysis, ncv_object.best_params))[44;150H[K[44;150H158,1[9C42%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9H[36mprint[0m([31m"Outer scores of %s is %s "[0m % (analysis, ncv_object.outer_scores))[44;150H[K[44;150H159,1[9C43%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9H[36mprint[0m([31m"Variance of %s is %s "[0m % (analysis, ncv_object.variance))[44;150H[K[44;150H160,1[9C43%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9Hnn_list = [ncv_object.best_inner_score_list, ncv_object.best_params, ncv_object.metric, ncv_object.outer_scores, ncv_object.variance][44;150H[K[44;150H161,1[9C43%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9H[33mwith[0m [36mopen[0m([31m'NCV_'[0m + [36mstr[0m(analysis) + [31m'_'[0m +  [36mstr[0m(snps) + [31m'_'[0m + [36mstr[0m(phenotype) + [31m'_'[0m + [36mstr[0m(num) + [31m'.pkl'[0m, [31m'wb'[0m) [33mas[0m ncvfile:[44;150H[K[44;150H162,1[9C44%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;17Hpickle.dump(nn_list, ncvfile) [34m#ncv_object = pickle.load(ncvfile)[0m[44;150H[K[44;150H163,1[9C44%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;9Hncv_object.model.model.save([31m"model_"[0m + [36mstr[0m(analysis) + [31m'_'[0m +  [36mstr[0m(snps) + [31m'_'[0m + [36mstr[0m(phenotype) + [31m'_'[0m + [36mstr[0m(num) + [31m".h5"[0m)[44;150H[K[44;150H164,1[9C44%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[44;150H[K[44;150H165,0-1[7C45%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[44;150H[K[44;150H166,0-1[7C45%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1H[36mprint[0m([31m"Performing SVM"[0m)[44;150H[K[44;150H167,1[9C45%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1Hc_param = [[31m1[0m,[31m2[0m][44;150H[K[44;150H168,1[9C46%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1Hgamma_param = [[36mfloat[0m(x) [33mfor[0m x [33min[0m np.linspace([31m0.1[0m, [31m1[0m, [31m4[0m)][44;150H[K[44;150H169,1[9C46%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[44;150H[K[44;150H170,0-1[7C47%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[44;150H[K[44;150H171,0-1[7C47%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1Hepsilon_param = [[36mfloat[0m(x) [33mfor[0m x [33min[0m np.linspace([31m0.1[0m, [31m1[0m, [31m4[0m)][44;150H[K[44;150H172,1[9C47%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1Hloss_param = [[31m'epsilon_insensitive'[0m, [31m'squared_epsilon_insensitive'[0m][44;150H[K[44;150H173,1[9C48%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1Hkernel_param = [[31m'poly'[0m][44;150H[K[44;150H174,1[9C48%[43;1H[34h[?25h[?25l[1;43r[43;1H
[1;44r[43;1Hdegree = [[31m1[0m,[31m2[0m,[31m3[0m][44;150H[K[44;150H175,1[9C48%[43;1H[34h[?25h[?25l[44;150H[K[44;1H:[34h[?25hq[?25l[34h[?25h![?25l[34h[?25h[?25l[44;1H[K[44;1H[?1l>[34h[?25h[?1049l(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ s[Kexit
exit
There are stopped jobs.
(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ exit
exit

Script done on Thu 14 Oct 2021 20:23:14 IST
