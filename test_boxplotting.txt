Script started on Sun 19 Dec 2021 17:08:16 GMT
bash: alias: NF} $1 | sort -nu | tail -n 1 : not found
(base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ source ~/venv/bin/activate
(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ source ~/venv/bin/activate[6Pvi cv_grid_all_ml.py~/j[Kvi cv_grid_all_ml.pysource ~/venv/bin/activate[Kpython cv_grid_all_ml.py 30 /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del /home/ciaran/completed_big_matrix_binary_new_snps_ids _first_10k.raw shuf FT10cv_new_10k 10000M[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del /home/ciaran/completed_big_matrix_binary_new_snps_ids_f[1PM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C1 /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del /home/ciaran/completed_big_matrix_binary_new_snps_ids_[1@fM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C#python cv_grid_all_ml.py 31 /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del /home/ciaran/completed_big_matrix_binary_new_snps_ids[1@_M[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C

(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ #python cv_grid_all_ml.py 31 /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del /home/ciaran/completed_big_matrix_binary_new_snps_idss_first_10k.raw shuf FT10cv_new_10k 10000M[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cpython cv_grid_all_ml.py 31 /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del /home/ciaran/completed_big_matrix_binary_new_snps_ids_[1PM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C

Please remember to set the right set size in the nested_cv code
/external_storage/ciaran/machine_learning2
WARNING THIS IS AN EDITED SCRIPT - Ciaran Kelly 2021 
 Edited with permission under liscence 
 Flex version
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
31
/home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del
/home/ciaran/completed_big_matrix_binary_new_snps_ids_first_10k.raw
shuf
FT10cv_new_10k
10000
2021-12-19 17:11:00
Performing SVM
{'degree': [1, 2, 3], 'C': [1, 2], 'gamma': [0.1, 0.4, 0.7, 1.0], 'kernel': ['poly']}
{'C': [1, 2], 'loss': ['epsilon_insensitive', 'squared_epsilon_insensitive']}
{'degree': [1, 2, 3], 'C': [1, 2], 'gamma': [0.1, 0.4, 0.7, 1.0], 'kernel': ['poly']}
Traceback (most recent call last):
  File "cv_grid_all_ml.py", line 246, in <module>
    SVM_NCV = NestedCV(model_name='LinearSVR', name_list = name_list, model=LinearSVR(), goal_dict=svm_goal_dict, time_dict=svm_time_dict, params_grid=svm_random_grid2, outer_kfolds=2, inner_kfolds=2, n_jobs = 8,cv_options={'randomized_search':True, 'randomized_search_iter':10, 'sqrt_of_score':False,'recursive_feature_elimination':False, 'metric':sklearn.metrics.r2_score, 'metric_score_indicator_lower':False})
NameError: name 'svm_goal_dict' is not defined
(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ python cv_grid_all_ml.py 31 /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del /home/ciaran/completed_big_matrix_binary_new_snps_ids__first_10k.raw shuf FT10cv_new_10k 10000M[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[K
[KM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cpython cv_grid_all_ml.py 31 /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del /home/ciaran/completed_big_matrix_binary_new_snps_ids__first_10k.raw shuf FT10cv_new_10k 10000M[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C#python cv_grid_all_ml.py 31 /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del /home/ciaran/completed_big_matrix_binary_new_snps_ids[1@_M[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C

(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ cv_grid_all_ml.pyvcv_grid_all_ml.pyicv_grid_all_ml.py cv_grid_all_ml.py
[?1049h[?1h=[1;52r[34l[34h[?25h[23m[24m[0m[H[J[?25l[52;1H"cv_grid_all_ml.py" 388L, 22892C[1;1H[34m#Warning : best model selected by NMAE and R2 might not be the same
#performs linear regression, linear regression, neural network, svm and random forest, LASSO, RIDGE, CNN
#source ~/venv/bin/activate #in python 3.5.2
#print a log to a .txt file!
#model = pickle.load(open('FILEPATH', 'rb')) 
#dependencies = {'coeff_determination':coeff_determination}
#model = tf.keras.models.load_model('FILEPATH', custom_objects=dependencies)
#

#import tensorflow
#import numpy as np; import scipy #need to do this before path insert
#import sys
#sys.path.insert(1, '/external_storage/ciaran/Library/Python/3.7/python/site-packages/')
#import dill as pickle
#sys.path.insert(1, '/external_storage/ciaran/Library/Python/3.7/python/site-packages/nested_cv')
#from nested_cv import NestedCV
#with open('NCV_NN.pkl', 'rb') as f:
#     red = pickle.load(f)[0m

[36mprint[0m([31m"Please remember to set the right set size in the nested_cv code"[0m)
[35mimport[0m sys
sys.path.insert([31m1[0m, [31m'/external_storage/ciaran/Library/Python/3.7/python/site-packages/nested_cv'[0m)
num = sys.argv[[31m1[0m] [34m#script number for saving out[0m
phenfile = [36mstr[0m(sys.argv[[31m2[0m]) [34m#txt file with phenotypes[0m
data = [36mstr[0m(sys.argv[[31m3[0m]) [34m#needs to be same size as set_size[0m
snps = [36mstr[0m(sys.argv[[31m4[0m]) [34m#top or shuf[0m
phenotype = [36mstr[0m(sys.argv[[31m5[0m]) [34m#make a directory for the results[0m
set_size = [36mint[0m(sys.argv[[31m6[0m]) [34m#how many SNPs[0m
[35mfrom[0m nested_cv [35mimport[0m NestedCV
[35mimport[0m statistics
[35mimport[0m numpy [33mas[0m np
[35mimport[0m sklearn
[35mimport[0m seaborn [33mas[0m sns
[35mfrom[0m sklearn [35mimport[0m preprocessing
[35mfrom[0m sklearn.pipeline [35mimport[0m make_pipeline, Pipeline
[35mfrom[0m sklearn.preprocessing [35mimport[0m StandardScaler
[35mimport[0m datetime
[35mimport[0m time
[35mfrom[0m statistics [35mimport[0m mean
[35mimport[0m matplotlib.pyplot [33mas[0m plt
[35mfrom[0m matplotlib [35mimport[0m style
[35mfrom[0m sklearn.svm [35mimport[0m LinearSVR
[35mfrom[0m sklearn.svm [35mimport[0m SVR
[35mfrom[0m sklearn.model_selection [35mimport[0m train_test_split
[35mfrom[0m sklearn.model_selection [35mimport[0m RandomizedSearchCV
[35mfrom[0m sklearn.linear_model [35mimport[0m LinearRegression, Lasso, Ridge
[34m#import pickle #use dill instead below[0m
[35mfrom[0m sklearn.ensemble [35mimport[0m RandomForestRegressor [34m#based in part on https://towardsdatascience.com/random-forest-in-python-24d0893d51c0[0m
[35mfrom[0m statistics [35mimport[0m mean
[35mimport[0m os
[35mimport[0m pandas [33mas[0m pd [34m# data processing, CSV file I/O (e.g. pd.read_csv)[0m[52;194H1,1[11CTop[1;1H[34h[?25h[?25l[52;194H2[2;1H[34h[?25h[?25l[52;194H3[3;1H[34h[?25h[?25l[52;194H4[4;1H[34h[?25h[?25l[52;194H5[5;1H[34h[?25h[?25l[52;194H6[6;1H[34h[?25h[?25l[52;194H7[7;1H[34h[?25h[?25l[52;194H8[8;1H[34h[?25h[?25l[52;194H9,0-1[9;1H[34h[?25h[?25l[52;194H10,1 [10;1H[34h[?25h[?25l[52;195H1[11;1H[34h[?25h[?25l[52;195H2[12;1H[34h[?25h[?25l[52;195H3[13;1H[34h[?25h[?25l[52;195H4[14;1H[34h[?25h[?25l[52;195H5[15;1H[34h[?25h[?25l[52;195H6[16;1H[34h[?25h[?25l[52;195H7[17;1H[34h[?25h[?25l[52;195H8[18;1H[34h[?25h[?25l[52;195H9,0-1[19;1H[34h[?25h[?25l[52;194H20,1  [20;1H[34h[?25h[?25l[52;195H1[21;1H[34h[?25h[?25l[52;195H2[22;1H[34h[?25h[?25l[52;195H3[23;1H[34h[?25h[?25l[52;195H4[24;1H[34h[?25h[?25l[52;195H5[25;1H[34h[?25h[?25l[52;195H6[26;1H[34h[?25h[?25l[52;195H7[27;1H[34h[?25h[?25l[52;195H8[28;1H[34h[?25h[?25l[52;195H9[29;1H[34h[?25h[?25l[52;194H30[30;1H[34h[?25h[?25l[52;195H1[31;1H[34h[?25h[?25l[52;195H2[32;1H[34h[?25h[?25l[52;195H3[33;1H[34h[?25h[?25l[52;195H4[34;1H[34h[?25h[?25l[52;195H5[35;1H[34h[?25h[?25l[52;195H6[36;1H[34h[?25h[?25l[52;195H7[37;1H[34h[?25h[?25l[52;195H8[38;1H[34h[?25h[?25l[52;195H9[39;1H[34h[?25h[?25l[52;194H40[40;1H[34h[?25h[?25l[52;195H1[41;1H[34h[?25h[?25l[52;195H2[42;1H[34h[?25h[?25l[52;195H3[43;1H[34h[?25h[?25l[52;195H4[44;1H[34h[?25h[?25l[52;195H5[45;1H[34h[?25h[?25l[52;195H6[46;1H[34h[?25h[?25l[52;195H7[47;1H[34h[?25h[?25l[52;195H8[48;1H[34h[?25h[?25l[52;195H9[49;1H[34h[?25h[?25l[52;194H50[50;1H[34h[?25h[?25l[52;195H1[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m sklearn.metrics [35mimport[0m make_scorer[52;1H[K[52;194H52,1[11C0%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mimport[0m tensorflow [33mas[0m tf[52;194H[K[52;194H53,1[11C0%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.python.keras [35mimport[0m backend [33mas[0m K[52;194H[K[52;194H54,1[11C0%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.layers [35mimport[0m Dense, Dropout, Flatten [34m#.core[0m[52;194H[K[52;194H55,1[11C1%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.optimizers [35mimport[0m SGD[52;194H[K[52;194H56,1[11C1%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.layers [35mimport[0m Dropout[52;194H[K[52;194H57,1[11C1%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m sklearn.externals [35mimport[0m joblib[52;194H[K[52;194H58,1[11C2%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.models [35mimport[0m Sequential, Model[52;194H[K[52;194H59,1[11C2%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.layers [35mimport[0m Dense[52;194H[K[52;194H60,1[11C2%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.wrappers.scikit_learn [35mimport[0m KerasRegressor [34m# or Classifier[0m[52;194H[K[52;194H61,1[11C2%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m sklearn.model_selection [35mimport[0m KFold[52;194H[K[52;194H62,1[11C3%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.optimizers [35mimport[0m SGD[52;194H[K[52;194H63,1[11C3%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.layers [35mimport[0m Dropout[52;194H[K[52;194H64,1[11C3%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mimport[0m random[52;194H[K[52;194H65,1[11C4%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb[0m[52;194H[K[52;194H66,1[11C4%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorboard.plugins.hparams [35mimport[0m api [33mas[0m hp[52;194H[K[52;194H67,1[11C4%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mimport[0m random[52;194H[K[52;194H68,1[11C5%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.python.keras.layers [35mimport[0m deserialize, serialize[52;194H[K[52;194H69,1[11C5%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.python.keras.saving [35mimport[0m saving_utils[52;194H[K[52;194H70,1[11C5%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H71,0-1[9C5%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H72,0-1[9C6%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H73,0-1[9C6%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#if snps == 'shuf' :[0m[52;194H[K[52;194H74,1[11C6%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#       print("Shuf nestedCV in usage")[0m[52;194H[K[52;194H75,1[11C7%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#       from nested_cv_shuf import NestedCV[0m[52;194H[K[52;194H76,1[11C7%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#elif snps == 'top':[0m[52;194H[K[52;194H77,1[11C7%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#       from nested_cv import NestedCV[0m[52;194H[K[52;194H78,1[11C8%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#else:[0m[52;194H[K[52;194H79,1[11C8%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#       print("snnps must be top or shuf")[0m[52;194H[K[52;194H80,1[11C8%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H81,1-8[9C8%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H82,0-1[9C9%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hsys.path.insert([31m1[0m, [31m'/external_storage/ciaran/Library/Python/3.7/python/site-packages/'[0m)[52;194H[K[52;194H83,1[11C9%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mimport[0m dill [33mas[0m pickle[52;194H[K[52;194H84,1[11C9%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mfor[0m i [33min[0m [36mrange[0m([31m1[0m,[36mlen[0m(sys.argv)):[52;194H[K[52;194H85,1[10C10%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m(sys.argv[i])[52;194H[K[52;194H86,1-8[8C10%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H87,0-1[8C10%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mif[0m [33mnot[0m os.path.exists([31m'/external_storage/ciaran/arabadopsis/'[0m + phenotype+ [31m'/'[0m + snps):[52;194H[K[52;194H88,1[10C10%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5Hos.makedirs([31m'/external_storage/ciaran/arabadopsis/'[0m + phenotype+ [31m'/'[0m + snps)[52;194H[K[52;194H89,1[10C11%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H90,0-1[8C11%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hos.chdir([31m'/external_storage/ciaran/arabadopsis/'[0m + phenotype+ [31m'/'[0m + snps)[52;194H[K[52;194H91,1[10C11%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hdate_object = datetime.datetime.now().replace(second=[31m0[0m,microsecond=[31m0[0m)[52;194H[K[52;194H92,1[10C12%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[36mprint[0m(date_object)[52;194H[K[52;194H93,1[10C12%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H94,0-1[8C12%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mcoeff_determination[0m(y_true, y_pred):[52;194H[K[52;194H95,1[10C13%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9HSS_res = K.sum(K.square( y_true-y_pred ))[52;194H[K[52;194H96,1[10C13%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9HSS_tot = K.sum(K.square( y_true - K.mean(y_true)))[52;194H[K[52;194H97,1[10C13%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mreturn[0m ([31m1[0m-SS_res/SS_tot)[52;194H[K[52;194H98,1[10C13%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H99,0-1[8C14%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H100,0-1[7C14%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mload_data[0m(data):[52;194H[K[52;194H101,1[9C14%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hdataset = np.loadtxt(data, skiprows=[31m1[0m)[52;194H[K[52;194H102,1[9C15%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hx = dataset[: , [31m6[0m:set_size+[31m6[0m]/[31m2[0m[52;194H[K[52;194H103,1[9C15%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hy = dataset[: , [31m5[0m ][52;194H[K[52;194H104,1[9C15%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hy = y.reshape(-[31m1[0m,[31m1[0m)[52;194H[K[52;194H105,1[9C16%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[34m#print("Performing split of raw data....")[0m[52;194H[K[52;194H106,1[9C16%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[34m#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.8, random_state=42)[0m[52;194H[K[52;194H107,1[9C16%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mreturn[0m x, y [34m#x_train, y_train, x_test, y_test[0m[52;194H[K[52;194H108,1[9C16%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H109,0-1[7C17%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H110,0-1[7C17%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H111,0-1[7C17%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mwith[0m [36mopen[0m(([31m'validation_results_'[0m+ [36mstr[0m(snps) +[36mstr[0m(num) + phenotype + [36mstr[0m([31m"{:%Y_%m_%d}"[0m.format(datetime.datetime.now())) + [31m'.vallog'[0m ), [31m'a'[0m) [33mas[0m f:[52;194H[K[52;194H112,1[9C18%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Horiginal_stdout = sys.stdout [34m# Save a reference to the original standard output[0m[52;194H[K[52;194H113,1[9C18%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hsys.stdout = f [34m# Change the standard output to the file we created.[0m[52;194H[K[52;194H114,1[9C18%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m(datetime.datetime.now())[52;194H[K[52;194H115,1[9C18%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hsys.stdout = original_stdout[52;194H[K[52;194H116,1[9C19%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H117,0-1[7C19%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mbaseline[0m(x, y):[52;194H[K[52;194H118,1[9C19%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hmodel = LinearRegression()[52;194H[K[52;194H119,1[9C20%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hmodel.fit(x, y)[52;194H[K[52;194H120,1[9C20%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mreturn[0m model[52;194H[K[52;194H121,1[9C20%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H122,0-1[7C21%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H123,0-1[7C21%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mavg_cv_result[0m(measure,cv_result):[52;194H[K[52;194H124,1[9C21%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hmy_var_name = [k [33mfor[0m k,v [33min[0m [36mlocals[0m().items() [33mif[0m v == measure][[31m0[0m] [34m#just to print out the name[0m[52;194H[K[52;194H125,1-8[7C21%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m(my_var_name)[52;194H[K[52;194H126,1-8[7C22%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hn_combos = [36mlen[0m(cv_result.cv_results_[[31m'split0_test_neg_mean_absolute_error'[0m])[52;194H[K[52;194H127,1-8[7C22%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hnamed_dict = {} [34m#dictonary will have a list of results PER grid combination across all CV results and this will return the average result. [0m[52;194H[K[52;194H128,1-8[7C22%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Havg_list = [][52;194H[K[52;194H129,1-8[7C23%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mfor[0m combo [33min[0m [36mrange[0m([31m0[0m, n_combos):[52;194H[K[52;194H130,1-8[7C23%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hnamed_dict[[36mstr[0m(combo)] = [][52;194H[K[52;194H131,1-8[7C23%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17H[33mfor[0m split [33min[0m measure:[52;194H[K[52;194H132,1-8[7C24%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;25Hnamed_dict[[36mstr[0m(combo)].append(cv_result.cv_results_[split][combo])[52;194H[K[52;194H133,1-8[7C24%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Havg_list.append(statistics.mean(named_dict[[36mstr[0m(combo)]))[52;194H[K[52;194H134,1-8[7C24%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17H[36mprint[0m(combo, statistics.mean(named_dict[[36mstr[0m(combo)]))[52;194H[K[52;194H135,1-8[7C24%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H136,0-1[7C25%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m'Max'[0m, np.nanmax(avg_list), np.where(avg_list == np.nanmax(avg_list)))[52;194H[K[52;194H137,1-8[7C25%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m'Min'[0m, np.nanmin(avg_list), np.where(avg_list == np.nanmin(avg_list)))[52;194H[K[52;194H138,1-8[7C25%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mreturn[0m avg_list[52;194H[K[52;194H139,1-8[7C26%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H140,0-1[7C26%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H141,0-1[7C26%[51;1H[34h[?25h[?25l[1;51r[1;1H[2M[1;52r[50;1H[33mdef[0m [36munpack[0m(model, training_config, weights): [34m##https://github.com/tensorflow/tensorflow/issues/34697 #fixes an error that the early stopping callback throws up in the nested cv #something about the parralele fitt[51;1Hting step needing everything to be pickle-able and the callback isnt [0m[52;194H[K[52;194H142,1[9C27%[50;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5Hrestored_model = deserialize(model)[52;194H[K[52;194H143,1[9C27%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5H[33mif[0m training_config [33mis[0m [33mnot[0m [36mNone[0m:[52;194H[K[52;194H144,1[9C27%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hrestored_model.compile([52;194H[K[52;194H145,1[9C28%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;13H**saving_utils.compile_args_from_training_config([52;194H[K[52;194H146,1[9C28%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Htraining_config[52;194H[K[52;194H147,1[9C28%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;13H)[52;194H[K[52;194H148,1[9C28%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H)[52;194H[K[52;194H149,1[9C29%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5Hrestored_model.set_weights(weights)[52;194H[K[52;194H150,1[9C29%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5H[33mreturn[0m restored_model[52;194H[K[52;194H151,1[9C29%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H152,0-1[7C30%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m# Hotfix function[0m[52;194H[K[52;194H153,1[9C30%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mmake_keras_picklable[0m():[52;194H[K[52;194H154,1[9C30%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H155,0-1[7C31%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5H[33mdef[0m [36m__reduce__[0m(self):[52;194H[K[52;194H156,1[9C31%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hmodel_metadata = saving_utils.model_metadata(self)[52;194H[K[52;194H157,1[9C31%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Htraining_config = model_metadata.get([31m"training_config"[0m, [36mNone[0m)[52;194H[K[52;194H158,1[9C31%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hmodel = serialize(self)[52;194H[K[52;194H159,1[9C32%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hweights = self.get_weights()[52;194H[K[52;194H160,1[9C32%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mreturn[0m (unpack, (model, training_config, weights))[52;194H[K[52;194H161,1[9C32%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H162,0-1[7C33%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5Hcls = Model[52;194H[K[52;194H163,1[9C33%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5Hcls.__reduce__ = __reduce__[52;194H[K[52;194H164,1[9C33%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H165,0-1[7C34%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H166,0-1[7C34%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mmake_param_box_plot[0m(goal_dict, time_dict):[52;194H[K[52;194H167,1[9C34%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mfor[0m param [33min[0m goal_dict:[52;194H[K[52;194H168,1-8[7C34%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.subplots([31m1[0m,[31m2[0m,figsize=([31m12[0m,[31m8[0m))[52;194H[K[52;194H169,1[9C35%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.subplot([31m121[0m)[52;194H[K[52;194H170,1[9C35%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.boxplot(goal_dict[param].values(), bootstrap=[36mNone[0m,showmeans=[36mFalse[0m, meanline=[36mFalse[0m, notch=[36mTrue[0m,labels=goal_dict[param].keys()) [34m#orange line is median, green dotted line is mean[0m[52;194H[K[52;194H171,1[9C35%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.xlabel([36mstr[0m(param).upper(), fontsize=[31m10[0m, fontweight=[31m'bold'[0m)[52;194H[K[52;194H172,1[9C36%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.ylabel([31m'R^2'[0m, fontsize=[31m10[0m,fontweight=[31m'bold'[0m)[52;194H[K[52;194H173,1[9C36%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.title([31m'R^2 Score vs %s'[0m % param, fontsize=[31m14[0m, fontweight=[31m'bold'[0m)[52;194H[K[52;194H174,1[9C36%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17H[33mif[0m param == [31m'initialization'[0m:[52;194H[K[52;194H175,1[9C36%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;25Hplt.xticks(fontsize=[31m6[0m)[52;194H[K[52;194H176,1[9C37%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.subplot([31m122[0m)[52;194H[K[52;194H177,1[9C37%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.boxplot(time_dict[param].values(), bootstrap=[36mNone[0m,showmeans=[36mFalse[0m, meanline=[36mFalse[0m, notch=[36mFalse[0m,labels=time_dict[param].keys())[52;194H[K[52;194H178,1[9C37%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.xlabel([36mstr[0m(param).upper(), fontsize=[31m10[0m, fontweight=[31m'bold'[0m)[52;194H[K[52;194H179,1[9C38%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.ylabel([31m'Training Time'[0m, fontsize=[31m10[0m,fontweight=[31m'bold'[0m)[52;194H[K[52;194H180,1[9C38%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.title([31m'Training Time vs %s'[0m % param, fontsize=[31m14[0m, fontweight=[31m'bold'[0m)[52;194H[K[52;194H181,1[9C38%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.tight_layout(pad=[31m4[0m)[52;194H[K[52;194H182,1[9C39%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17H[33mif[0m param == [31m'initialization'[0m:[52;194H[K[52;194H183,1[9C39%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;25Hplt.xticks(fontsize=[31m6[0m)[52;194H[K[52;194H184,1[9C39%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hmy_fig_name = [31m"plots_of_"[0m + [36mstr[0m(param) + [31m'_'[0m + [36mstr[0m([31m"{:%Y_%m_%d}"[0m.format(datetime.datetime.now())) + [31m'_'[0m +[36mstr[0m(snps) +[36mstr[0m(num)+ [31m".png"[0m[52;194H[K[52;194H185,1[9C39%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.savefig(my_fig_name, dpi=[31m300[0m)[52;194H[K[52;194H186,1[9C40%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.show()[52;194H[K[52;194H187,1[9C40%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.clf()[52;194H[K[52;194H188,1[9C40%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.close()[52;194H[K[52;194H189,1[9C41%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H190,1-8[7C41%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mmake_goal_dict[0m(whole_dict):[52;194H[K[52;194H191,1[9C41%[51;1H[34h[?25h[?25l[1;51r[1;1H[2M[1;52r[50;9H[36mprint[0m(whole_dict)[51;9Hgoal_dict = {key:{} [33mfor[0m key [33min[0m whole_dict}[52;194H[K[52;194H192,1-8[7C42%[50;8H[34h[?25h[?25l[52;196H3[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mfor[0m key [33min[0m whole_dict:[52;194H[K[52;194H194,1-8[7C42%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17H[33mfor[0m item [33min[0m whole_dict[key]:[52;194H[K[52;194H195,1-8[7C42%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;25Hgoal_dict[key][item] = [][52;194H[K[52;194H196,1-8[7C43%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Htime_dict = {key:{} [33mfor[0m key [33min[0m whole_dict} [34m#both empty[0m[52;194H[K[52;194H197,1-8[7C43%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mfor[0m key [33min[0m whole_dict:[52;194H[K[52;194H198,1-8[7C43%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17H[33mfor[0m item [33min[0m whole_dict[key]:[52;194H[K[52;194H199,1[9C43%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;25Htime_dict[key][item] = [][52;194H[K[52;194H200,1[9C44%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mreturn[0m goal_dict, time_dict[52;194H[K[52;194H201,1-8[7C44%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H202,0-1[7C44%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hx_train, y_train = load_data(data)[52;194H[K[52;194H203,1[9C45%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hname_list = np.loadtxt(data, skiprows=[31m1[0m, usecols=([31m0[0m,), dtype=[31m'str'[0m)[52;194H[K[52;194H204,1[9C45%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H205,0-1[7C45%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hscaler = preprocessing.StandardScaler().fit(y_train)[52;194H[K[52;194H206,1[9C45%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#pickle.dump(scaler, open('scaler.pkl', 'wb'))[0m[52;194H[K[52;194H207,1[9C46%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#scaler = pickle.load(open('scaler.pkl', 'rb'))[0m[52;194H[K[52;194H208,1[9C46%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H209,0-1[7C46%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hy_train = scaler.transform(y_train)[52;194H[K[52;194H210,1[9C47%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H211,0-1[7C47%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hn_snps = x_train.shape[[31m1[0m][52;194H[K[52;194H212,1[9C47%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hmy_cv = sklearn.model_selection.KFold(n_splits=[31m10[0m, shuffle=[36mTrue[0m, random_state=[31m42[0m)[52;194H[K[52;194H213,1[9C48%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#################################################SVM####SVM#####SVM####################################################################[0m[52;194H[K[52;194H214,1[9C48%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mncv_results[0m(analysis, ncv_object):[52;194H[K[52;194H215,1[9C48%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Best Params of %s is %s "[0m % (analysis, ncv_object.best_params))[52;194H[K[52;194H216,1[9C48%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Outer scores of %s is %s and mean is %s"[0m % (analysis, ncv_object.outer_scores, np.mean(ncv_object.outer_scores)))[52;194H[K[52;194H217,1[9C49%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Variance of %s is %s "[0m % (analysis, ncv_object.variance))[52;194H[K[52;194H218,1[9C49%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mwith[0m [36mopen[0m([31m'NCV_'[0m + [36mstr[0m(analysis) + [31m'_'[0m +  [36mstr[0m(snps) + [31m'_'[0m + [36mstr[0m(phenotype) + [31m'_'[0m + [36mstr[0m(num) + [31m'.pkl'[0m, [31m'wb'[0m) [33mas[0m ncvfile: [34m#with open("fname.pkl", 'rb') as ncvfile:[0m[52;194H[K[52;194H219,1[9C49%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hpickle.dump(ncv_object, ncvfile) [34m#ncv_object = pickle.load(ncvfile)[0m[52;194H[K[52;194H220,1[9C50%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H221,0-1[7C50%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mnn_results[0m(analysis, ncv_object):[52;194H[K[52;194H222,1[9C50%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Best Params of %s is %s "[0m % (analysis, ncv_object.best_params))[52;194H[K[52;194H223,1[9C51%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Outer scores of %s is %s and mean is %s"[0m % (analysis, ncv_object.outer_scores, np.mean(ncv_object.outer_scores)))[52;194H[K[52;194H224,1[9C51%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Variance of %s is %s "[0m % (analysis, ncv_object.variance))[52;194H[K[52;194H225,1[9C51%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hnn_list = [ncv_object.best_inner_params_list, ncv_object.best_inner_score_list, ncv_object.best_params, ncv_object.metric, ncv_object.outer_scores, ncv_object.variance][52;194H[K[52;194H226,1[9C51%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mwith[0m [36mopen[0m([31m'NCV_'[0m + [36mstr[0m(analysis) + [31m'_'[0m +  [36mstr[0m(snps) + [31m'_'[0m + [36mstr[0m(phenotype) + [31m'_'[0m + [36mstr[0m(num) + [31m'.pkl'[0m, [31m'wb'[0m) [33mas[0m ncvfile:[52;194H[K[52;194H227,1[9C52%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hpickle.dump(nn_list, ncvfile) [34m#ncv_object = pickle.load(ncvfile)[0m[52;194H[K[52;194H228,1[9C52%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hncv_object.model.model.save([31m"model_"[0m + [36mstr[0m(analysis) + [31m'_'[0m +  [36mstr[0m(snps) + [31m'_'[0m + [36mstr[0m(phenotype) + [31m'_'[0m + [36mstr[0m(num) + [31m".h5"[0m)[52;194H[K[52;194H229,1[9C52%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H230,0-1[7C53%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H231,0-1[7C53%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[36mprint[0m([31m"Performing SVM"[0m)[52;194H[K[52;194H232,1[9C53%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hc_param = [[31m1[0m,[31m2[0m][52;194H[K[52;194H233,1[9C54%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hgamma_param = [[36mfloat[0m(x) [33mfor[0m x [33min[0m np.linspace([31m0.1[0m, [31m1[0m, [31m4[0m)][52;194H[K[52;194H234,1[9C54%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H235,0-1[7C54%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H236,0-1[7C54%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hepsilon_param = [[36mfloat[0m(x) [33mfor[0m x [33min[0m np.linspace([31m0.1[0m, [31m1[0m, [31m4[0m)][52;194H[K[52;194H237,1[9C55%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hloss_param = [[31m'epsilon_insensitive'[0m, [31m'squared_epsilon_insensitive'[0m][52;194H[K[52;194H238,1[9C55%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hkernel_param = [[31m'poly'[0m][52;194H[K[52;194H239,1[9C55%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hdegree = [[31m1[0m,[31m2[0m,[31m3[0m][52;194H[K[52;194H240,1[9C56%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hsvm_random_grid = {[31m'gamma'[0m:gamma_param, [31m'C'[0m:c_param,[31m'kernel'[0m:kernel_param, [31m"degree"[0m:degree}[52;194H[K[52;194H241,1[9C56%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[36mprint[0m(svm_random_grid)[52;194H[K[52;194H242,1[9C56%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hsvm_random_grid2 = {[31m'C'[0m : c_param, [31m'loss'[0m:loss_param}[52;194H[K[52;194H243,1[9C56%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[36mprint[0m(svm_random_grid2)[52;194H[K[52;194H244,1[9C57%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hrbg_goal_dict, rbg_time_dict = make_goal_dict(svm_random_grid)[52;194H[K[52;194H245,1[9C57%[51;1H[34h[?25h[?25l[1;51r[1;1H[2M[1;52r[50;1HSVM_NCV = NestedCV(model_name=[31m'LinearSVR'[0m, name_list = name_list, model=LinearSVR(), goal_dict=svm_goal_dict, time_dict=svm_time_dict, params_grid=svm_random_grid2, outer_kfolds=[31m2[0m, inner_kfolds=[31m2[0m, n_jobs = [31m8[0m,cv__[51;1Hoptions={[31m'randomized_search'[0m:[36mTrue[0m, [31m'randomized_search_iter'[0m:[31m10[0m, [31m'sqrt_of_score'[0m:[36mFalse[0m,[31m'recursive_feature_elimination'[0m:[36mFalse[0m, [31m'metric'[0m:sklearn.metrics.r2_score, [31m'metric_score_indicator_lower'[0m:[36mFalse[0m})[52;194H[K[52;194H246,1[9C57%[50;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1HSVM_NCV.fit(x_train, y_train.ravel(), name_list=name_list, phenfile=phenfile, set_size=set_size, snps=snps, model_name=[31m'SVM'[0m, goal_dict=svm_goal_dict, time_dict=svm_time_dict)[52;194H[K[52;194H247,1[9C58%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H248,0-1[7C58%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H249,0-1[7C58%[51;1H[34h[?25h[?25l[52;196H8[50;1H[34h[?25h[?25l[52;196H7,1  [49;1H[34h[?25h[?25l[52;196H6[47;1H[34h[?25h[?25l[52;196H5[46;1H[34h[?25h[?25l[52;196H4[45;1H[34h[?25h[?25l[52;196H3[44;1H[34h[?25h[?25l[52;196H2[43;1H[34h[?25h[?25l[52;196H1[42;1H[34h[?25h[?25l[52;196H0[41;1H[34h[?25h[?25l[52;195H39[40;1H[34h[?25h[?25l[52;196H8[39;1H[34h[?25h[?25l[52;196H7[38;1H[34h[?25h[?25l[52;196H6,0-1[37;1H[34h[?25h[?25l[52;196H7,1  [38;1H[34h[?25h[?25l[52;196H8[39;1H[34h[?25h[?25l[52;196H9[40;1H[34h[?25h[?25l[52;195H40[41;1H[34h[?25h[?25l[52;196H1[42;1H[34h[?25h[?25l[52;196H2[43;1H[34h[?25h[?25l[52;196H3[44;1H[34h[?25h[?25l[52;196H4[45;1H[34h[?25h[?25l[52;196H5[46;1H[34h[?25h[?25l[52;196H6[47;1H[34h[?25h[?25l[52;196H7[49;1H[34h[?25h[?25l[52;196H8,0-1[50;1H[34h[?25h[?25l[52;196H9[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hncv_results([31m'SVM'[0m, SVM_NCV)[52;194H[K[52;194H250,1[9C59%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31m'''[0m[52;194H[K[52;194H251,1[9C59%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mprint("Performing RBG")[0m[52;194H[K[52;194H252,1[9C59%[51;1H[34h[?25h[?25l[1;51r[1;1H[2M[1;52r[50;1H[31mRBG_NCV = NestedCV(model_name='RBG', name_list=name_list, model=SVR(), params_grid=svm_random_grid, outer_kfolds=4, inner_kfolds=4, n_jobs = 8,cv_options={'randomized_search':True, 'randomized_search_iter':50, ''[51;1Hsqrt_of_score':False,'recursive_feature_elimination':False, 'metric':sklearn.metrics.r2_score, 'metric_score_indicator_lower':False})[0m[52;194H[K[52;194H253,1[9C60%[50;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mRBG_NCV.fit(x_train, y_train.ravel(), name_list=name_list, phenfile=phenfile, set_size=set_size, snps=snps, model_name='RBG')[0m[52;194H[K[52;194H254,1[9C60%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mncv_results('RBG', RBG_NCV)[0m[52;194H[K[52;194H255,1[9C60%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H256,0-1[7C61%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mprint("Performing LASSO")[0m[52;194H[K[52;194H257,1[9C61%[51;1H[34h[?25h[?25l[52;196H6,0-1[50;1H[34h[?25h[?25l[52;196H5,1  [49;1H[34h[?25h[?25l[52;196H4[48;1H[34h[?25h[?25l[52;196H3[46;1H[34h[?25h[?25l[52;196H2[45;1H[34h[?25h[?25l[52;196H1[44;1H[34h[?25h[?25l[52;196H0[43;1H[34h[?25h[?25l[52;195H49,0-1[42;1H[34h[?25h[?25l[52;196H8[41;1H[34h[?25h[?25l[52;196H7,1  [40;1H[34h[?25h[?25l[52;196H6[38;1H[34h[?25h[?25l[52;196H5[37;1H[34h[?25h[?25l[52;196H6[38;1H[34h[?25h[?25l[52;1H[1m-- INSERT --[0m[52;194H[K[52;194H246,1[9C61%[38;1H[34h[?25h[?25l[40;51r[40;1H[L[1;52r[38;1H[K[39;1HSVM_NCV = NestedCV(model_name=[31m'LinearSVR'[0m, name_list = name_list, model=LinearSVR(), goal_dict=svm_goal_dict, time_dict=svm_time_dict, params_grid=svm_random_grid2, outer_kfolds=[31m2[0m, inner_kfolds=[31m2[0m, n_jobs = [31m8[0m,cv__[40;1Hoptions={[31m'randomized_search'[0m:[36mTrue[0m, [31m'randomized_search_iter'[0m:[31m10[0m, [31m'sqrt_of_score'[0m:[36mFalse[0m,[31m'recursive_feature_elimination'[0m:[36mFalse[0m, [31m'metric'[0m:sklearn.metrics.r2_score, [31m'metric_score_indicator_lower'[0m:[36mFalse[0m})[52;194H[K[52;194H247,1[9C61%[39;1H[34h[?25h[?25l[52;196H6[38;1H[34h[?25h[?25ls[52;198H2[38;2H[34h[?25h[?25lv[52;198H3[38;3H[34h[?25h[?25lm[52;198H4[38;4H[34h[?25h[?25l_[52;198H5[38;5H[34h[?25h[?25lgoal_dict, rbg_time_dict = make_goal_dict(svm_random_grid)[38;46H[46m([15C)[0m[52;198H63[38;63H[34h[?25h[?25l[52;199H2[38;62H[34h[?25h[?25l[46m2[0m)2[46m)[0m[52;199H3[38;63H[34h[?25h[?25l[38;46H([16C)[52;199H2[38;62H[34h[?25h[?25l[52;199H1[38;61H[34h[?25h[?25l[52;199H0[38;60H[34h[?25h[?25l[52;198H59[38;59H[34h[?25h[?25l[52;199H8[38;58H[34h[?25h[?25l[52;199H7[38;57H[34h[?25h[?25l[52;199H6[38;56H[34h[?25h[?25l[52;199H5[38;55H[34h[?25h[?25l[52;199H4[38;54H[34h[?25h[?25l[52;199H3[38;53H[34h[?25h[?25l[52;199H2[38;52H[34h[?25h[?25l[52;199H1[38;51H[34h[?25h[?25l[52;199H0[38;50H[34h[?25h[?25l[52;198H49[38;49H[34h[?25h[?25l[52;199H8[38;48H[34h[?25h[?25l[46m([16C)[0m[52;199H7[38;47H[34h[?25h[?25l[52;199H6[38;46H[34h[?25h[?25l([16C)[52;199H5[38;45H[34h[?25h[?25l[52;199H4[38;44H[34h[?25h[?25l[52;199H3[38;43H[34h[?25h[?25l[52;199H2[38;42H[34h[?25h[?25l[52;199H1[38;41H[34h[?25h[?25l[52;199H0[38;40H[34h[?25h[?25l[52;198H39[38;39H[34h[?25h[?25l[52;199H8[38;38H[34h[?25h[?25l[52;199H7[38;37H[34h[?25h[?25l[52;199H6[38;36H[34h[?25h[?25l[52;199H5[38;35H[34h[?25h[?25l[52;199H4[38;34H[34h[?25h[?25l[52;199H3[38;33H[34h[?25h[?25l[52;199H2[38;32H[34h[?25h[?25l[52;199H1[38;31H[34h[?25h[?25l[52;199H0[38;30H[34h[?25h[?25l[52;198H29[38;29H[34h[?25h[?25l[52;199H8[38;28H[34h[?25h[?25l[52;199H7[38;27H[34h[?25h[?25l[52;199H6[38;26H[34h[?25h[?25l[52;199H5[38;25H[34h[?25h[?25l[52;199H4[38;24H[34h[?25h[?25l[52;199H3[38;23H[34h[?25h[?25l[52;199H2[38;22H[34h[?25h[?25l[52;199H1[38;21H[34h[?25h[?25l[52;199H0[38;20H[34h[?25h[?25l[52;198H19[38;19H[34h[?25h[?25l_time_dict = make_goal_dict(svm_random_grid2)[38;63H[K[52;199H8[38;18H[34h[?25h[?25l_time_dict = make_goal_dict(svm_random_grid2)[38;62H[K[52;199H7[38;17H[34h[?25h[?25l_time_dict = make_goal_dict(svm_random_grid2)[38;61H[K[52;199H6[38;16H[34h[?25h[?25ls_time_dict = make_goal_dict(svm_random_grid2)[52;199H7[38;17H[34h[?25h[?25lv_time_dict = make_goal_dict(svm_random_grid2)[52;199H8[38;18H[34h[?25h[?25lm_time_dict = make_goal_dict(svm_random_grid2)[52;199H9[38;19H[34h[?25h[52;1H[K[38;18H[?25l[52;194H246,18[8C61%[38;18H[34h[?25h[?25l[52;194H[K[52;1H:[34h[?25hx[?25l[34h[?25h[?25l"cv_grid_all_ml.py" 389L, 22956C written
[?1l>[34h[?25h[?1049l(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ vi cv_grid_all_ml.py#python cv_grid_all_ml.py 31 /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del /home/ciaran/completed_big_matrix_binary_new_snps_idss_first_10k.raw shuf FT10cv_new_10k 10000M[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cpython cv_grid_all_ml.py 31 /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del /home/ciaran/completed_big_matrix_binary_new_snps_ids_[1PM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C

Please remember to set the right set size in the nested_cv code
/external_storage/ciaran/machine_learning2
WARNING THIS IS AN EDITED SCRIPT - Ciaran Kelly 2021 
 Edited with permission under liscence 
 Flex version
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
31
/home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del
/home/ciaran/completed_big_matrix_binary_new_snps_ids_first_10k.raw
shuf
FT10cv_new_10k
10000
2021-12-19 17:13:00
Performing SVM
{'C': [1, 2], 'degree': [1, 2, 3], 'gamma': [0.1, 0.4, 0.7, 1.0], 'kernel': ['poly']}
{'C': [1, 2], 'loss': ['epsilon_insensitive', 'squared_epsilon_insensitive']}
{'C': [1, 2], 'degree': [1, 2, 3], 'gamma': [0.1, 0.4, 0.7, 1.0], 'kernel': ['poly']}
{'C': [1, 2], 'loss': ['epsilon_insensitive', 'squared_epsilon_insensitive']}
ADBUCE
INNER COUNT NO.  1
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_1_in_1_in.raw
Set size set to 10000
Set size set to 10000
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.
  % (grid_size, self.n_iter, grid_size), UserWarning)
{'C': 1, 'loss': 'epsilon_insensitive'}
Blue
Red
0.18789471361508492
Typer LinearSVR
{'C': 1, 'loss': 'squared_epsilon_insensitive'}
Blue
Red
0.1904531757008896
Typer LinearSVR
{'C': 2, 'loss': 'epsilon_insensitive'}
Blue
Red
0.18788922220406257
Typer LinearSVR
{'C': 2, 'loss': 'squared_epsilon_insensitive'}
Blue
Red
0.18919697469892227
Typer LinearSVR
INNER COUNT NO.  2
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_1_in_2_in.raw
Set size set to 10000
Set size set to 10000
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.
  % (grid_size, self.n_iter, grid_size), UserWarning)
{'C': 1, 'loss': 'epsilon_insensitive'}
Blue
Red
0.40006938597469965
Typer LinearSVR
{'C': 1, 'loss': 'squared_epsilon_insensitive'}
Blue
Red
0.40223227721763155
Typer LinearSVR
{'C': 2, 'loss': 'epsilon_insensitive'}
Blue
Red
0.40007704042987724
Typer LinearSVR
{'C': 2, 'loss': 'squared_epsilon_insensitive'}
Blue
Red
0.40117389950943394
Typer LinearSVR
OUTER COUNT NO.  1
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
SETTING OFF CUSTOM BASH SCRIPT
1
2
out
blue
purple
Pheno is FT10
PLINK v1.90b3.31 64-bit (3 Feb 2016)       https://www.cog-genomics.org/plink2
(C) 2005-2016 Shaun Purcell, Christopher Chang   GNU General Public License v3
Logging to train_raw_plink_shuf_1_in_2_out.log.
Options in effect:
  --bfile /home/ciaran/completed_big_matrix_binary_new_snps_ids
  --extract shuf_10000_snps_1_in_2_out.txt
  --keep name_vector_train.txt
  --out train_raw_plink_shuf_1_in_2_out
  --pheno /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del
  --prune
  --recode A

1029912 MB RAM detected; reserving 514956 MB for main workspace.
10709465 variants loaded from .bim file.
2029 people (2029 males, 0 females) loaded from .fam.
1058 phenotype values present after --pheno.
^Z
[1]+  Stopped                 python cv_grid_all_ml.py 31 /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del /home/ciaran/completed_big_matrix_binary_new_snps_ids_first_10k.raw shuf FT10cv_new_10k 10000
(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ python cv_grid_all_ml.py 31 /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del /home/ciaran/completed_big_matrix_binary_new_snps_ids__first_10k.raw shuf FT10cv_new_10k 10000
Please remember to set the right set size in the nested_cv code
/external_storage/ciaran/machine_learning2
WARNING THIS IS AN EDITED SCRIPT - Ciaran Kelly 2021 
 Edited with permission under liscence 
 Flex version
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
31
/home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del
/home/ciaran/completed_big_matrix_binary_new_snps_ids_first_10k.raw
shuf
FT10cv_new_10k
10000
2021-12-19 17:17:00
Performing SVM
{'degree': [1, 2, 3], 'kernel': ['poly'], 'C': [1, 2], 'gamma': [0.1, 0.4, 0.7, 1.0]}
{'loss': ['epsilon_insensitive', 'squared_epsilon_insensitive'], 'C': [1, 2]}
{'degree': [1, 2, 3], 'kernel': ['poly'], 'C': [1, 2], 'gamma': [0.1, 0.4, 0.7, 1.0]}
{'loss': ['epsilon_insensitive', 'squared_epsilon_insensitive'], 'C': [1, 2]}
ADBUCE
INNER COUNT NO.  1
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_1_in_1_in.raw
Set size set to 10000
Set size set to 10000
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.
  % (grid_size, self.n_iter, grid_size), UserWarning)
{'loss': 'epsilon_insensitive', 'C': 1}
Blue
Red
0.1878835550884872
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 1}
Blue
Red
0.1904604608511533
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 2}
Blue
Red
0.18789281610435038
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 2}
Blue
Red
0.18919931611548757
Typer LinearSVR
INNER COUNT NO.  2
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_1_in_2_in.raw
Set size set to 10000
Set size set to 10000
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.
  % (grid_size, self.n_iter, grid_size), UserWarning)
{'loss': 'epsilon_insensitive', 'C': 1}
Blue
Red
0.40006801639432976
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 1}
Blue
Red
0.40223930668070573
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 2}
Blue
Red
0.40007266137367714
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 2}
Blue
Red
0.4011830498306005
Typer LinearSVR
OUTER COUNT NO.  1
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
SETTING OFF CUSTOM BASH SCRIPT
1
2
out
blue
purple
Pheno is FT10
PLINK v1.90b3.31 64-bit (3 Feb 2016)       https://www.cog-genomics.org/plink2
(C) 2005-2016 Shaun Purcell, Christopher Chang   GNU General Public License v3
Logging to train_raw_plink_shuf_1_in_2_out.log.
Options in effect:
  --bfile /home/ciaran/completed_big_matrix_binary_new_snps_ids
  --extract shuf_10000_snps_1_in_2_out.txt
  --keep name_vector_train.txt
  --out train_raw_plink_shuf_1_in_2_out
  --pheno /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del
  --prune
  --recode A

1029912 MB RAM detected; reserving 514956 MB for main workspace.
10709465 variants loaded from .bim file.
2029 people (2029 males, 0 females) loaded from .fam.
1058 phenotype values present after --pheno.
--extract: 10000 variants remaining.
--keep: 510 people remaining.
--prune: 510 people remaining.
Using 1 thread (no multithreaded calculations invoked.
Before main variant filters, 510 founders and 0 nonfounders present.
Calculating allele frequencies... 0%1%2%3%4%5%6%7%8%9%10%11%12%13%14%15%16%17%18%19%20%21%22%23%24%25%26%27%28%29%30%31%32%33%34%35%36%37%38%39%40%41%42%43%44%45%46%47%48%49%50%51%52%53%54%55%56%57%58%59%60%61%62%63%64%65%66%67%68%69%70%71%72%73%74%75%76%77%78%79%80%81%82%83%84%85%86%87%88%89%90%91%92%93%94%95%96%97%98%99% done.
10000 variants and 510 people pass filters and QC.
Phenotype data is quantitative.
--recode A to train_raw_plink_shuf_1_in_2_out.raw ... 0%1%2%3%4%5%6%7%8%9%10%11%12%13%14%15%16%17%18%19%20%21%22%23%24%25%26%27%28%29%30%31%32%33%34%35%36%37%38%39%40%41%42%43%44%45%46%47%48%49%50%51%52%53%54%55%56%57%58%59%60%61%62%63%64%65%66%67%68%69%70%71%72%73%74%75%76%77%78%79%80%81%82%83%84%85%86%87%88%89%90%91%92%93%94%95%96%97%98%99%done.
PLINK v1.90b3.31 64-bit (3 Feb 2016)       https://www.cog-genomics.org/plink2
(C) 2005-2016 Shaun Purcell, Christopher Chang   GNU General Public License v3
Logging to test_raw_plink_shuf_1_in_2_out.log.
Options in effect:
  --bfile /home/ciaran/completed_big_matrix_binary_new_snps_ids
  --extract shuf_10000_snps_1_in_2_out.txt
  --keep name_vector_test.txt
  --out test_raw_plink_shuf_1_in_2_out
  --pheno /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del
  --prune
  --recode A

1029912 MB RAM detected; reserving 514956 MB for main workspace.
10709465 variants loaded from .bim file.
2029 people (2029 males, 0 females) loaded from .fam.
1058 phenotype values present after --pheno.
--extract: 10000 variants remaining.
--keep: 511 people remaining.
--prune: 511 people remaining.
Using 1 thread (no multithreaded calculations invoked.
Before main variant filters, 511 founders and 0 nonfounders present.
Calculating allele frequencies... 0%1%2%3%4%5%6%7%8%9%10%11%12%13%14%15%16%17%18%19%20%21%22%23%24%25%26%27%28%29%30%31%32%33%34%35%36%37%38%39%40%41%42%43%44%45%46%47%48%49%50%51%52%53%54%55%56%57%58%59%60%61%62%63%64%65%66%67%68%69%70%71%72%73%74%75%76%77%78%79%80%81%82%83%84%85%86%87%88%89%90%91%92%93%94%95%96%97%98%99% done.
10000 variants and 511 people pass filters and QC.
Phenotype data is quantitative.
--recode A to test_raw_plink_shuf_1_in_2_out.raw ... 0%1%2%3%4%5%6%7%8%9%10%11%12%13%14%15%16%17%18%19%20%21%22%23%24%25%26%27%28%29%30%31%32%33%34%35%36%37%38%39%40%41%42%43%44%45%46%47%48%49%50%51%52%53%54%55%56%57%58%59%60%61%62%63%64%65%66%67%68%69%70%71%72%73%74%75%76%77%78%79%80%81%82%83%84%85%86%87%88%89%90%91%92%93%94%95%96%97%98%99%done.
test_raw_plink_shuf_1_in_2_out.raw
Set size set to 10000
Set size set to 10000
0.3962748812555281
INNER COUNT NO.  1
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_2_in_1_in.raw
Set size set to 10000
Set size set to 10000
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.
  % (grid_size, self.n_iter, grid_size), UserWarning)
{'loss': 'epsilon_insensitive', 'C': 1}
Blue
Red
0.40100062022520055
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 1}
Blue
Red
0.4027976242649238
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 2}
Blue
Red
0.40100316845961537
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 2}
Blue
Red
0.40191449410457714
Typer LinearSVR
INNER COUNT NO.  2
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_2_in_2_in.raw
Set size set to 10000
Set size set to 10000
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.
  % (grid_size, self.n_iter, grid_size), UserWarning)
{'loss': 'epsilon_insensitive', 'C': 1}
Blue
Red
0.4208409537866301
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 1}
Blue
Red
0.4229214074014491
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 2}
Blue
Red
0.42084229787397764
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 2}
Blue
Red
0.42190154647458666
Typer LinearSVR
OUTER COUNT NO.  2
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
SETTING OFF CUSTOM BASH SCRIPT
2
2
out
blue
purple
Pheno is FT10
PLINK v1.90b3.31 64-bit (3 Feb 2016)       https://www.cog-genomics.org/plink2
(C) 2005-2016 Shaun Purcell, Christopher Chang   GNU General Public License v3
Logging to train_raw_plink_shuf_2_in_2_out.log.
Options in effect:
  --bfile /home/ciaran/completed_big_matrix_binary_new_snps_ids
  --extract shuf_10000_snps_2_in_2_out.txt
  --keep name_vector_train.txt
  --out train_raw_plink_shuf_2_in_2_out
  --pheno /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del
  --prune
  --recode A

1029912 MB RAM detected; reserving 514956 MB for main workspace.
10709465 variants loaded from .bim file.
2029 people (2029 males, 0 females) loaded from .fam.
1058 phenotype values present after --pheno.
--extract: 10000 variants remaining.
--keep: 511 people remaining.
--prune: 511 people remaining.
Using 1 thread (no multithreaded calculations invoked.
Before main variant filters, 511 founders and 0 nonfounders present.
Calculating allele frequencies... 0%1%2%3%4%5%6%7%8%9%10%11%12%13%14%15%16%17%18%19%20%21%22%23%24%25%26%27%28%29%30%31%32%33%34%35%36%37%38%39%40%41%42%43%44%45%46%47%48%49%50%51%52%53%54%55%56%57%58%59%60%61%62%63%64%65%66%67%68%69%70%71%72%73%74%75%76%77%78%79%80%81%82%83%84%85%86%87%88%89%90%91%92%93%94%95%96%97%98%99% done.
10000 variants and 511 people pass filters and QC.
Phenotype data is quantitative.
--recode A to train_raw_plink_shuf_2_in_2_out.raw ... 0%1%2%3%4%5%6%7%8%9%10%11%12%13%14%15%16%17%18%19%20%21%22%23%24%25%26%27%28%29%30%31%32%33%34%35%36%37%38%39%40%41%42%43%44%45%46%47%48%49%50%51%52%53%54%55%56%57%58%59%60%61%62%63%64%65%66%67%68%69%70%71%72%73%74%75%76%77%78%79%80%81%82%83%84%85%86%87%88%89%90%91%92%93%94%95%96%97%98%99%done.
PLINK v1.90b3.31 64-bit (3 Feb 2016)       https://www.cog-genomics.org/plink2
(C) 2005-2016 Shaun Purcell, Christopher Chang   GNU General Public License v3
Logging to test_raw_plink_shuf_2_in_2_out.log.
Options in effect:
  --bfile /home/ciaran/completed_big_matrix_binary_new_snps_ids
  --extract shuf_10000_snps_2_in_2_out.txt
  --keep name_vector_test.txt
  --out test_raw_plink_shuf_2_in_2_out
  --pheno /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del
  --prune
  --recode A

1029912 MB RAM detected; reserving 514956 MB for main workspace.
10709465 variants loaded from .bim file.
2029 people (2029 males, 0 females) loaded from .fam.
1058 phenotype values present after --pheno.
--extract: 10000 variants remaining.
--keep: 510 people remaining.
--prune: 510 people remaining.
Using 1 thread (no multithreaded calculations invoked.
Before main variant filters, 510 founders and 0 nonfounders present.
Calculating allele frequencies... 0%1%2%3%4%5%6%7%8%9%10%11%12%13%14%15%16%17%18%19%20%21%22%23%24%25%26%27%28%29%30%31%32%33%34%35%36%37%38%39%40%41%42%43%44%45%46%47%48%49%50%51%52%53%54%55%56%57%58%59%60%61%62%63%64%65%66%67%68%69%70%71%72%73%74%75%76%77%78%79%80%81%82%83%84%85%86%87%88%89%90%91%92%93%94%95%96%97%98%99% done.
10000 variants and 510 people pass filters and QC.
Phenotype data is quantitative.
--recode A to test_raw_plink_shuf_2_in_2_out.raw ... 0%1%2%3%4%5%6%7%8%9%10%11%12%13%14%15%16%17%18%19%20%21%22%23%24%25%26%27%28%29%30%31%32%33%34%35%36%37%38%39%40%41%42%43%44%45%46%47%48%49%50%51%52%53%54%55%56%57%58%59%60%61%62%63%64%65%66%67%68%69%70%71%72%73%74%75%76%77%78%79%80%81%82%83%84%85%86%87%88%89%90%91%92%93%94%95%96%97%98%99%done.
test_raw_plink_shuf_2_in_2_out.raw
Set size set to 10000
Set size set to 10000
0.3439661094917219
Traceback (most recent call last):
  File "cv_grid_all_ml.py", line 248, in <module>
    SVM_NCV.fit(x_train, y_train.ravel(), name_list=name_list, phenfile=phenfile, set_size=set_size, snps=snps, model_name='SVM', goal_dict=svm_goal_dict, time_dict=svm_time_dict)
  File "/external_storage/ciaran/Library/Python/3.7/python/site-packages/nested_cv/nested_cv.py", line 365, in fit
    self.goal_list = goal_list
NameError: name 'goal_list' is not defined
(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ /external_storage/ciaran/Library/Python/3.7/python/site-packages/nested_cv/nested_cv.py[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Ci/external_storage/ciaran/Library/Python/3.7/python/site-packages/nested_cv/nested_cv.py[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[1P/external_storage/ciaran/Library/Python/3.7/python/site-packages/nested_cv/nested_cv.py[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cv/external_storage/ciaran/Library/Python/3.7/python/site-packages/nested_cv/nested_cv.py[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Ci/external_storage/ciaran/Library/Python/3.7/python/site-packages/nested_cv/nested_cv.py[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C /external_storage/ciaran/Library/Python/3.7/python/site-packages/nested_cv/nested_cv.py[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C
[?1049h[?1h=[1;52r[34l[34h[?25h[23m[24m[0m[H[J[?25l[52;1H"/external_storage/ciaran/Library/Python/3.7/python/site-packages/nested_cv/nested_cv.py" 382L, 18005C[1;1H[35mimport[0m logging [33mas[0m log
[35mimport[0m pandas [33mas[0m pd
[35mimport[0m numpy [33mas[0m np
[35mfrom[0m matplotlib [35mimport[0m pyplot [33mas[0m plt
[35mfrom[0m sklearn.model_selection [35mimport[0m KFold, ParameterGrid, ParameterSampler
[35mfrom[0m sklearn.metrics [35mimport[0m mean_squared_error
[35mfrom[0m sklearn.feature_selection [35mimport[0m RFECV
[35mfrom[0m sklearn.utils.multiclass [35mimport[0m type_of_target
[35mfrom[0m joblib [35mimport[0m Parallel, delayed
[35mimport[0m os.path
[35mimport[0m time
[35mimport[0m subprocess
[35mimport[0m sys
[35mimport[0m sklearn
[35mfrom[0m sklearn [35mimport[0m preprocessing
[35mimport[0m os
[36mprint[0m(os.getcwd())
[36mprint[0m([31m"WARNING THIS IS AN EDITED SCRIPT - Ciaran Kelly 2021 [0m[35m\n[0m[31m Edited with permission under liscence [0m[35m\n[0m[31m Flex version"[0m)
[34m#set_size = 10006    
#print("Set size set to %s" % set_size)[0m

[33mdef[0m [36mload_data[0m(data, set_size):[23;9H[36mprint[0m([31m"Set size set to %s"[0m % set_size)[24;9Hdataset = np.loadtxt(data, skiprows=[31m1[0m)[25;9Hx = dataset[: , [31m6[0m:(set_size+[31m6[0m)]/[31m2[0m[26;9Hy = dataset[: , [31m5[0m ][27;9Hy = y.reshape(-[31m1[0m,[31m1[0m)[28;9H[34m#print("Performing split of raw data....")[29;9H#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.8, random_state=42)[0m[30;9H[33mreturn[0m x, y [34m#x_train, y_train, x_test, y_test[0m


[33mdef[0m [36mbash_script[0m(train_index, test_index, train_names, test_names, outer_count, inner_count, phenfile, set_size, snps, outer=[36mFalse[0m):[34;9H[36mprint[0m(os.getcwd())[35;9H[33mif[0m outer==[36mTrue[0m:[36;13Hfoo=[31m'out'[0m[37;9H[33melse[0m:[38;13Hfoo=[31m'in'[0m[39;9H[33mif[0m [33mnot[0m os.path.exists([31m'train_raw_plink_'[0m + [36mstr[0m(snps) + [31m'_'[0m + [36mstr[0m(outer_count) + [31m'_in_'[0m + [36mstr[0m(inner_count) + [31m'_'[0m + foo + [31m'.raw'[0m):[40;13H[36mprint[0m([31m"SETTING OFF CUSTOM BASH SCRIPT"[0m)[41;13H[33mwith[0m [36mopen[0m([31m"name_vector_train.txt"[0m, [31m'w'[0m) [33mas[0m f:[42;17H[33mfor[0m item [33min[0m train_names:[43;21Hf.write([31m"%s %s[0m[35m\n[0m[31m"[0m % (item, item))[44;13H[33mwith[0m [36mopen[0m([31m"name_vector_test.txt"[0m, [31m'w'[0m) [33mas[0m f:[45;17H[33mfor[0m item [33min[0m test_names:[46;21Hf.write([31m"%s %s[0m[35m\n[0m[31m"[0m % (item, item))[48;13Hsubprocess.run([[31m"/external_storage/ciaran/machine_learning2/bash_script.sh"[0m, [36mstr[0m(outer_count), [36mstr[0m(inner_count), foo, [36mstr[0m(phenfile), [36mstr[0m(set_size), [36mstr[0m(snps)])[49;9H[34m#while not os.path.exists('train_raw_plink_shuf_' + str(outer_count) + '_in_' + str(inner_count) + '.raw'):[0m[50;9H[33mwhile[0m [33mnot[0m os.path.exists([31m'test_raw_plink_'[0m +  [36mstr[0m(snps) +  [31m'_'[0m + [36mstr[0m(outer_count) + [31m'_in_'[0m + [36mstr[0m(inner_count) + [31m'_'[0m + foo + [31m'.raw'[0m):[51;13Htime.sleep([31m20[0m)[52;194H1,1[11CTop[1;1H[34h[?25h[?25l[52;1H[K[52;1H/[34h[?25hg[?25l[34h[?25ho[?25l[34h[?25ha[?25l[34h[?25hl[?25l[34h[?25h_[?25l[34h[?25hl[?25l[34h[?25hi[?25l[34h[?25hs[?25l[34h[?25ht[?25l[34h[?25h[?25l[23m[24m[0m[H[J[2;13Hbest_inner_score, best_inner_params = self._best_of_results(search_scores)[4;13Hbest_inner_params_list.append(best_inner_params)[5;13Hbest_inner_score_list.append(best_inner_score)[6;13H[34m#inner_count = 0[0m[7;13H[36mprint[0m([31m"OUTER COUNT NO. "[0m, [36mstr[0m(outer_count))[8;13H[34m# Fit the best hyperparameters from one of the K inner loops[0m[9;13Hself.model.set_params(**best_inner_params)[10;13HX_train_outer, X_test_outer, y_train_outer, y_test_outer = bash_script(train_index, test_index, outer_train_names, outer_test_names, outer_count, inner_count, phenfile, set_size, snps, outer=[36mTrue[0m)[11;13Houter_count += [31m1[0m[12;13H[33mif[0m model_name == [31m'CNN'[0m:[13;17HX_train_outer = X_train_outer.reshape(X_train_outer.shape[[31m0[0m],X_train_outer.shape[[31m1[0m],[31m1[0m)[14;17HX_test_outer = X_test_outer.reshape(X_test_outer.shape[[31m0[0m],X_test_outer.shape[[31m1[0m],[31m1[0m)[15;13Hself.model.fit(X_train_outer, y_train_outer.ravel())[17;13H[34m# Get score and prediction[0m[18;13Hscore,pred = self._predict_and_score(X_test_outer, y_test_outer.ravel())[19;13Houter_scores.append(self._transform_score_format(score))[21;13H[34m# Append variance[0m[22;13Hvariance.append(np.var(pred, ddof=[31m1[0m))[24;13Hlog.debug([31m'[0m[35m\n[0m[31mResults for outer fold:[0m[35m\n[0m[31mBest inner parameters was: {0}'[0m.format([25;17Hbest_inner_params_list[i]))[26;13Hlog.debug([31m'Outer score: {0}'[0m.format(outer_scores[i]))[27;13Hlog.debug([31m'Inner score: {0}'[0m.format(best_inner_score_list[i]))[29;9Hself.variance = variance[30;9Hself.outer_scores = outer_scores[31;9Hself.best_inner_score_list = best_inner_score_list[32;9Hself.best_params = self._score_to_best_params(best_inner_params_list)[33;9Hself.best_inner_params_list = best_inner_params_list[34;9Hself.goal_list = goal_list[35;9Hself.time_list = time_list[37;5H[34m# Method to show score vs variance chart. You can run it only after fitting the model.[0m
    [33mdef[0m [36mscore_vs_variance_plot[0m(self):[39;9H[34m# Plot score vs variance[0m[40;9Hplt.figure()[41;9Hplt.subplot([31m211[0m)[43;9Hvariance_plot, = plt.plot(self.variance, color=[31m'b'[0m)[44;9Hscore_plot, = plt.plot(self.outer_scores, color=[31m'r'[0m)[46;9Hplt.legend([variance_plot, score_plot],[47;20H[[31m"Variance"[0m, [31m"Score"[0m],[48;20Hbbox_to_anchor=([31m0[0m, [31m.4[0m, [31m.5[0m, [31m0[0m))[50;9Hplt.title([31m"{0}: Score VS Variance"[0m.format([36mtype[0m(self.model).__name__),[51;19Hx=[31m.5[0m, y=[31m1.1[0m, fontsize=[31m"15"[0m)[52;194H365,14[8CBot[34;14H[34h[?25h[?25l[52;199H5[34;15H[34h[?25h[?25l[52;199H6[34;16H[34h[?25h[?25l[52;199H7[34;17H[34h[?25h[?25l[52;199H8[34;18H[34h[?25h[?25l[52;199H9[34;19H[34h[?25h[?25l[52;198H20[34;20H[34h[?25h[?25l[52;199H1[34;21H[34h[?25h[?25l[52;199H2[34;22H[34h[?25h[?25l[52;199H3[34;23H[34h[?25h[?25l[52;1H[1m-- INSERT --[0m[52;194H[K[52;194H365,23[8CBot[34;23H[34h[?25h[?25l = goal_list[34;34H[K[52;199H2[34;22H[34h[?25h[?25l = goal_list[34;33H[K[52;199H1[34;21H[34h[?25h[?25l = goal_list[34;32H[K[52;199H0[34;20H[34h[?25h[?25l = goal_list[34;31H[K[52;198H19[34;19H[34h[?25h[?25ld = goal_list[52;198H20[34;20H[34h[?25h[?25li = goal_list[52;199H1[34;21H[34h[?25h[?25lc = goal_list[52;199H2[34;22H[34h[?25h[?25lt = goal_list[52;199H3[34;23H[34h[?25h[?25l[52;199H4[34;24H[34h[?25h[?25l[52;199H5[34;25H[34h[?25h[?25l[52;199H6[34;26H[34h[?25h[?25l[52;199H7[34;27H[34h[?25h[?25l[52;199H8[34;28H[34h[?25h[?25l[52;199H9[34;29H[34h[?25h[?25l[52;198H30[34;30H[34h[?25h[?25l[52;199H1[34;31H[34h[?25h[?25l[52;199H2[34;32H[34h[?25h[?25l[52;199H3[34;33H[34h[?25h[?25l[52;199H4[34;34H[34h[?25h[?25l[52;199H5[34;35H[34h[?25h[?25l[34;34H[K[52;199H4[34;34H[34h[?25h[?25l[34;33H[K[52;199H3[34;33H[34h[?25h[?25l[34;32H[K[52;199H2[34;32H[34h[?25h[?25l[34;31H[K[52;199H1[34;31H[34h[?25h[?25ld[52;199H2[34;32H[34h[?25h[?25li[52;199H3[34;33H[34h[?25h[?25lc[52;199H4[34;34H[34h[?25h[?25lt[52;199H5[34;35H[34h[?25h[?25l[52;196H6[35;35H[34h[?25h[?25l[35;34H[K[52;199H4[35;34H[34h[?25h[?25l[35;33H[K[52;199H3[35;33H[34h[?25h[?25l[35;32H[K[52;199H2[35;32H[34h[?25h[?25l[35;31H[K[52;199H1[35;31H[34h[?25h[?25ld[52;199H2[35;32H[34h[?25h[?25li[52;199H3[35;33H[34h[?25h[?25lc[52;199H4[35;34H[34h[?25h[?25lt[52;199H5[35;35H[34h[?25h[?25l[52;199H4[35;34H[34h[?25h[?25l[52;199H3[35;33H[34h[?25h[?25l[52;199H2[35;32H[34h[?25h[?25l[52;199H1[35;31H[34h[?25h[?25l[52;199H0[35;30H[34h[?25h[?25l[52;198H29[35;29H[34h[?25h[?25l[52;199H8[35;28H[34h[?25h[?25l[52;199H7[35;27H[34h[?25h[?25l[52;199H6[35;26H[34h[?25h[?25l[52;199H5[35;25H[34h[?25h[?25l[52;199H4[35;24H[34h[?25h[?25l[52;199H3[35;23H[34h[?25h[?25l = time_dict[35;34H[K[52;199H2[35;22H[34h[?25h[?25l = time_dict[35;33H[K[52;199H1[35;21H[34h[?25h[?25l = time_dict[35;32H[K[52;199H0[35;20H[34h[?25h[?25l = time_dict[35;31H[K[52;198H19[35;19H[34h[?25h[?25ld = time_dict[52;198H20[35;20H[34h[?25h[?25li = time_dict[52;199H1[35;21H[34h[?25h[?25lc = time_dict[52;199H2[35;22H[34h[?25h[?25lt = time_dict[52;199H3[35;23H[34h[?25h[52;1H[K[35;22H[?25l[52;194H366,22[8CBot[35;22H[34h[?25h[?25l[52;194H[K[52;1H:[34h[?25hx[?25l[34h[?25h[?25l"/external_storage/ciaran/Library/Python/3.7/python/site-packages/nested_cv/nested_cv.py" 382L, 18005C written
[?1l>[34h[?25h[?1049l(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ ~/j
total 308K
-rw-rw-r-- 1 ciaran ciaran  84K Dec 19 17:18 test_boxplotting.txt
-rw-rw-r-- 1 ciaran ciaran  23K Dec 19 17:12 cv_grid_all_ml.py
-rw-rw-r-- 1 ciaran ciaran  18K Dec 19 16:06 nested_cv_new_name.py
-rw-rw-r-- 1 ciaran ciaran  222 Dec 10 17:40 done
-rw-rw-r-- 1 ciaran ciaran 1.9K Dec  9 11:12 blup_cv.sh
-rwxrwxrwx 1 ciaran ciaran  367 Dec  6 13:19 r2_score.py
(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ vi cv_grid_all_ml.py
[?1049h[?1h=[1;52r[34l[34h[?25h[23m[24m[0m[H[J[?25l[52;1H"cv_grid_all_ml.py" 389L, 22956C[1;1H[34m#Warning : best model selected by NMAE and R2 might not be the same
#performs linear regression, linear regression, neural network, svm and random forest, LASSO, RIDGE, CNN
#source ~/venv/bin/activate #in python 3.5.2
#print a log to a .txt file!
#model = pickle.load(open('FILEPATH', 'rb')) 
#dependencies = {'coeff_determination':coeff_determination}
#model = tf.keras.models.load_model('FILEPATH', custom_objects=dependencies)
#

#import tensorflow
#import numpy as np; import scipy #need to do this before path insert
#import sys
#sys.path.insert(1, '/external_storage/ciaran/Library/Python/3.7/python/site-packages/')
#import dill as pickle
#sys.path.insert(1, '/external_storage/ciaran/Library/Python/3.7/python/site-packages/nested_cv')
#from nested_cv import NestedCV
#with open('NCV_NN.pkl', 'rb') as f:
#     red = pickle.load(f)[0m

[36mprint[0m([31m"Please remember to set the right set size in the nested_cv code"[0m)
[35mimport[0m sys
sys.path.insert([31m1[0m, [31m'/external_storage/ciaran/Library/Python/3.7/python/site-packages/nested_cv'[0m)
num = sys.argv[[31m1[0m] [34m#script number for saving out[0m
phenfile = [36mstr[0m(sys.argv[[31m2[0m]) [34m#txt file with phenotypes[0m
data = [36mstr[0m(sys.argv[[31m3[0m]) [34m#needs to be same size as set_size[0m
snps = [36mstr[0m(sys.argv[[31m4[0m]) [34m#top or shuf[0m
phenotype = [36mstr[0m(sys.argv[[31m5[0m]) [34m#make a directory for the results[0m
set_size = [36mint[0m(sys.argv[[31m6[0m]) [34m#how many SNPs[0m
[35mfrom[0m nested_cv [35mimport[0m NestedCV
[35mimport[0m statistics
[35mimport[0m numpy [33mas[0m np
[35mimport[0m sklearn
[35mimport[0m seaborn [33mas[0m sns
[35mfrom[0m sklearn [35mimport[0m preprocessing
[35mfrom[0m sklearn.pipeline [35mimport[0m make_pipeline, Pipeline
[35mfrom[0m sklearn.preprocessing [35mimport[0m StandardScaler
[35mimport[0m datetime
[35mimport[0m time
[35mfrom[0m statistics [35mimport[0m mean
[35mimport[0m matplotlib.pyplot [33mas[0m plt
[35mfrom[0m matplotlib [35mimport[0m style
[35mfrom[0m sklearn.svm [35mimport[0m LinearSVR
[35mfrom[0m sklearn.svm [35mimport[0m SVR
[35mfrom[0m sklearn.model_selection [35mimport[0m train_test_split
[35mfrom[0m sklearn.model_selection [35mimport[0m RandomizedSearchCV
[35mfrom[0m sklearn.linear_model [35mimport[0m LinearRegression, Lasso, Ridge
[34m#import pickle #use dill instead below[0m
[35mfrom[0m sklearn.ensemble [35mimport[0m RandomForestRegressor [34m#based in part on https://towardsdatascience.com/random-forest-in-python-24d0893d51c0[0m
[35mfrom[0m statistics [35mimport[0m mean
[35mimport[0m os
[35mimport[0m pandas [33mas[0m pd [34m# data processing, CSV file I/O (e.g. pd.read_csv)[0m[52;194H1,1[11CTop[1;1H[34h[?25h[?25l[52;1H[K[52;1H:[34h[?25hq[?25l[34h[?25h![?25l[34h[?25h[?25l[52;1H[K[52;1H[?1l>[34h[?25h[?1049l(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ vi nested_cv_new_name.py
[?1049h[?1h=[1;52r[34l[34h[?25h[23m[24m[0m[H[J[?25l[52;1H"nested_cv_new_name.py" 382L, 18005C[1;1H[35mimport[0m logging [33mas[0m log
[35mimport[0m pandas [33mas[0m pd
[35mimport[0m numpy [33mas[0m np
[35mfrom[0m matplotlib [35mimport[0m pyplot [33mas[0m plt
[35mfrom[0m sklearn.model_selection [35mimport[0m KFold, ParameterGrid, ParameterSampler
[35mfrom[0m sklearn.metrics [35mimport[0m mean_squared_error
[35mfrom[0m sklearn.feature_selection [35mimport[0m RFECV
[35mfrom[0m sklearn.utils.multiclass [35mimport[0m type_of_target
[35mfrom[0m joblib [35mimport[0m Parallel, delayed
[35mimport[0m os.path
[35mimport[0m time
[35mimport[0m subprocess
[35mimport[0m sys
[35mimport[0m sklearn
[35mfrom[0m sklearn [35mimport[0m preprocessing
[35mimport[0m os
[36mprint[0m(os.getcwd())
[36mprint[0m([31m"WARNING THIS IS AN EDITED SCRIPT - Ciaran Kelly 2021 [0m[35m\n[0m[31m Edited with permission under liscence [0m[35m\n[0m[31m Flex version"[0m)
[34m#set_size = 10006    
#print("Set size set to %s" % set_size)[0m

[33mdef[0m [36mload_data[0m(data, set_size):[23;9H[36mprint[0m([31m"Set size set to %s"[0m % set_size)[24;9Hdataset = np.loadtxt(data, skiprows=[31m1[0m)[25;9Hx = dataset[: , [31m6[0m:(set_size+[31m6[0m)]/[31m2[0m[26;9Hy = dataset[: , [31m5[0m ][27;9Hy = y.reshape(-[31m1[0m,[31m1[0m)[28;9H[34m#print("Performing split of raw data....")[29;9H#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.8, random_state=42)[0m[30;9H[33mreturn[0m x, y [34m#x_train, y_train, x_test, y_test[0m


[33mdef[0m [36mbash_script[0m(train_index, test_index, train_names, test_names, outer_count, inner_count, phenfile, set_size, snps, outer=[36mFalse[0m):[34;9H[36mprint[0m(os.getcwd())[35;9H[33mif[0m outer==[36mTrue[0m:[36;13Hfoo=[31m'out'[0m[37;9H[33melse[0m:[38;13Hfoo=[31m'in'[0m[39;9H[33mif[0m [33mnot[0m os.path.exists([31m'train_raw_plink_'[0m + [36mstr[0m(snps) + [31m'_'[0m + [36mstr[0m(outer_count) + [31m'_in_'[0m + [36mstr[0m(inner_count) + [31m'_'[0m + foo + [31m'.raw'[0m):[40;13H[36mprint[0m([31m"SETTING OFF CUSTOM BASH SCRIPT"[0m)[41;13H[33mwith[0m [36mopen[0m([31m"name_vector_train.txt"[0m, [31m'w'[0m) [33mas[0m f:[42;17H[33mfor[0m item [33min[0m train_names:[43;21Hf.write([31m"%s %s[0m[35m\n[0m[31m"[0m % (item, item))[44;13H[33mwith[0m [36mopen[0m([31m"name_vector_test.txt"[0m, [31m'w'[0m) [33mas[0m f:[45;17H[33mfor[0m item [33min[0m test_names:[46;21Hf.write([31m"%s %s[0m[35m\n[0m[31m"[0m % (item, item))[48;13Hsubprocess.run([[31m"/external_storage/ciaran/machine_learning2/bash_script.sh"[0m, [36mstr[0m(outer_count), [36mstr[0m(inner_count), foo, [36mstr[0m(phenfile), [36mstr[0m(set_size), [36mstr[0m(snps)])[49;9H[34m#while not os.path.exists('train_raw_plink_shuf_' + str(outer_count) + '_in_' + str(inner_count) + '.raw'):[0m[50;9H[33mwhile[0m [33mnot[0m os.path.exists([31m'test_raw_plink_'[0m +  [36mstr[0m(snps) +  [31m'_'[0m + [36mstr[0m(outer_count) + [31m'_in_'[0m + [36mstr[0m(inner_count) + [31m'_'[0m + foo + [31m'.raw'[0m):[51;13Htime.sleep([31m20[0m)[52;194H1,1[11CTop[1;1H[34h[?25h[?25l[52;1H[K[52;1H/[34h[?25hg[?25l[34h[?25ho[?25l[34h[?25hl[?25l[34h[?25h[?25l[52;4H[K[52;4H[34h[?25ha[?25l[34h[?25h;[?25l[34h[?25h[?25l[52;5H[K[52;5H[34h[?25hl[?25l[34h[?25h_[?25l[34h[?25hl[?25l[34h[?25hi[?25l[34h[?25hs[?25l[34h[?25ht[?25l[34h[?25h[?25l[23m[24m[0m[H[J[2;13Hbest_inner_score, best_inner_params = self._best_of_results(search_scores)[4;13Hbest_inner_params_list.append(best_inner_params)[5;13Hbest_inner_score_list.append(best_inner_score)[6;13H[34m#inner_count = 0[0m[7;13H[36mprint[0m([31m"OUTER COUNT NO. "[0m, [36mstr[0m(outer_count))[8;13H[34m# Fit the best hyperparameters from one of the K inner loops[0m[9;13Hself.model.set_params(**best_inner_params)[10;13HX_train_outer, X_test_outer, y_train_outer, y_test_outer = bash_script(train_index, test_index, outer_train_names, outer_test_names, outer_count, inner_count, phenfile, set_size, snps, outer=[36mTrue[0m)[11;13Houter_count += [31m1[0m[12;13H[33mif[0m model_name == [31m'CNN'[0m:[13;17HX_train_outer = X_train_outer.reshape(X_train_outer.shape[[31m0[0m],X_train_outer.shape[[31m1[0m],[31m1[0m)[14;17HX_test_outer = X_test_outer.reshape(X_test_outer.shape[[31m0[0m],X_test_outer.shape[[31m1[0m],[31m1[0m)[15;13Hself.model.fit(X_train_outer, y_train_outer.ravel())[17;13H[34m# Get score and prediction[0m[18;13Hscore,pred = self._predict_and_score(X_test_outer, y_test_outer.ravel())[19;13Houter_scores.append(self._transform_score_format(score))[21;13H[34m# Append variance[0m[22;13Hvariance.append(np.var(pred, ddof=[31m1[0m))[24;13Hlog.debug([31m'[0m[35m\n[0m[31mResults for outer fold:[0m[35m\n[0m[31mBest inner parameters was: {0}'[0m.format([25;17Hbest_inner_params_list[i]))[26;13Hlog.debug([31m'Outer score: {0}'[0m.format(outer_scores[i]))[27;13Hlog.debug([31m'Inner score: {0}'[0m.format(best_inner_score_list[i]))[29;9Hself.variance = variance[30;9Hself.outer_scores = outer_scores[31;9Hself.best_inner_score_list = best_inner_score_list[32;9Hself.best_params = self._score_to_best_params(best_inner_params_list)[33;9Hself.best_inner_params_list = best_inner_params_list[34;9Hself.goal_list = goal_list[35;9Hself.time_list = time_list[37;5H[34m# Method to show score vs variance chart. You can run it only after fitting the model.[0m
    [33mdef[0m [36mscore_vs_variance_plot[0m(self):[39;9H[34m# Plot score vs variance[0m[40;9Hplt.figure()[41;9Hplt.subplot([31m211[0m)[43;9Hvariance_plot, = plt.plot(self.variance, color=[31m'b'[0m)[44;9Hscore_plot, = plt.plot(self.outer_scores, color=[31m'r'[0m)[46;9Hplt.legend([variance_plot, score_plot],[47;20H[[31m"Variance"[0m, [31m"Score"[0m],[48;20Hbbox_to_anchor=([31m0[0m, [31m.4[0m, [31m.5[0m, [31m0[0m))[50;9Hplt.title([31m"{0}: Score VS Variance"[0m.format([36mtype[0m(self.model).__name__),[51;19Hx=[31m.5[0m, y=[31m1.1[0m, fontsize=[31m"15"[0m)[52;194H365,14[8CBot[34;14H[34h[?25h[?25l[52;199H5[34;15H[34h[?25h[?25l[52;199H6[34;16H[34h[?25h[?25l[52;199H7[34;17H[34h[?25h[?25l[52;199H8[34;18H[34h[?25h[?25l[52;199H9[34;19H[34h[?25h[?25l[52;198H20[34;20H[34h[?25h[?25l[52;199H1[34;21H[34h[?25h[?25l[52;199H2[34;22H[34h[?25h[?25l[52;199H3[34;23H[34h[?25h[?25l[52;1H[1m-- INSERT --[0m[52;194H[K[52;194H365,23[8CBot[34;23H[34h[?25h[?25l = goal_list[34;34H[K[52;199H2[34;22H[34h[?25h[?25l = goal_list[34;33H[K[52;199H1[34;21H[34h[?25h[?25l = goal_list[34;32H[K[52;199H0[34;20H[34h[?25h[?25l = goal_list[34;31H[K[52;198H19[34;19H[34h[?25h[?25ld = goal_list[52;198H20[34;20H[34h[?25h[?25li = goal_list[52;199H1[34;21H[34h[?25h[?25lc = goal_list[52;199H2[34;22H[34h[?25h[?25lt = goal_list[52;199H3[34;23H[34h[?25h[?25l[52;199H4[34;24H[34h[?25h[?25l[52;199H5[34;25H[34h[?25h[?25l[52;199H6[34;26H[34h[?25h[?25l[52;199H7[34;27H[34h[?25h[?25l[52;199H8[34;28H[34h[?25h[?25l[52;199H9[34;29H[34h[?25h[?25l[52;198H30[34;30H[34h[?25h[?25l[52;199H1[34;31H[34h[?25h[?25l[52;199H2[34;32H[34h[?25h[?25l[52;199H3[34;33H[34h[?25h[?25l[52;199H4[34;34H[34h[?25h[?25l[52;199H5[34;35H[34h[?25h[?25l[34;34H[K[52;199H4[34;34H[34h[?25h[?25l[34;33H[K[52;199H3[34;33H[34h[?25h[?25l[34;32H[K[52;199H2[34;32H[34h[?25h[?25l[34;31H[K[52;199H1[34;31H[34h[?25h[?25ld[52;199H2[34;32H[34h[?25h[?25li[52;199H3[34;33H[34h[?25h[?25lc[52;199H4[34;34H[34h[?25h[?25lt[52;199H5[34;35H[34h[?25h[?25l[52;196H6[35;35H[34h[?25h[?25l[35;34H[K[52;199H4[35;34H[34h[?25h[?25l[35;33H[K[52;199H3[35;33H[34h[?25h[?25l[35;32H[K[52;199H2[35;32H[34h[?25h[?25l[35;31H[K[52;199H1[35;31H[34h[?25h[?25li[52;199H2[35;32H[34h[?25h[?25ld[52;199H3[35;33H[34h[?25h[?25l[35;32H[K[52;199H2[35;32H[34h[?25h[?25l[35;31H[K[52;199H1[35;31H[34h[?25h[?25ld[52;199H2[35;32H[34h[?25h[?25lc[52;199H3[35;33H[34h[?25h[?25l[35;32H[K[52;199H2[35;32H[34h[?25h[?25li[52;199H3[35;33H[34h[?25h[?25lc[52;199H4[35;34H[34h[?25h[?25lt[52;199H5[35;35H[34h[?25h[?25l[52;199H4[35;34H[34h[?25h[?25l[52;199H3[35;33H[34h[?25h[?25l[52;199H2[35;32H[34h[?25h[?25l[52;199H1[35;31H[34h[?25h[?25l[52;199H0[35;30H[34h[?25h[?25l[52;198H29[35;29H[34h[?25h[?25l[52;199H8[35;28H[34h[?25h[?25l[52;199H7[35;27H[34h[?25h[?25l[52;199H6[35;26H[34h[?25h[?25l[52;199H5[35;25H[34h[?25h[?25l[52;199H4[35;24H[34h[?25h[?25l[52;199H3[35;23H[34h[?25h[?25l = time_dict[35;34H[K[52;199H2[35;22H[34h[?25h[?25l = time_dict[35;33H[K[52;199H1[35;21H[34h[?25h[?25l = time_dict[35;32H[K[52;199H0[35;20H[34h[?25h[?25l = time_dict[35;31H[K[52;198H19[35;19H[34h[?25h[?25ld = time_dict[52;198H20[35;20H[34h[?25h[?25li = time_dict[52;199H1[35;21H[34h[?25h[?25lc = time_dict[52;199H2[35;22H[34h[?25h[?25lt = time_dict[52;199H3[35;23H[34h[?25h[52;1H[K[35;22H[?25l[52;194H366,22[8CBot[35;22H[34h[?25h[?25l[52;194H[K[52;1H:[34h[?25hx[?25l[34h[?25h[?25l"nested_cv_new_name.py" 382L, 18005C written
[?1l>[34h[?25h[?1049l(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ vi nested_cv_new_name.py[4Pcv_grid_all_ml.py~/j[Kvi /external_storage/ciaran/Library/Python/3.7/python/site-packages/nested_cv/nested_cv.py[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cpython cv_grid_all_ml.py 31 /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del /home/ciaran/completed_big_matrix_binary_new_snps_ids__first_10k.raw shuf FT10cv_new_10k 10000
Please remember to set the right set size in the nested_cv code
/external_storage/ciaran/machine_learning2
WARNING THIS IS AN EDITED SCRIPT - Ciaran Kelly 2021 
 Edited with permission under liscence 
 Flex version
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
31
/home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del
/home/ciaran/completed_big_matrix_binary_new_snps_ids_first_10k.raw
shuf
FT10cv_new_10k
10000
2021-12-19 17:19:00
Performing SVM
{'kernel': ['poly'], 'degree': [1, 2, 3], 'gamma': [0.1, 0.4, 0.7, 1.0], 'C': [1, 2]}
{'loss': ['epsilon_insensitive', 'squared_epsilon_insensitive'], 'C': [1, 2]}
{'kernel': ['poly'], 'degree': [1, 2, 3], 'gamma': [0.1, 0.4, 0.7, 1.0], 'C': [1, 2]}
{'loss': ['epsilon_insensitive', 'squared_epsilon_insensitive'], 'C': [1, 2]}
ADBUCE
INNER COUNT NO.  1
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_1_in_1_in.raw
Set size set to 10000
Set size set to 10000
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.
  % (grid_size, self.n_iter, grid_size), UserWarning)
{'loss': 'epsilon_insensitive', 'C': 1}
Blue
Red
0.18788975942491137
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 1}
Blue
Red
0.1904596389152493
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 2}
Blue
Red
0.18789832310388443
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 2}
Blue
Red
0.18921895808924982
Typer LinearSVR
INNER COUNT NO.  2
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_1_in_2_in.raw
Set size set to 10000
Set size set to 10000
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.
  % (grid_size, self.n_iter, grid_size), UserWarning)
{'loss': 'epsilon_insensitive', 'C': 1}
Blue
Red
0.40007393430857896
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 1}
Blue
Red
0.4022359349858281
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 2}
Blue
Red
0.4000787831049921
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 2}
Blue
Red
0.40119023747093474
Typer LinearSVR
OUTER COUNT NO.  1
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_1_in_2_out.raw
Set size set to 10000
Set size set to 10000
0.39627940625857716
INNER COUNT NO.  1
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_2_in_1_in.raw
Set size set to 10000
Set size set to 10000
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.
  % (grid_size, self.n_iter, grid_size), UserWarning)
{'loss': 'epsilon_insensitive', 'C': 1}
Blue
Red
0.40100215299231334
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 1}
Blue
Red
0.4027996392139388
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 2}
Blue
Red
0.401020067623584
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 2}
Blue
Red
0.40191572591611
Typer LinearSVR
INNER COUNT NO.  2
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_2_in_2_in.raw
Set size set to 10000
Set size set to 10000
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.
  % (grid_size, self.n_iter, grid_size), UserWarning)
{'loss': 'epsilon_insensitive', 'C': 1}
Blue
Red
0.42083726969875757
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 1}
Blue
Red
0.42290808225302967
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 2}
Blue
Red
0.42084888400690534
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 2}
Blue
Red
0.4218925269773851
Typer LinearSVR
OUTER COUNT NO.  2
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_2_in_2_out.raw
Set size set to 10000
Set size set to 10000
0.34396481901856946
Best Params of SVM is {'loss': ['squared_epsilon_insensitive'], 'C': [1]} 
Outer scores of SVM is [0.39627940625857716, 0.34396481901856946] and mean is 0.3701221126385733
Variance of SVM is [0.540167831991053, 0.4136620482633837] 
Performing Neural Network
Traceback (most recent call last):
  File "cv_grid_all_ml.py", line 341, in <module>
    NN_NCV = NestedCV(model_name='nn_model', name_list = name_list, model=nn_model, params_grid=param_grid, outer_kfolds=4, inner_kfolds=4, n_jobs = 32,cv_options={'randomized_search':True, 'randomized_search_iter':100, 'sqrt_of_score':False,'recursive_feature_elimination':False, 'metric':sklearn.metrics.r2_score, 'metric_score_indicator_lower':False})
TypeError: __init__() missing 2 required positional arguments: 'goal_dict' and 'time_dict'
(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ cv_grid_all_ml.pyvcv_grid_all_ml.pyicv_grid_all_ml.py cv_grid_all_ml.py
[?1049h[?1h=[1;52r[34l[34h[?25h[23m[24m[0m[H[J[?25l[52;1H"cv_grid_all_ml.py" 389L, 22956C[1;1H[34m#Warning : best model selected by NMAE and R2 might not be the same
#performs linear regression, linear regression, neural network, svm and random forest, LASSO, RIDGE, CNN
#source ~/venv/bin/activate #in python 3.5.2
#print a log to a .txt file!
#model = pickle.load(open('FILEPATH', 'rb')) 
#dependencies = {'coeff_determination':coeff_determination}
#model = tf.keras.models.load_model('FILEPATH', custom_objects=dependencies)
#

#import tensorflow
#import numpy as np; import scipy #need to do this before path insert
#import sys
#sys.path.insert(1, '/external_storage/ciaran/Library/Python/3.7/python/site-packages/')
#import dill as pickle
#sys.path.insert(1, '/external_storage/ciaran/Library/Python/3.7/python/site-packages/nested_cv')
#from nested_cv import NestedCV
#with open('NCV_NN.pkl', 'rb') as f:
#     red = pickle.load(f)[0m

[36mprint[0m([31m"Please remember to set the right set size in the nested_cv code"[0m)
[35mimport[0m sys
sys.path.insert([31m1[0m, [31m'/external_storage/ciaran/Library/Python/3.7/python/site-packages/nested_cv'[0m)
num = sys.argv[[31m1[0m] [34m#script number for saving out[0m
phenfile = [36mstr[0m(sys.argv[[31m2[0m]) [34m#txt file with phenotypes[0m
data = [36mstr[0m(sys.argv[[31m3[0m]) [34m#needs to be same size as set_size[0m
snps = [36mstr[0m(sys.argv[[31m4[0m]) [34m#top or shuf[0m
phenotype = [36mstr[0m(sys.argv[[31m5[0m]) [34m#make a directory for the results[0m
set_size = [36mint[0m(sys.argv[[31m6[0m]) [34m#how many SNPs[0m
[35mfrom[0m nested_cv [35mimport[0m NestedCV
[35mimport[0m statistics
[35mimport[0m numpy [33mas[0m np
[35mimport[0m sklearn
[35mimport[0m seaborn [33mas[0m sns
[35mfrom[0m sklearn [35mimport[0m preprocessing
[35mfrom[0m sklearn.pipeline [35mimport[0m make_pipeline, Pipeline
[35mfrom[0m sklearn.preprocessing [35mimport[0m StandardScaler
[35mimport[0m datetime
[35mimport[0m time
[35mfrom[0m statistics [35mimport[0m mean
[35mimport[0m matplotlib.pyplot [33mas[0m plt
[35mfrom[0m matplotlib [35mimport[0m style
[35mfrom[0m sklearn.svm [35mimport[0m LinearSVR
[35mfrom[0m sklearn.svm [35mimport[0m SVR
[35mfrom[0m sklearn.model_selection [35mimport[0m train_test_split
[35mfrom[0m sklearn.model_selection [35mimport[0m RandomizedSearchCV
[35mfrom[0m sklearn.linear_model [35mimport[0m LinearRegression, Lasso, Ridge
[34m#import pickle #use dill instead below[0m
[35mfrom[0m sklearn.ensemble [35mimport[0m RandomForestRegressor [34m#based in part on https://towardsdatascience.com/random-forest-in-python-24d0893d51c0[0m
[35mfrom[0m statistics [35mimport[0m mean
[35mimport[0m os
[35mimport[0m pandas [33mas[0m pd [34m# data processing, CSV file I/O (e.g. pd.read_csv)[0m[52;194H1,1[11CTop[1;1H[34h[?25h[?25l[52;1H[K[52;1H/[34h[?25hg[?25l[34h[?25ho[?25l[34h[?25ha[?25l[34h[?25hl[?25l[34h[?25h_[?25l[34h[?25hd[?25l[34h[?25hi[?25l[34h[?25hc[?25l[34h[?25ht[?25l[34h[?25h[?25l[23m[24m[0m[H[J[1;5Hrestored_model = deserialize(model)
    [33mif[0m training_config [33mis[0m [33mnot[0m [36mNone[0m:[3;9Hrestored_model.compile([4;13H**saving_utils.compile_args_from_training_config([5;17Htraining_config[6;13H)[7;9H)
    restored_model.set_weights(weights)
    [33mreturn[0m restored_model

[34m# Hotfix function[0m
[33mdef[0m [36mmake_keras_picklable[0m():[14;5H[33mdef[0m [36m__reduce__[0m(self):[15;9Hmodel_metadata = saving_utils.model_metadata(self)[16;9Htraining_config = model_metadata.get([31m"training_config"[0m, [36mNone[0m)[17;9Hmodel = serialize(self)[18;9Hweights = self.get_weights()[19;9H[33mreturn[0m (unpack, (model, training_config, weights))[21;5Hcls = Model
    cls.__reduce__ = __reduce__


[33mdef[0m [36mmake_param_box_plot[0m(goal_dict, time_dict):[26;9H[33mfor[0m param [33min[0m goal_dict:[27;17Hplt.subplots([31m1[0m,[31m2[0m,figsize=([31m12[0m,[31m8[0m))[28;17Hplt.subplot([31m121[0m)[29;17Hplt.boxplot(goal_dict[param].values(), bootstrap=[36mNone[0m,showmeans=[36mFalse[0m, meanline=[36mFalse[0m, notch=[36mTrue[0m,labels=goal_dict[param].keys()) [34m#orange line is median, green dotted line is mean[0m[30;17Hplt.xlabel([36mstr[0m(param).upper(), fontsize=[31m10[0m, fontweight=[31m'bold'[0m)[31;17Hplt.ylabel([31m'R^2'[0m, fontsize=[31m10[0m,fontweight=[31m'bold'[0m)[32;17Hplt.title([31m'R^2 Score vs %s'[0m % param, fontsize=[31m14[0m, fontweight=[31m'bold'[0m)[33;17H[33mif[0m param == [31m'initialization'[0m:[34;25Hplt.xticks(fontsize=[31m6[0m)[35;17Hplt.subplot([31m122[0m)[36;17Hplt.boxplot(time_dict[param].values(), bootstrap=[36mNone[0m,showmeans=[36mFalse[0m, meanline=[36mFalse[0m, notch=[36mFalse[0m,labels=time_dict[param].keys())[37;17Hplt.xlabel([36mstr[0m(param).upper(), fontsize=[31m10[0m, fontweight=[31m'bold'[0m)[38;17Hplt.ylabel([31m'Training Time'[0m, fontsize=[31m10[0m,fontweight=[31m'bold'[0m)[39;17Hplt.title([31m'Training Time vs %s'[0m % param, fontsize=[31m14[0m, fontweight=[31m'bold'[0m)[40;17Hplt.tight_layout(pad=[31m4[0m)[41;17H[33mif[0m param == [31m'initialization'[0m:[42;25Hplt.xticks(fontsize=[31m6[0m)[43;17Hmy_fig_name = [31m"plots_of_"[0m + [36mstr[0m(param) + [31m'_'[0m + [36mstr[0m([31m"{:%Y_%m_%d}"[0m.format(datetime.datetime.now())) + [31m'_'[0m +[36mstr[0m(snps) +[36mstr[0m(num)+ [31m".png"[0m[44;17Hplt.savefig(my_fig_name, dpi=[31m300[0m)[45;17Hplt.show()[46;17Hplt.clf()[47;17Hplt.close()

[33mdef[0m [36mmake_goal_dict[0m(whole_dict):[50;9H[36mprint[0m(whole_dict)[51;9Hgoal_dict = {key:{} [33mfor[0m key [33min[0m whole_dict}[52;194H167,25[8C42%[25;25H[34h[?25h[?25l[52;1H/goal_dict[52;194H[K[52;1H[193C168,15-22     42%[26;22H[34h[?25h[?25l[52;194H[K[52;1H[193C171,29[8C42%[29;29H[34h[?25h[?25l[52;194H[K[52;1H[193C171,122[7C42%[29;122H[34h[?25h[?25l[52;194H[K[52;1H[193C191,10[8C42%[49;10H[34h[?25h[?25l[52;194H[K[52;1H[193C193,2-9[7C42%[51;9H[34h[?25h[?25l[52;194H[K[52;1H[1;51r[1;1H[3M[1;52r[49;9H[33mfor[0m key [33min[0m whole_dict:[50;17H[33mfor[0m item [33min[0m whole_dict[key]:[51;25Hgoal_dict[key][item] = [][52;1H[K[52;194H196,4-25      42%[51;25H[34h[?25h[?25l
/goal_dict[52;194H[K[52;1H[1;51r[1;1H[5M[1;52r[47;9Htime_dict = {key:{} [33mfor[0m key [33min[0m whole_dict} [34m#both empty[0m[48;9H[33mfor[0m key [33min[0m whole_dict:[49;17H[33mfor[0m item [33min[0m whole_dict[key]:[50;25Htime_dict[key][item] = [][51;9H[33mreturn[0m goal_dict, time_dict[52;1H[K[52;194H201,9-16      44%[51;16H[34h[?25h[?25l
/goal_dict[52;194H[K[52;1H[23m[24m[0m[H[J[2;1H[33mdef[0m [36mnn_results[0m(analysis, ncv_object):[3;9H[36mprint[0m([31m"Best Params of %s is %s "[0m % (analysis, ncv_object.best_params))[4;9H[36mprint[0m([31m"Outer scores of %s is %s and mean is %s"[0m % (analysis, ncv_object.outer_scores, np.mean(ncv_object.outer_scores)))[5;9H[36mprint[0m([31m"Variance of %s is %s "[0m % (analysis, ncv_object.variance))[6;9Hnn_list = [ncv_object.best_inner_params_list, ncv_object.best_inner_score_list, ncv_object.best_params, ncv_object.metric, ncv_object.outer_scores, ncv_object.variance][7;9H[33mwith[0m [36mopen[0m([31m'NCV_'[0m + [36mstr[0m(analysis) + [31m'_'[0m +  [36mstr[0m(snps) + [31m'_'[0m + [36mstr[0m(phenotype) + [31m'_'[0m + [36mstr[0m(num) + [31m'.pkl'[0m, [31m'wb'[0m) [33mas[0m ncvfile:[8;17Hpickle.dump(nn_list, ncvfile) [34m#ncv_object = pickle.load(ncvfile)[0m[9;9Hncv_object.model.model.save([31m"model_"[0m + [36mstr[0m(analysis) + [31m'_'[0m +  [36mstr[0m(snps) + [31m'_'[0m + [36mstr[0m(phenotype) + [31m'_'[0m + [36mstr[0m(num) + [31m".h5"[0m)


[36mprint[0m([31m"Performing SVM"[0m)
c_param = [[31m1[0m,[31m2[0m]
gamma_param = [[36mfloat[0m(x) [33mfor[0m x [33min[0m np.linspace([31m0.1[0m, [31m1[0m, [31m4[0m)]


epsilon_param = [[36mfloat[0m(x) [33mfor[0m x [33min[0m np.linspace([31m0.1[0m, [31m1[0m, [31m4[0m)]
loss_param = [[31m'epsilon_insensitive'[0m, [31m'squared_epsilon_insensitive'[0m]
kernel_param = [[31m'poly'[0m]
degree = [[31m1[0m,[31m2[0m,[31m3[0m]
svm_random_grid = {[31m'gamma'[0m:gamma_param, [31m'C'[0m:c_param,[31m'kernel'[0m:kernel_param, [31m"degree"[0m:degree}
[36mprint[0m(svm_random_grid)
svm_random_grid2 = {[31m'C'[0m : c_param, [31m'loss'[0m:loss_param}
[36mprint[0m(svm_random_grid2)
rbg_goal_dict, rbg_time_dict = make_goal_dict(svm_random_grid)
svm_goal_dict, svm_time_dict = make_goal_dict(svm_random_grid2)
SVM_NCV = NestedCV(model_name=[31m'LinearSVR'[0m, name_list = name_list, model=LinearSVR(), goal_dict=svm_goal_dict, time_dict=svm_time_dict, params_grid=svm_random_grid2, outer_kfolds=[31m2[0m, inner_kfolds=[31m2[0m, n_jobs = [31m8[0m,cv__[28;1Hoptions={[31m'randomized_search'[0m:[36mTrue[0m, [31m'randomized_search_iter'[0m:[31m10[0m, [31m'sqrt_of_score'[0m:[36mFalse[0m,[31m'recursive_feature_elimination'[0m:[36mFalse[0m, [31m'metric'[0m:sklearn.metrics.r2_score, [31m'metric_score_indicator_lower'[0m:[36mFalse[0m})
SVM_NCV.fit(x_train, y_train.ravel(), name_list=name_list, phenfile=phenfile, set_size=set_size, snps=snps, model_name=[31m'SVM'[0m, goal_dict=svm_goal_dict, time_dict=svm_time_dict)


ncv_results([31m'SVM'[0m, SVM_NCV)
[31m'''
print("Performing RBG")
RBG_NCV = NestedCV(model_name='RBG', name_list=name_list, model=SVR(), params_grid=svm_random_grid, outer_kfolds=4, inner_kfolds=4, n_jobs = 8,cv_options={'randomized_search':True, 'randomized_search_iter':50, ''[36;1Hsqrt_of_score':False,'recursive_feature_elimination':False, 'metric':sklearn.metrics.r2_score, 'metric_score_indicator_lower':False})
RBG_NCV.fit(x_train, y_train.ravel(), name_list=name_list, phenfile=phenfile, set_size=set_size, snps=snps, model_name='RBG')
ncv_results('RBG', RBG_NCV)

print("Performing LASSO")
alpha = [0.0001, 0.001, 0.01, 0.1, 0.5, 1, 10, 100]
alpha_dict = {'alpha':alpha}
print(alpha_dict)
alpha_name_dict = {'alpha':"Alpha"}
LASS_NCV = NestedCV(model_name='LASS', name_list=name_list, model=Lasso(), params_grid=alpha_dict, outer_kfolds=4, inner_kfolds=4, n_jobs = 8,cv_options={'randomized_search':True, 'randomized_search_iter':50, 'ss[46;1Hqrt_of_score':False,'recursive_feature_elimination':False, 'metric':sklearn.metrics.r2_score, 'metric_score_indicator_lower':False})
LASS_NCV.fit(x_train, y_train.ravel(), name_list=name_list, phenfile=phenfile, set_size=set_size, snps=snps, model_name='LASS')
ncv_results('LASS', LASS_NCV)
print("Performing Ridge")
RIDGE_NCV = NestedCV(model_name='RIDGE', name_list=name_list, model=Ridge(), params_grid=alpha_dict, outer_kfolds=4, inner_kfolds=4, n_jobs = 8,cv_options={'randomized_search':True, 'randomized_search_iter':50,  [51;1H'sqrt_of_score':False,'recursive_feature_elimination':False, 'metric':sklearn.metrics.r2_score, 'metric_score_indicator_lower':False})[0m[52;194H245,5[9C64%[25;5H[34h[?25h[?25l[52;196H6[26;5H[34h[?25h[?25l[52;196H7[27;5H[34h[?25h[?25l[52;196H8[29;5H[34h[?25h[?25l[52;196H7[27;5H[34h[?25h[?25l[52;198H6[27;6H[34h[?25h[?25l[52;198H7[27;7H[34h[?25h[?25l[52;198H8[27;8H[34h[?25h[?25l[52;198H9[27;9H[34h[?25h[?25l[52;198H10[27;10H[34h[?25h[?25l[52;199H1[27;11H[34h[?25h[?25l[52;199H2[27;12H[34h[?25h[?25l[52;199H3[27;13H[34h[?25h[?25l[52;199H4[27;14H[34h[?25h[?25l[52;199H5[27;15H[34h[?25h[?25l[52;199H6[27;16H[34h[?25h[?25l[52;199H7[27;17H[34h[?25h[?25l[52;199H8[27;18H[34h[?25h[?25lV[46m([0m[191C__[28;1Ho[196C[46m)[0m[52;199H9[27;19H[34h[?25h[?25l([191C__[28;1Ho[196C)[52;198H20[27;20H[34h[?25h[?25l[52;199H1[27;21H[34h[?25h[?25l[52;199H2[27;22H[34h[?25h[?25l[52;199H3[27;23H[34h[?25h[?25l[52;199H4[27;24H[34h[?25h[?25l[52;199H5[27;25H[34h[?25h[?25l[52;199H6[27;26H[34h[?25h[?25l[52;199H7[27;27H[34h[?25h[?25l[52;199H8[27;28H[34h[?25h[?25l[52;199H9[27;29H[34h[?25h[?25l[52;198H30[27;30H[34h[?25h[?25l[52;199H1[27;31H[34h[?25h[?25l[52;199H2[27;32H[34h[?25h[?25l[52;199H3[27;33H[34h[?25h[?25l[52;199H4[27;34H[34h[?25h[?25l[52;199H5[27;35H[34h[?25h[?25l[52;199H6[27;36H[34h[?25h[?25l[52;199H7[27;37H[34h[?25h[?25l[52;199H8[27;38H[34h[?25h[?25l[52;199H9[27;39H[34h[?25h[?25l[52;198H40[27;40H[34h[?25h[?25l[52;199H1[27;41H[34h[?25h[?25l[52;199H2[27;42H[34h[?25h[?25l[52;199H3[27;43H[34h[?25h[?25l[52;199H4[27;44H[34h[?25h[?25l[52;199H5[27;45H[34h[?25h[?25l[52;199H6[27;46H[34h[?25h[?25l[52;199H7[27;47H[34h[?25h[?25l[52;199H8[27;48H[34h[?25h[?25l[52;199H9[27;49H[34h[?25h[?25l[52;198H50[27;50H[34h[?25h[?25l[52;199H1[27;51H[34h[?25h[?25l[52;199H2[27;52H[34h[?25h[?25l[52;199H3[27;53H[34h[?25h[?25l[52;199H4[27;54H[34h[?25h[?25l[52;199H5[27;55H[34h[?25h[?25l[52;199H6[27;56H[34h[?25h[?25l[52;199H7[27;57H[34h[?25h[?25l[52;199H8[27;58H[34h[?25h[?25l[52;199H9[27;59H[34h[?25h[?25l[52;198H60[27;60H[34h[?25h[?25l[52;199H1[27;61H[34h[?25h[?25l[52;199H2[27;62H[34h[?25h[?25l[52;199H3[27;63H[34h[?25h[?25l[52;199H4[27;64H[34h[?25h[?25l[52;199H5[27;65H[34h[?25h[?25l[52;199H6[27;66H[34h[?25h[?25l[52;199H7[27;67H[34h[?25h[?25l[52;199H8[27;68H[34h[?25h[?25l[52;199H9[27;69H[34h[?25h[?25l[52;198H70[27;70H[34h[?25h[?25l[52;199H1[27;71H[34h[?25h[?25l[52;199H2[27;72H[34h[?25h[?25l[52;199H3[27;73H[34h[?25h[?25l[52;199H4[27;74H[34h[?25h[?25l[52;199H5[27;75H[34h[?25h[?25l[52;199H6[27;76H[34h[?25h[?25l[52;199H7[27;77H[34h[?25h[?25l[52;199H8[27;78H[34h[?25h[?25l[52;199H9[27;79H[34h[?25h[?25l[52;198H80[27;80H[34h[?25h[?25l[52;199H1[27;81H[34h[?25h[?25lR[46m()[0m[127C__[28;1Ho[52;199H2[27;82H[34h[?25h[?25l[129C__[28;1Ho[52;199H3[27;83H[34h[?25h[?25l()[127C__[28;1Ho[52;199H4[27;84H[34h[?25h[?25l[52;199H5[27;85H[34h[?25h[?25l[52;199H6[27;86H[34h[?25h[?25l[52;199H7[27;87H[34h[?25h[?25l[52;199H8[27;88H[34h[?25h[?25l[52;199H9[27;89H[34h[?25h[?25l[52;198H90[27;90H[34h[?25h[?25l[52;199H1[27;91H[34h[?25h[?25l[52;199H2[27;92H[34h[?25h[?25l[52;199H3[27;93H[34h[?25h[?25l[52;199H4[27;94H[34h[?25h[?25l[52;199H5[27;95H[34h[?25h[?25l[52;199H6[27;96H[34h[?25h[?25l[52;199H7[27;97H[34h[?25h[?25l[52;199H8[27;98H[34h[?25h[?25l[52;199H9[27;99H[34h[?25h[?25l[52;198H100[27;100H[34h[?25h[?25l[52;200H1[27;101H[34h[?25h[?25l[52;200H2[27;102H[34h[?25h[?25l[52;200H3[27;103H[34h[?25h[?25l[52;200H4[27;104H[34h[?25h[?25l[52;200H5[27;105H[34h[?25h[?25l[52;200H6[27;106H[34h[?25h[?25l[52;200H7[27;107H[34h[?25h[?25l[52;200H8[27;108H[34h[?25h[?25l[52;200H9[27;109H[34h[?25h[?25l[52;199H10[27;110H[34h[?25h[?25l[52;200H1[27;111H[34h[?25h[?25l[52;200H2[27;112H[34h[?25h[?25l[52;200H3[27;113H[34h[?25h[?25l[52;200H4[27;114H[34h[?25h[?25l[52;200H5[27;115H[34h[?25h[?25l[52;200H6[27;116H[34h[?25h[?25l[52;200H7[27;117H[34h[?25h[?25l[52;200H8[27;118H[34h[?25h[?25l[52;200H9[27;119H[34h[?25h[?25l[52;199H20[27;120H[34h[?25h[?25l[52;200H1[27;121H[34h[?25h[?25l[52;200H2[27;122H[34h[?25h[?25l[52;200H3[27;123H[34h[?25h[?25l[52;200H4[27;124H[34h[?25h[?25l[52;200H5[27;125H[34h[?25h[?25l[52;200H6[27;126H[34h[?25h[?25l[52;200H7[27;127H[34h[?25h[?25l[52;200H8[27;128H[34h[?25h[?25l[52;200H9[27;129H[34h[?25h[?25l[52;199H30[27;130H[34h[?25h[?25l[52;200H1[27;131H[34h[?25h[?25l[52;200H2[27;132H[34h[?25h[?25l[52;200H3[27;133H[34h[?25h[?25l[52;200H4[27;134H[34h[?25h[?25l[52;200H5[27;135H[34h[?25h[?25l[52;200H6[27;136H[34h[?25h[?25l[52;200H7[27;137H[34h[?25h[?25l[52;200H8[27;138H[34h[?25h[?25l[52;200H9[27;139H[34h[?25h[?25l[52;199H40[27;140H[34h[?25h[?25l[52;194H[K[52;1H:[34h[?25hq[?25l[34h[?25h![?25l[34h[?25h[?25l[52;1H[K[52;1H[?1l>[34h[?25h[?1049l(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ vi cv_grid_all_ml.py
[?1049h[?1h=[1;52r[34l[34h[?25h[23m[24m[0m[H[J[?25l[52;1H"cv_grid_all_ml.py" 389L, 22956C[1;1H[34m#Warning : best model selected by NMAE and R2 might not be the same
#performs linear regression, linear regression, neural network, svm and random forest, LASSO, RIDGE, CNN
#source ~/venv/bin/activate #in python 3.5.2
#print a log to a .txt file!
#model = pickle.load(open('FILEPATH', 'rb')) 
#dependencies = {'coeff_determination':coeff_determination}
#model = tf.keras.models.load_model('FILEPATH', custom_objects=dependencies)
#

#import tensorflow
#import numpy as np; import scipy #need to do this before path insert
#import sys
#sys.path.insert(1, '/external_storage/ciaran/Library/Python/3.7/python/site-packages/')
#import dill as pickle
#sys.path.insert(1, '/external_storage/ciaran/Library/Python/3.7/python/site-packages/nested_cv')
#from nested_cv import NestedCV
#with open('NCV_NN.pkl', 'rb') as f:
#     red = pickle.load(f)[0m

[36mprint[0m([31m"Please remember to set the right set size in the nested_cv code"[0m)
[35mimport[0m sys
sys.path.insert([31m1[0m, [31m'/external_storage/ciaran/Library/Python/3.7/python/site-packages/nested_cv'[0m)
num = sys.argv[[31m1[0m] [34m#script number for saving out[0m
phenfile = [36mstr[0m(sys.argv[[31m2[0m]) [34m#txt file with phenotypes[0m
data = [36mstr[0m(sys.argv[[31m3[0m]) [34m#needs to be same size as set_size[0m
snps = [36mstr[0m(sys.argv[[31m4[0m]) [34m#top or shuf[0m
phenotype = [36mstr[0m(sys.argv[[31m5[0m]) [34m#make a directory for the results[0m
set_size = [36mint[0m(sys.argv[[31m6[0m]) [34m#how many SNPs[0m
[35mfrom[0m nested_cv [35mimport[0m NestedCV
[35mimport[0m statistics
[35mimport[0m numpy [33mas[0m np
[35mimport[0m sklearn
[35mimport[0m seaborn [33mas[0m sns
[35mfrom[0m sklearn [35mimport[0m preprocessing
[35mfrom[0m sklearn.pipeline [35mimport[0m make_pipeline, Pipeline
[35mfrom[0m sklearn.preprocessing [35mimport[0m StandardScaler
[35mimport[0m datetime
[35mimport[0m time
[35mfrom[0m statistics [35mimport[0m mean
[35mimport[0m matplotlib.pyplot [33mas[0m plt
[35mfrom[0m matplotlib [35mimport[0m style
[35mfrom[0m sklearn.svm [35mimport[0m LinearSVR
[35mfrom[0m sklearn.svm [35mimport[0m SVR
[35mfrom[0m sklearn.model_selection [35mimport[0m train_test_split
[35mfrom[0m sklearn.model_selection [35mimport[0m RandomizedSearchCV
[35mfrom[0m sklearn.linear_model [35mimport[0m LinearRegression, Lasso, Ridge
[34m#import pickle #use dill instead below[0m
[35mfrom[0m sklearn.ensemble [35mimport[0m RandomForestRegressor [34m#based in part on https://towardsdatascience.com/random-forest-in-python-24d0893d51c0[0m
[35mfrom[0m statistics [35mimport[0m mean
[35mimport[0m os
[35mimport[0m pandas [33mas[0m pd [34m# data processing, CSV file I/O (e.g. pd.read_csv)[0m[52;194H1,1[11CTop[1;1H[34h[?25h[?25l[23m[24m[0m[H[J[1;1HNN_NCV.fit(x_train, y_train.ravel(), name_list=name_list, phenfile=phenfile, set_size=set_size, snps=snps, model_name=[31m'NN'[0m)
nn_results([31m'NN'[0m, NN_NCV)

[36mprint[0m([31m"Performing a convulutional neural network"[0m)
[35mfrom[0m tensorboard.plugins.hparams [35mimport[0m api [33mas[0m hp
[35mimport[0m random
[35mfrom[0m tensorflow.keras.layers [35mimport[0m Dense, Conv1D, Flatten
cnn_param_grid = {[31m'model__epochs'[0m:[[31m200[0m],[31m'model__learning_rate'[0m : [[31m0.01[0m,[31m0.001[0m],[31m'model__HP_L1_REG'[0m : [[31m0.1[0m],[31m'model__HP_L2_REG'[0m : [[31m0.2[0m],[9;15H[31m'model__kernel_initializer'[0m : [[31m'glorot_uniform'[0m],[31m'model__activation'[0m : [[31m'tanh'[0m],[31m'model__HP_NUM_HIDDEN_LAYERS'[0m : [[31m3[0m],[10;15H[31m'model__units'[0m : [[31m200[0m], [31m'model__rate'[0m : [[36mfloat[0m([31m0[0m)],[31m'model__HP_OPTIMIZER'[0m : [[31m'SGD'[0m], [31m'model__batch_size'[0m: [[31m32[0m],[11;15H[31m'model__filters'[0m:[[31m1[0m,[31m2[0m],[31m'model__strides'[0m:[[31m1[0m,[31m2[0m],[31m'model__pool'[0m:[[31m1[0m,[31m2[0m],[31m'model__kernel'[0m:[[31m1[0m,[31m2[0m]}


cnn_param_grid = {[31m'epochs'[0m:[[31m200[0m, [31m100[0m, [31m500[0m],[31m'batch_size'[0m : [[31m16[0m,[31m64[0m,[31m128[0m], [31m'learning_rate'[0m : [[31m0.01[0m,[31m0.001[0m],[31m'HP_L1_REG'[0m : [[31m0.1[0m],[31m'HP_L2_REG'[0m : [[31m0.2[0m],[31m'kernel_initializer'[0m : [[31m'glorot_uniform'[0m],[31m'activation'[0m : [[31m'tanh'[0m],[31m'HPP[15;1H_NUM_HIDDEN_LAYERS'[0m : [[31m3[0m],[31m'units'[0m : [[31m200[0m], [31m'rate'[0m : [[36mfloat[0m([31m0[0m)],[31m'HP_OPTIMIZER'[0m : [[31m'SGD'[0m], [31m'filters'[0m:[[31m1[0m,[31m2[0m],[31m'strides'[0m:[[31m1[0m,[31m2[0m],[31m'pool'[0m:[[31m1[0m,[31m2[0m],[31m'kernel'[0m:[[31m1[0m,[31m2[0m]}

METRIC_ACCURACY = [31m'coeff_determination'[0m
[34m#not sure if strides is relevant[0m
[36mprint[0m(x_train.shape)
[34m#x_train = x_train.reshape(x_train.shape[0], 1, x_train.shape[1]) # You needs to reshape your input data according to Conv1D layer input format - (batch_size, steps, input_dim)
#x_train = x_train.reshape(x_train.shape[0],x_train.shape[1],1)
#x_test = x_test.reshape(x_test.shape[0],x_test.shape[1],1)
#x_val = x_val.reshape(x_val.shape[0],x_val.shape[1],1)
#x_test = x_test.reshape(x_test.shape[0], 1, x_test.shape[1]) # You needs to reshape your input data according to Conv1D layer input format - (batch_size, steps, input_dim)[0m
[36mprint[0m(x_train.shape)

[33mdef[0m [36mconv_model[0m(HP_OPTIMIZER, HP_NUM_HIDDEN_LAYERS, units, activation, learning_rate, HP_L1_REG, HP_L2_REG, rate, kernel_initializer,strides,pool,filters,kernel):[28;9Hopt = HP_OPTIMIZER[29;9H[33mif[0m HP_NUM_HIDDEN_LAYERS == [31m1[0m :[30;17H[36mprint[0m([31m"HP_NUM_HIDDEN_LAYERS is equal to 1; this could cause building problems"[0m)[31;9Hchosen_opt = [36mgetattr[0m(tf.keras.optimizers,opt)[32;9Hreg = tf.keras.regularizers.l1_l2(l1=HP_L1_REG, l2=HP_L2_REG)[33;9Hmodel = Sequential() [34m# Only use dropout on fully-connected layers, and implement batch normalization between convolutions.[0m[34;9Hmodel.add(Conv1D(filters=filters, strides=strides, input_shape=(x_train.shape[[31m1[0m],[31m1[0m), activation=activation, kernel_regularizer=reg, kernel_initializer=kernel_initializer, kernel_size=kernel))[35;9Hmodel.add(tf.keras.layers.MaxPool1D(pool_size=pool, strides=strides))[36;9H[33mfor[0m i [33min[0m [36mrange[0m(HP_NUM_HIDDEN_LAYERS-[31m1[0m):[37;17Hmodel.add(Conv1D(filters=filters, strides=strides, activation=activation, kernel_regularizer=reg, kernel_initializer=kernel_initializer, kernel_size=kernel))[38;17Hmodel.add(tf.keras.layers.MaxPool1D(pool_size=pool, strides=strides))[39;9Hmodel.add(Flatten())[40;9Hmodel.add(Dense([31m1[0m, activation=[31m'linear'[0m))[41;9Hmodel.compile(loss=[31m'mean_absolute_error'[0m,metrics=[[31m'accuracy'[0m, [31m'mae'[0m, coeff_determination],optimizer=chosen_opt(learning_rate=learning_rate))[42;9H[36mprint[0m([31m"Summary "[0m, model.summary())[43;9H[33mreturn[0m model


cnn_model = KerasRegressor(build_fn = conv_model,verbose=[31m0[0m, callbacks=[callback])
CNN_NCV = NestedCV(model_name=[31m'CNN'[0m, name_list=name_list,model=cnn_model, params_grid=cnn_param_grid, outer_kfolds=[31m4[0m, inner_kfolds=[31m4[0m, n_jobs = [31m16[0m,cv_options={[31m'randomized_search'[0m:[36mTrue[0m, [31m'randomized_search_iter'[0m:[31m500[0m[48;1H, [31m'sqrt_of_score'[0m:[36mFalse[0m,[31m'recursive_feature_elimination'[0m:[36mFalse[0m, [31m'metric'[0m:sklearn.metrics.r2_score, [31m'metric_score_indicator_lower'[0m:[36mFalse[0m})
CNN_NCV.fit(x_train, y_train.ravel(), name_list=name_list, phenfile=phenfile, set_size=set_size, snps=snps, model_name=[31m'CNN'[0m)
nn_results([31m'CNN'[0m, CNN_NCV)
[1m[34m~                                                                                                                                                                                                                  [0m[52;194H389,1[9CBot[50;1H[34h[?25h[?25l[52;196H8[49;1H[34h[?25h[?25l[52;196H7[47;1H[34h[?25h[?25l[52;196H6[46;1H[34h[?25h[?25l[52;196H5,1-8[45;8H[34h[?25h[?25l[52;196H4,1  [44;1H[34h[?25h[?25l[52;196H3[43;1H[34h[?25h[?25l[52;196H2[42;1H[34h[?25h[?25l[52;196H1[41;1H[34h[?25h[?25l[52;196H0[40;1H[34h[?25h[?25l[52;195H79[39;1H[34h[?25h[?25l[52;196H8[38;1H[34h[?25h[?25l[52;196H7[37;1H[34h[?25h[?25l[52;196H6[36;1H[34h[?25h[?25l[52;196H5[35;1H[34h[?25h[?25l[52;196H4[34;1H[34h[?25h[?25l[52;196H3[33;1H[34h[?25h[?25l[52;196H2[32;1H[34h[?25h[?25l[52;196H1[31;1H[34h[?25h[?25l[52;196H0[30;1H[34h[?25h[?25l[52;195H69[29;1H[34h[?25h[?25l[52;196H8[28;1H[34h[?25h[?25l[52;196H7[27;1H[34h[?25h[?25l[52;196H6,1-8[26;8H[34h[?25h[?25l[52;196H5,1  [25;1H[34h[?25h[?25l[52;196H4[24;1H[34h[?25h[?25l[52;196H3[23;1H[34h[?25h[?25l[52;196H2[22;1H[34h[?25h[?25l[52;196H1[21;1H[34h[?25h[?25l[52;196H0[20;1H[34h[?25h[?25l[52;195H59[19;1H[34h[?25h[?25l[52;196H8[18;1H[34h[?25h[?25l[52;196H7[17;1H[34h[?25h[?25l[52;196H6,0-1[16;1H[34h[?25h[?25l[52;196H5,1  [14;1H[34h[?25h[?25l[52;196H4,0-1[13;1H[34h[?25h[?25l[52;196H3[12;1H[34h[?25h[?25l[52;196H2,1-8[11;8H[34h[?25h[?25l[52;196H1[10;8H[34h[?25h[?25l[52;196H0[9;8H[34h[?25h[?25l[52;195H49,1  [8;1H[34h[?25h[?25l[52;196H8[7;1H[34h[?25h[?25l[52;196H7[6;1H[34h[?25h[?25l[52;196H6[5;1H[34h[?25h[?25l[52;196H5[4;1H[34h[?25h[?25l[52;196H4,0-1[3;1H[34h[?25h[?25l[52;196H3,1  [2;1H[34h[?25h[?25l[52;196H2[1;1H[34h[?25h[?25l[1;51r[1;1H[2L[1;52r[1;1HNN_NCV = NestedCV(model_name=[31m'nn_model'[0m, name_list = name_list, model=nn_model, params_grid=param_grid, outer_kfolds=[31m4[0m, inner_kfolds=[31m4[0m, n_jobs = [31m32[0m,cv_options={[31m'randomized_search'[0m:[36mTrue[0m, [31m'randomized_search_iter'[0m::[2;1H[31m100[0m, [31m'sqrt_of_score'[0m:[36mFalse[0m,[31m'recursive_feature_elimination'[0m:[36mFalse[0m, [31m'metric'[0m:sklearn.metrics.r2_score, [31m'metric_score_indicator_lower'[0m:[36mFalse[0m})[52;194H[K[52;194H341,1[9C99%[1;1H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[52;194H[K[52;194H340,0-1[7C99%[1;1H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[51;1H[1m[34m@                                                                                                                                                                                                                  [0m[52;194H[K[52;194H339,0-1[7C99%[1;1H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;1H[35mfrom[0m sklearn.model_selection [35mimport[0m cross_val_score[52;194H[K[52;194H338,1[9C99%[1;1H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[52;194H[K[52;194H337,0-1[7C98%[1;1H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;1Hnn_model = KerasRegressor(build_fn = build_nn, verbose=[31m1[0m, callbacks=[callback])[52;194H[K[52;194H336,1[9C98%[1;1H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;1H[34m#pipeline_keras = Pipeline([('model', regressor_keras)])[0m[52;194H[K[52;194H335,1[9C98%[1;1H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;1H[34m#regressor_keras = KerasRegressor(build_fn = build_nn, epochs=10, verbose=1, batch_size=32)[0m[52;194H[K[52;194H334,1[9C97%[1;1H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[52;194H[K[52;194H333,0-1[7C97%[1;1H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[52;194H[K[52;194H332,0-1[7C97%[1;1H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;9H[33mreturn[0m model[52;194H[K[52;194H331,1-8[7C97%[1;8H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;9H[36mprint[0m(model.summary())[52;194H[K[52;194H330,1-8[7C96%[1;8H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;9Hmodel.compile(loss=[31m'mean_absolute_error'[0m,metrics=[[31m'accuracy'[0m, [31m'mae'[0m, coeff_determination],optimizer=chosen_opt(learning_rate=learning_rate))[52;194H[K[52;194H329,1-8[7C96%[1;8H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;9Hmodel.add(Dense([31m1[0m, activation=[31m'linear'[0m))[52;194H[K[52;194H328,1-8[7C96%[1;8H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;25Hmodel.add(Dropout(rate=rate))[52;194H[K[52;194H327,1-8[7C95%[1;8H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;17H[33mif[0m rate != [31m0[0m:[52;194H[K[52;194H326,1-8[7C95%[1;8H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;17Hmodel.add(Dense(units=units, activation=activation, kernel_regularizer=reg, kernel_initializer=kernel_initializer))[52;194H[K[52;194H325,1-8[7C95%[1;8H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;9H[33mfor[0m i [33min[0m [36mrange[0m(HP_NUM_HIDDEN_LAYERS-[31m1[0m):[52;194H[K[52;194H324,1-8[7C95%[1;8H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;17Hmodel.add(Dropout(rate=rate))[52;194H[K[52;194H323,1-8[7C94%[1;8H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;9H[33mif[0m rate != [31m0[0m:[52;194H[K[52;194H322,1-8[7C94%[1;8H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;9Hmodel.add(Dense(units=units, activation=activation, kernel_regularizer=reg, kernel_initializer=kernel_initializer, input_shape=(x_train.shape[[31m1[0m],)))[52;194H[K[52;194H321,1-8[7C94%[1;8H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;9Hmodel = Sequential()[52;194H[K[52;194H320,1-8[7C93%[1;8H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;9Hreg = tf.keras.regularizers.l1_l2(l1=HP_L1_REG, l2=HP_L2_REG)[52;194H[K[52;194H319,1-8[7C93%[1;8H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;9Hchosen_opt = [36mgetattr[0m(tf.keras.optimizers,opt)[52;194H[K[52;194H318,1-8[7C93%[1;8H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;9Hopt = HP_OPTIMIZER[52;194H[K[52;194H317,1-8[7C92%[1;8H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;1H[33mdef[0m [36mbuild_nn[0m(HP_OPTIMIZER, HP_NUM_HIDDEN_LAYERS, units, activation, learning_rate, HP_L1_REG, HP_L2_REG, rate, kernel_initializer):[52;194H[K[52;194H316,1[9C92%[1;1H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[52;194H[K[52;194H315,0-1[7C92%[1;1H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;1Hmake_keras_picklable()[52;194H[K[52;194H314,1[9C92%[1;1H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;1Hcallback = tf.keras.callbacks.EarlyStopping(monitor=[31m'coeff_determination'[0m, patience=[31m20[0m, mode=[31m'max'[0m, baseline=[31m0.0[0m) [34m#min above 0[0m[52;194H[K[52;194H313,1[9C91%[1;1H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;1H[34m#tf.config.experimental_run_functions_eagerly(True) #needed to avoid error # tensorflow.python.eager.core._SymbolicException[0m[52;194H[K[52;194H312,1[9C91%[1;1H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;1Htf.config.threading.set_intra_op_parallelism_threads([31m64[0m)[52;194H[K[52;194H311,1[9C91%[1;1H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;1Htf.config.threading.set_inter_op_parallelism_threads([31m64[0m)[52;194H[K[52;194H310,1[9C90%[1;1H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;1HMETRIC_ACCURACY = coeff_determination[52;194H[K[52;194H309,1[9C90%[1;1H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[52;194H[K[52;194H308,0-1[7C90%[1;1H[34h[?25h[?25l[1;51r[1;1H[2L[1;52r[1;1Hparam_grid = {[31m'epochs'[0m : [[31m50[0m,[31m100[0m,[31m200[0m],[31m'batch_size'[0m : [[31m16[0m,[31m64[0m, [31m128[0m],[31m'learning_rate'[0m : [[31m0.01[0m, [31m0.001[0m, [31m0.0001[0m, [31m0.00001[0m],[31m'HP_L1_REG'[0m : [[31m1e-4[0m, [31m1e-2[0m, [31m0.1[0m, [31m1e-3[0m],[31m'HP_L2_REG'[0m : [[31m1e-8[0m, [31m0.2[0m, [31m1e-4[0m, [31m1e-2[0m], [31m'kernel_initializerr[2;1H'[0m : [[31m'glorot_uniform'[0m],[31m'activation'[0m : [[31m'tanh'[0m, [31m'relu'[0m],[31m'HP_NUM_HIDDEN_LAYERS'[0m : [[31m2[0m,[31m3[0m,[31m4[0m, [31m5[0m],[31m'units'[0m : [[31m200[0m, [31m400[0m, [31m1000[0m], [31m'rate'[0m : [[36mfloat[0m([31m0[0m), [31m0.1[0m, [31m0.2[0m, [31m0.5[0m],[31m'HP_OPTIMIZER'[0m : [[31m'Adam'[0m, [31m'SGD'[0m, [31m'Adagrad'[0m]}[51;1H[1m[34m@                                                                                                                                                                                                                  [0m[52;194H[K[52;194H307,1[9C89%[1;1H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;1H[36mprint[0m([31m"Performing Neural Network"[0m)[52;194H[K[52;194H306,1[9C89%[1;1H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;1H[35mimport[0m random[52;194H[K[52;194H305,1[9C89%[1;1H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[52;194H[K[52;194H304,0-1[7C89%[1;1H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[52;194H[K[52;194H303,0-1[7C88%[1;1H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;1H[31m'''[0m[52;194H[K[52;194H302,1[9C88%[1;1H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;1H[31mncv_results('baseline', BASELINE_NCV)[0m[52;194H[K[52;194H301,1[9C88%[1;1H[34h[?25h[?25l[52;194H[K[52;1H:[34h[?25hq[?25l[34h[?25h![?25l[34h[?25h[?25l[52;1H[K[52;1H[?1l>[34h[?25h[?1049l(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ ~/h[Kj
total 368K
-rw-rw-r-- 1 ciaran ciaran 140K Dec 19 17:21 test_boxplotting.txt
-rw-rw-r-- 1 ciaran ciaran  18K Dec 19 17:19 nested_cv_new_name.py
-rw-rw-r-- 1 ciaran ciaran  23K Dec 19 17:12 cv_grid_all_ml.py
-rw-rw-r-- 1 ciaran ciaran  222 Dec 10 17:40 done
-rw-rw-r-- 1 ciaran ciaran 1.9K Dec  9 11:12 blup_cv.sh
-rwxrwxrwx 1 ciaran ciaran  367 Dec  6 13:19 r2_score.py
(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ nested_cv_new_name.pyvnested_cv_new_name.pyinested_cv_new_name.py nested_cv_new_name.py
[?1049h[?1h=[1;52r[34l[34h[?25h[23m[24m[0m[H[J[?25l[52;1H"nested_cv_new_name.py" 382L, 18005C[1;1H[35mimport[0m logging [33mas[0m log
[35mimport[0m pandas [33mas[0m pd
[35mimport[0m numpy [33mas[0m np
[35mfrom[0m matplotlib [35mimport[0m pyplot [33mas[0m plt
[35mfrom[0m sklearn.model_selection [35mimport[0m KFold, ParameterGrid, ParameterSampler
[35mfrom[0m sklearn.metrics [35mimport[0m mean_squared_error
[35mfrom[0m sklearn.feature_selection [35mimport[0m RFECV
[35mfrom[0m sklearn.utils.multiclass [35mimport[0m type_of_target
[35mfrom[0m joblib [35mimport[0m Parallel, delayed
[35mimport[0m os.path
[35mimport[0m time
[35mimport[0m subprocess
[35mimport[0m sys
[35mimport[0m sklearn
[35mfrom[0m sklearn [35mimport[0m preprocessing
[35mimport[0m os
[36mprint[0m(os.getcwd())
[36mprint[0m([31m"WARNING THIS IS AN EDITED SCRIPT - Ciaran Kelly 2021 [0m[35m\n[0m[31m Edited with permission under liscence [0m[35m\n[0m[31m Flex version"[0m)
[34m#set_size = 10006    
#print("Set size set to %s" % set_size)[0m

[33mdef[0m [36mload_data[0m(data, set_size):[23;9H[36mprint[0m([31m"Set size set to %s"[0m % set_size)[24;9Hdataset = np.loadtxt(data, skiprows=[31m1[0m)[25;9Hx = dataset[: , [31m6[0m:(set_size+[31m6[0m)]/[31m2[0m[26;9Hy = dataset[: , [31m5[0m ][27;9Hy = y.reshape(-[31m1[0m,[31m1[0m)[28;9H[34m#print("Performing split of raw data....")[29;9H#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.8, random_state=42)[0m[30;9H[33mreturn[0m x, y [34m#x_train, y_train, x_test, y_test[0m


[33mdef[0m [36mbash_script[0m(train_index, test_index, train_names, test_names, outer_count, inner_count, phenfile, set_size, snps, outer=[36mFalse[0m):[34;9H[36mprint[0m(os.getcwd())[35;9H[33mif[0m outer==[36mTrue[0m:[36;13Hfoo=[31m'out'[0m[37;9H[33melse[0m:[38;13Hfoo=[31m'in'[0m[39;9H[33mif[0m [33mnot[0m os.path.exists([31m'train_raw_plink_'[0m + [36mstr[0m(snps) + [31m'_'[0m + [36mstr[0m(outer_count) + [31m'_in_'[0m + [36mstr[0m(inner_count) + [31m'_'[0m + foo + [31m'.raw'[0m):[40;13H[36mprint[0m([31m"SETTING OFF CUSTOM BASH SCRIPT"[0m)[41;13H[33mwith[0m [36mopen[0m([31m"name_vector_train.txt"[0m, [31m'w'[0m) [33mas[0m f:[42;17H[33mfor[0m item [33min[0m train_names:[43;21Hf.write([31m"%s %s[0m[35m\n[0m[31m"[0m % (item, item))[44;13H[33mwith[0m [36mopen[0m([31m"name_vector_test.txt"[0m, [31m'w'[0m) [33mas[0m f:[45;17H[33mfor[0m item [33min[0m test_names:[46;21Hf.write([31m"%s %s[0m[35m\n[0m[31m"[0m % (item, item))[48;13Hsubprocess.run([[31m"/external_storage/ciaran/machine_learning2/bash_script.sh"[0m, [36mstr[0m(outer_count), [36mstr[0m(inner_count), foo, [36mstr[0m(phenfile), [36mstr[0m(set_size), [36mstr[0m(snps)])[49;9H[34m#while not os.path.exists('train_raw_plink_shuf_' + str(outer_count) + '_in_' + str(inner_count) + '.raw'):[0m[50;9H[33mwhile[0m [33mnot[0m os.path.exists([31m'test_raw_plink_'[0m +  [36mstr[0m(snps) +  [31m'_'[0m + [36mstr[0m(outer_count) + [31m'_in_'[0m + [36mstr[0m(inner_count) + [31m'_'[0m + foo + [31m'.raw'[0m):[51;13Htime.sleep([31m20[0m)[52;194H1,1[11CTop[1;1H[34h[?25h[?25l[52;1H[K[52;1H/[34h[?25hp[?25l[34h[?25hn[?25l[34h[?25hg[?25l[34h[?25h[?25l[31msearch hit BOTTOM, continuing at TOP[0m[1m[37m[41mE486: Pattern not found: png[0m[52;29H[K[52;194H1,1[11CTop[1;1H[34h[?25h[?25l[52;1H[K[52;1H/[34h[?25h[?25l[52;1H[K[52;194H1,1[11CTop[1;1H[34h[?25h[?25l[52;194H[K[52;1H:[34h[?25hq[?25l[34h[?25h![?25l[34h[?25h[?25l[52;1H[K[52;1H[?1l>[34h[?25h[?1049l(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ vi nested_cv_new_name.py[Kvi cv_grid_all_ml.py
[?1049h[?1h=[1;52r[34l[34h[?25h[23m[24m[0m[H[J[?25l[52;1H"cv_grid_all_ml.py" 389L, 22956C[1;1H[34m#Warning : best model selected by NMAE and R2 might not be the same
#performs linear regression, linear regression, neural network, svm and random forest, LASSO, RIDGE, CNN
#source ~/venv/bin/activate #in python 3.5.2
#print a log to a .txt file!
#model = pickle.load(open('FILEPATH', 'rb')) 
#dependencies = {'coeff_determination':coeff_determination}
#model = tf.keras.models.load_model('FILEPATH', custom_objects=dependencies)
#

#import tensorflow
#import numpy as np; import scipy #need to do this before path insert
#import sys
#sys.path.insert(1, '/external_storage/ciaran/Library/Python/3.7/python/site-packages/')
#import dill as pickle
#sys.path.insert(1, '/external_storage/ciaran/Library/Python/3.7/python/site-packages/nested_cv')
#from nested_cv import NestedCV
#with open('NCV_NN.pkl', 'rb') as f:
#     red = pickle.load(f)[0m

[36mprint[0m([31m"Please remember to set the right set size in the nested_cv code"[0m)
[35mimport[0m sys
sys.path.insert([31m1[0m, [31m'/external_storage/ciaran/Library/Python/3.7/python/site-packages/nested_cv'[0m)
num = sys.argv[[31m1[0m] [34m#script number for saving out[0m
phenfile = [36mstr[0m(sys.argv[[31m2[0m]) [34m#txt file with phenotypes[0m
data = [36mstr[0m(sys.argv[[31m3[0m]) [34m#needs to be same size as set_size[0m
snps = [36mstr[0m(sys.argv[[31m4[0m]) [34m#top or shuf[0m
phenotype = [36mstr[0m(sys.argv[[31m5[0m]) [34m#make a directory for the results[0m
set_size = [36mint[0m(sys.argv[[31m6[0m]) [34m#how many SNPs[0m
[35mfrom[0m nested_cv [35mimport[0m NestedCV
[35mimport[0m statistics
[35mimport[0m numpy [33mas[0m np
[35mimport[0m sklearn
[35mimport[0m seaborn [33mas[0m sns
[35mfrom[0m sklearn [35mimport[0m preprocessing
[35mfrom[0m sklearn.pipeline [35mimport[0m make_pipeline, Pipeline
[35mfrom[0m sklearn.preprocessing [35mimport[0m StandardScaler
[35mimport[0m datetime
[35mimport[0m time
[35mfrom[0m statistics [35mimport[0m mean
[35mimport[0m matplotlib.pyplot [33mas[0m plt
[35mfrom[0m matplotlib [35mimport[0m style
[35mfrom[0m sklearn.svm [35mimport[0m LinearSVR
[35mfrom[0m sklearn.svm [35mimport[0m SVR
[35mfrom[0m sklearn.model_selection [35mimport[0m train_test_split
[35mfrom[0m sklearn.model_selection [35mimport[0m RandomizedSearchCV
[35mfrom[0m sklearn.linear_model [35mimport[0m LinearRegression, Lasso, Ridge
[34m#import pickle #use dill instead below[0m
[35mfrom[0m sklearn.ensemble [35mimport[0m RandomForestRegressor [34m#based in part on https://towardsdatascience.com/random-forest-in-python-24d0893d51c0[0m
[35mfrom[0m statistics [35mimport[0m mean
[35mimport[0m os
[35mimport[0m pandas [33mas[0m pd [34m# data processing, CSV file I/O (e.g. pd.read_csv)[0m[52;194H1,1[11CTop[1;1H[34h[?25h[?25l[52;1H[K[52;1H/[34h[?25hp[?25l[34h[?25hl[?25l[34h[?25ho[?25l[34h[?25ht[?25l[34h[?25h[?25l[193C40,11[9CTop[40;11H[34h[?25h[?25l[52;194H[K[52;1H[193C40,21[9CTop[40;21H[34h[?25h[?25l[52;194H[K[52;1H[193C41,9[10CTop[41;9H[34h[?25h[?25l[52;194H[K[52;1H[23m[24m[0m[H[J[1;5Hrestored_model = deserialize(model)
    [33mif[0m training_config [33mis[0m [33mnot[0m [36mNone[0m:[3;9Hrestored_model.compile([4;13H**saving_utils.compile_args_from_training_config([5;17Htraining_config[6;13H)[7;9H)
    restored_model.set_weights(weights)
    [33mreturn[0m restored_model

[34m# Hotfix function[0m
[33mdef[0m [36mmake_keras_picklable[0m():[14;5H[33mdef[0m [36m__reduce__[0m(self):[15;9Hmodel_metadata = saving_utils.model_metadata(self)[16;9Htraining_config = model_metadata.get([31m"training_config"[0m, [36mNone[0m)[17;9Hmodel = serialize(self)[18;9Hweights = self.get_weights()[19;9H[33mreturn[0m (unpack, (model, training_config, weights))[21;5Hcls = Model
    cls.__reduce__ = __reduce__


[33mdef[0m [36mmake_param_box_plot[0m(goal_dict, time_dict):[26;9H[33mfor[0m param [33min[0m goal_dict:[27;17Hplt.subplots([31m1[0m,[31m2[0m,figsize=([31m12[0m,[31m8[0m))[28;17Hplt.subplot([31m121[0m)[29;17Hplt.boxplot(goal_dict[param].values(), bootstrap=[36mNone[0m,showmeans=[36mFalse[0m, meanline=[36mFalse[0m, notch=[36mTrue[0m,labels=goal_dict[param].keys()) [34m#orange line is median, green dotted line is mean[0m[30;17Hplt.xlabel([36mstr[0m(param).upper(), fontsize=[31m10[0m, fontweight=[31m'bold'[0m)[31;17Hplt.ylabel([31m'R^2'[0m, fontsize=[31m10[0m,fontweight=[31m'bold'[0m)[32;17Hplt.title([31m'R^2 Score vs %s'[0m % param, fontsize=[31m14[0m, fontweight=[31m'bold'[0m)[33;17H[33mif[0m param == [31m'initialization'[0m:[34;25Hplt.xticks(fontsize=[31m6[0m)[35;17Hplt.subplot([31m122[0m)[36;17Hplt.boxplot(time_dict[param].values(), bootstrap=[36mNone[0m,showmeans=[36mFalse[0m, meanline=[36mFalse[0m, notch=[36mFalse[0m,labels=time_dict[param].keys())[37;17Hplt.xlabel([36mstr[0m(param).upper(), fontsize=[31m10[0m, fontweight=[31m'bold'[0m)[38;17Hplt.ylabel([31m'Training Time'[0m, fontsize=[31m10[0m,fontweight=[31m'bold'[0m)[39;17Hplt.title([31m'Training Time vs %s'[0m % param, fontsize=[31m14[0m, fontweight=[31m'bold'[0m)[40;17Hplt.tight_layout(pad=[31m4[0m)[41;17H[33mif[0m param == [31m'initialization'[0m:[42;25Hplt.xticks(fontsize=[31m6[0m)[43;17Hmy_fig_name = [31m"plots_of_"[0m + [36mstr[0m(param) + [31m'_'[0m + [36mstr[0m([31m"{:%Y_%m_%d}"[0m.format(datetime.datetime.now())) + [31m'_'[0m +[36mstr[0m(snps) +[36mstr[0m(num)+ [31m".png"[0m[44;17Hplt.savefig(my_fig_name, dpi=[31m300[0m)[45;17Hplt.show()[46;17Hplt.clf()[47;17Hplt.close()

[33mdef[0m [36mmake_goal_dict[0m(whole_dict):[50;9H[36mprint[0m(whole_dict)[51;9Hgoal_dict = {key:{} [33mfor[0m key [33min[0m whole_dict}[52;194H167,20[8C42%[25;20H[34h[?25h[?25l[52;1H/plot[52;194H[K[52;1H[193C169,24[8C42%[27;24H[34h[?25h[?25l[52;194H[K[52;1H[193C170,24[8C42%[28;24H[34h[?25h[?25l[52;194H[K[52;1H[193C171,24[8C42%[29;24H[34h[?25h[?25l[52;194H[K[52;1H[193C177,24[8C42%[35;24H[34h[?25h[?25l[52;194H[K[52;1H[193C178,24[8C42%[36;24H[34h[?25h[?25l[52;194H[K[52;1H[193C185,32[8C42%[43;32H[34h[?25h[?25l[52;194H[K[52;1H[31msearch hit BOTTOM, continuing at TOP[23m[24m[0m[H[J[1;1H[34m#sys.path.insert(1, '/external_storage/ciaran/Library/Python/3.7/python/site-packages/nested_cv')
#from nested_cv import NestedCV
#with open('NCV_NN.pkl', 'rb') as f:
#     red = pickle.load(f)[0m

[36mprint[0m([31m"Please remember to set the right set size in the nested_cv code"[0m)
[35mimport[0m sys
sys.path.insert([31m1[0m, [31m'/external_storage/ciaran/Library/Python/3.7/python/site-packages/nested_cv'[0m)
num = sys.argv[[31m1[0m] [34m#script number for saving out[0m
phenfile = [36mstr[0m(sys.argv[[31m2[0m]) [34m#txt file with phenotypes[0m
data = [36mstr[0m(sys.argv[[31m3[0m]) [34m#needs to be same size as set_size[0m
snps = [36mstr[0m(sys.argv[[31m4[0m]) [34m#top or shuf[0m
phenotype = [36mstr[0m(sys.argv[[31m5[0m]) [34m#make a directory for the results[0m
set_size = [36mint[0m(sys.argv[[31m6[0m]) [34m#how many SNPs[0m
[35mfrom[0m nested_cv [35mimport[0m NestedCV
[35mimport[0m statistics
[35mimport[0m numpy [33mas[0m np
[35mimport[0m sklearn
[35mimport[0m seaborn [33mas[0m sns
[35mfrom[0m sklearn [35mimport[0m preprocessing
[35mfrom[0m sklearn.pipeline [35mimport[0m make_pipeline, Pipeline
[35mfrom[0m sklearn.preprocessing [35mimport[0m StandardScaler
[35mimport[0m datetime
[35mimport[0m time
[35mfrom[0m statistics [35mimport[0m mean
[35mimport[0m matplotlib.pyplot [33mas[0m plt
[35mfrom[0m matplotlib [35mimport[0m style
[35mfrom[0m sklearn.svm [35mimport[0m LinearSVR
[35mfrom[0m sklearn.svm [35mimport[0m SVR
[35mfrom[0m sklearn.model_selection [35mimport[0m train_test_split
[35mfrom[0m sklearn.model_selection [35mimport[0m RandomizedSearchCV
[35mfrom[0m sklearn.linear_model [35mimport[0m LinearRegression, Lasso, Ridge
[34m#import pickle #use dill instead below[0m
[35mfrom[0m sklearn.ensemble [35mimport[0m RandomForestRegressor [34m#based in part on https://towardsdatascience.com/random-forest-in-python-24d0893d51c0[0m
[35mfrom[0m statistics [35mimport[0m mean
[35mimport[0m os
[35mimport[0m pandas [33mas[0m pd [34m# data processing, CSV file I/O (e.g. pd.read_csv)[0m
[35mfrom[0m sklearn.metrics [35mimport[0m make_scorer
[35mimport[0m tensorflow [33mas[0m tf
[35mfrom[0m tensorflow.python.keras [35mimport[0m backend [33mas[0m K
[35mfrom[0m tensorflow.keras.layers [35mimport[0m Dense, Dropout, Flatten [34m#.core[0m
[35mfrom[0m tensorflow.keras.optimizers [35mimport[0m SGD
[35mfrom[0m tensorflow.keras.layers [35mimport[0m Dropout
[35mfrom[0m sklearn.externals [35mimport[0m joblib
[35mfrom[0m tensorflow.keras.models [35mimport[0m Sequential, Model
[35mfrom[0m tensorflow.keras.layers [35mimport[0m Dense
[35mfrom[0m tensorflow.keras.wrappers.scikit_learn [35mimport[0m KerasRegressor [34m# or Classifier[0m
[35mfrom[0m sklearn.model_selection [35mimport[0m KFold
[35mfrom[0m tensorflow.keras.optimizers [35mimport[0m SGD
[35mfrom[0m tensorflow.keras.layers [35mimport[0m Dropout
[35mimport[0m random[52;194H40,11[10C4%[31msearch hit BOTTOM, continuing at TOP[0m[52;194H[K[52;194H40,11[10C4%[26;11H[34h[?25h[?25l[52;1H/plot[52;6H[K[52;1H[193C40,21[10C4%[26;21H[34h[?25h[?25l[52;194H[K[52;1H[193C41,9[11C4%[27;9H[34h[?25h[?25l[52;194H[K[52;1H[23m[24m[0m[H[J[1;5Hrestored_model = deserialize(model)
    [33mif[0m training_config [33mis[0m [33mnot[0m [36mNone[0m:[3;9Hrestored_model.compile([4;13H**saving_utils.compile_args_from_training_config([5;17Htraining_config[6;13H)[7;9H)
    restored_model.set_weights(weights)
    [33mreturn[0m restored_model

[34m# Hotfix function[0m
[33mdef[0m [36mmake_keras_picklable[0m():[14;5H[33mdef[0m [36m__reduce__[0m(self):[15;9Hmodel_metadata = saving_utils.model_metadata(self)[16;9Htraining_config = model_metadata.get([31m"training_config"[0m, [36mNone[0m)[17;9Hmodel = serialize(self)[18;9Hweights = self.get_weights()[19;9H[33mreturn[0m (unpack, (model, training_config, weights))[21;5Hcls = Model
    cls.__reduce__ = __reduce__


[33mdef[0m [36mmake_param_box_plot[0m(goal_dict, time_dict):[26;9H[33mfor[0m param [33min[0m goal_dict:[27;17Hplt.subplots([31m1[0m,[31m2[0m,figsize=([31m12[0m,[31m8[0m))[28;17Hplt.subplot([31m121[0m)[29;17Hplt.boxplot(goal_dict[param].values(), bootstrap=[36mNone[0m,showmeans=[36mFalse[0m, meanline=[36mFalse[0m, notch=[36mTrue[0m,labels=goal_dict[param].keys()) [34m#orange line is median, green dotted line is mean[0m[30;17Hplt.xlabel([36mstr[0m(param).upper(), fontsize=[31m10[0m, fontweight=[31m'bold'[0m)[31;17Hplt.ylabel([31m'R^2'[0m, fontsize=[31m10[0m,fontweight=[31m'bold'[0m)[32;17Hplt.title([31m'R^2 Score vs %s'[0m % param, fontsize=[31m14[0m, fontweight=[31m'bold'[0m)[33;17H[33mif[0m param == [31m'initialization'[0m:[34;25Hplt.xticks(fontsize=[31m6[0m)[35;17Hplt.subplot([31m122[0m)[36;17Hplt.boxplot(time_dict[param].values(), bootstrap=[36mNone[0m,showmeans=[36mFalse[0m, meanline=[36mFalse[0m, notch=[36mFalse[0m,labels=time_dict[param].keys())[37;17Hplt.xlabel([36mstr[0m(param).upper(), fontsize=[31m10[0m, fontweight=[31m'bold'[0m)[38;17Hplt.ylabel([31m'Training Time'[0m, fontsize=[31m10[0m,fontweight=[31m'bold'[0m)[39;17Hplt.title([31m'Training Time vs %s'[0m % param, fontsize=[31m14[0m, fontweight=[31m'bold'[0m)[40;17Hplt.tight_layout(pad=[31m4[0m)[41;17H[33mif[0m param == [31m'initialization'[0m:[42;25Hplt.xticks(fontsize=[31m6[0m)[43;17Hmy_fig_name = [31m"plots_of_"[0m + [36mstr[0m(param) + [31m'_'[0m + [36mstr[0m([31m"{:%Y_%m_%d}"[0m.format(datetime.datetime.now())) + [31m'_'[0m +[36mstr[0m(snps) +[36mstr[0m(num)+ [31m".png"[0m[44;17Hplt.savefig(my_fig_name, dpi=[31m300[0m)[45;17Hplt.show()[46;17Hplt.clf()[47;17Hplt.close()

[33mdef[0m [36mmake_goal_dict[0m(whole_dict):[50;9H[36mprint[0m(whole_dict)[51;9Hgoal_dict = {key:{} [33mfor[0m key [33min[0m whole_dict}[52;194H167,20[8C42%[25;20H[34h[?25h[?25l[52;1H/plot[52;194H[K[52;1H[193C169,24[8C42%[27;24H[34h[?25h[?25l[52;194H[K[52;1H[193C170,24[8C42%[28;24H[34h[?25h[?25l[52;196H1[29;24H[34h[?25h[?25l[52;196H2[30;24H[34h[?25h[?25l[52;196H3[31;24H[34h[?25h[?25l[52;196H4[32;24H[34h[?25h[?25l[52;196H5[33;24H[34h[?25h[?25l[52;196H6[34;24H[34h[?25h[?25l[52;196H7[35;24H[34h[?25h[?25l[52;196H8[36;24H[34h[?25h[?25l[52;196H9[37;24H[34h[?25h[?25l[52;195H80[38;24H[34h[?25h[?25l[52;196H1[39;24H[34h[?25h[?25l[52;196H2[40;24H[34h[?25h[?25l[52;196H3[41;24H[34h[?25h[?25l[52;196H4[42;24H[34h[?25h[?25l[52;196H5[43;24H[34h[?25h[?25l[52;196H6[44;24H[34h[?25h[?25l[52;196H7[45;24H[34h[?25h[?25l[46;24H[46m()[0m[52;196H8[46;24H[34h[?25h[?25l()[52;196H9[47;24H[34h[?25h[?25l[52;195H90,2-16[48;16H[34h[?25h[?25l[52;196H1,24  [49;24H[34h[?25h[?25l[52;196H2,17-24[50;24H[34h[?25h[?25l[52;196H3[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mfor[0m key [33min[0m whole_dict:[52;1H[K[52;194H194,17-24     42%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17H[33mfor[0m item [33min[0m whole_dict[key]:[52;194H[K[52;194H195,10-24     42%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;25Hgoal_dict[key][item] = [][52;194H[K[52;194H196,3-24      42%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Htime_dict = {key:{} [33mfor[0m key [33min[0m whole_dict} [34m#both empty[0m[52;194H[K[52;194H197,17-24     43%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mfor[0m key [33min[0m whole_dict:[52;194H[K[52;194H198,17-24     43%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17H[33mfor[0m item [33min[0m whole_dict[key]:[52;194H[K[52;194H199,24[8C43%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;25Htime_dict[key][item] = [][52;194H[K[52;194H200,24[8C44%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mreturn[0m goal_dict, time_dict[52;194H[K[52;194H201,17-24     44%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H202,0-1[7C44%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hx_train, y_train = load_data(data)[52;194H[K[52;194H203,24[8C44%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hname_list = np.loadtxt(data, skiprows=[31m1[0m, usecols=([31m0[0m,), dtype=[31m'str'[0m)[52;194H[K[52;194H204,24[8C45%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H205,0-1[7C45%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hscaler = preprocessing.StandardScaler().fit(y_train)[52;194H[K[52;194H206,24[8C45%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#pickle.dump(scaler, open('scaler.pkl', 'wb'))[0m[52;194H[K[52;194H207,24[8C46%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#scaler = pickle.load(open('scaler.pkl', 'rb'))[0m[52;194H[K[52;194H208,24[8C46%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H209,0-1[7C46%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hy_train = scaler.transform(y_train)[52;194H[K[52;194H210,24[8C47%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H211,0-1[7C47%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hn_snps = x_train.shape[[31m1[0m][52;194H[K[52;194H212,24[8C47%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hmy_cv = sklearn.model_selection.KFold(n_splits=[31m10[0m, shuffle=[36mTrue[0m, random_state=[31m42[0m)[52;194H[K[52;194H213,24[8C47%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#################################################SVM####SVM#####SVM####################################################################[0m[52;194H[K[52;194H214,24[8C48%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mncv_results[0m(analysis, ncv_object):[52;194H[K[52;194H215,24[8C48%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Best Params of %s is %s "[0m % (analysis, ncv_object.best_params))[52;194H[K[52;194H216,24[8C48%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Outer scores of %s is %s and mean is %s"[0m % (analysis, ncv_object.outer_scores, np.mean(ncv_object.outer_scores)))[52;194H[K[52;194H217,24[8C49%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Variance of %s is %s "[0m % (analysis, ncv_object.variance))[52;194H[K[52;194H218,24[8C49%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mwith[0m [36mopen[0m([31m'NCV_'[0m + [36mstr[0m(analysis) + [31m'_'[0m +  [36mstr[0m(snps) + [31m'_'[0m + [36mstr[0m(phenotype) + [31m'_'[0m + [36mstr[0m(num) + [31m'.pkl'[0m, [31m'wb'[0m) [33mas[0m ncvfile: [34m#with open("fname.pkl", 'rb') as ncvfile:[0m[52;194H[K[52;194H219,24[8C49%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hpickle.dump(ncv_object, ncvfile) [34m#ncv_object = pickle.load(ncvfile)[0m[52;194H[K[52;194H220,24[8C50%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H221,0-1[7C50%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mnn_results[0m(analysis, ncv_object):[52;194H[K[52;194H222,24[8C50%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Best Params of %s is %s "[0m % (analysis, ncv_object.best_params))[52;194H[K[52;194H223,24[8C50%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Outer scores of %s is %s and mean is %s"[0m % (analysis, ncv_object.outer_scores, np.mean(ncv_object.outer_scores)))[52;194H[K[52;194H224,24[8C51%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Variance of %s is %s "[0m % (analysis, ncv_object.variance))[52;194H[K[52;194H225,24[8C51%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hnn_list = [ncv_object.best_inner_params_list, ncv_object.best_inner_score_list, ncv_object.best_params, ncv_object.metric, ncv_object.outer_scores, ncv_object.variance][52;194H[K[52;194H226,24[8C51%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mwith[0m [36mopen[0m([31m'NCV_'[0m + [36mstr[0m(analysis) + [31m'_'[0m +  [36mstr[0m(snps) + [31m'_'[0m + [36mstr[0m(phenotype) + [31m'_'[0m + [36mstr[0m(num) + [31m'.pkl'[0m, [31m'wb'[0m) [33mas[0m ncvfile:[52;194H[K[52;194H227,24[8C52%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hpickle.dump(nn_list, ncvfile) [34m#ncv_object = pickle.load(ncvfile)[0m[52;194H[K[52;194H228,24[8C52%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hncv_object.model.model.save([31m"model_"[0m + [36mstr[0m(analysis) + [31m'_'[0m +  [36mstr[0m(snps) + [31m'_'[0m + [36mstr[0m(phenotype) + [31m'_'[0m + [36mstr[0m(num) + [31m".h5"[0m)[52;194H[K[52;194H229,24[8C52%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H230,0-1[7C52%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H231,0-1[7C53%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[36mprint[0m[46m([0m[31m"Performing SVM"[0m[46m)[0m[52;194H[K[52;194H232,23[8C53%[51;23H[34h[?25h[?25l[1;51r[51;1H
[1;52r[50;6H([16C)
c_param = [46m[[0m[31m1[0m,[31m2[0m[46m][0m[52;194H[K[52;194H233,15[8C53%[51;15H[34h[?25h[?25l[1;51r[51;1H
[1;52r[50;11H[[3C]
gamma_param = [[36mfloat[0m(x) [33mfor[0m x [33min[0m np.linspace([31m0.1[0m, [31m1[0m, [31m4[0m)][52;194H[K[52;194H234,24[8C54%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H235,0-1[7C54%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H236,0-1[7C54%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hepsilon_param = [[36mfloat[0m(x) [33mfor[0m x [33min[0m np.linspace([31m0.1[0m, [31m1[0m, [31m4[0m)][52;194H[K[52;194H237,24[8C55%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hloss_param = [[31m'epsilon_insensitive'[0m, [31m'squared_epsilon_insensitive'[0m][52;194H[K[52;194H238,24[8C55%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hkernel_param = [46m[[0m[31m'poly'[0m[46m][0m[52;194H[K[52;194H239,23[8C55%[51;23H[34h[?25h[?25l[1;51r[51;1H
[1;52r[50;16H[[6C]
degree = [46m[[0m[31m1[0m,[31m2[0m,[31m3[0m[46m][0m[52;194H[K[52;194H240,16[8C55%[51;16H[34h[?25h[?25l[1;51r[51;1H
[1;52r[50;10H[[5C]
svm_random_grid = {[31m'gamma'[0m:gamma_param, [31m'C'[0m:c_param,[31m'kernel'[0m:kernel_param, [31m"degree"[0m:degree}[52;194H[K[52;194H241,24[8C56%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[36mprint[0m[46m([0msvm_random_grid[46m)[0m[52;194H[K[52;194H242,22[8C56%[51;22H[34h[?25h[?25l[1;51r[51;1H
[1;52r[50;6H([15C)
svm_random_grid2 = {[31m'C'[0m : c_param, [31m'loss'[0m:loss_param}[52;194H[K[52;194H243,24[8C56%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[36mprint[0m[46m([0msvm_random_grid2[46m)[0m[52;194H[K[52;194H244,23[8C57%[51;23H[34h[?25h[?25l[1;51r[51;1H
[1;52r[50;6H([16C)
rbg_goal_dict, rbg_time_dict = make_goal_dict(svm_random_grid)[52;194H[K[52;194H245,24[8C57%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hsvm_goal_dict, svm_time_dict = make_goal_dict(svm_random_grid2)[52;194H[K[52;194H246,24[8C57%[51;24H[34h[?25h[?25l[1;51r[1;1H[2M[1;52r[50;1HSVM_NCV = NestedCV(model_name=[31m'LinearSVR'[0m, name_list = name_list, model=LinearSVR(), goal_dict=svm_goal_dict, time_dict=svm_time_dict, params_grid=svm_random_grid2, outer_kfolds=[31m2[0m, inner_kfolds=[31m2[0m, n_jobs = [31m8[0m,cv__[51;1Hoptions={[31m'randomized_search'[0m:[36mTrue[0m, [31m'randomized_search_iter'[0m:[31m10[0m, [31m'sqrt_of_score'[0m:[36mFalse[0m,[31m'recursive_feature_elimination'[0m:[36mFalse[0m, [31m'metric'[0m:sklearn.metrics.r2_score, [31m'metric_score_indicator_lower'[0m:[36mFalse[0m})[52;194H[K[52;194H247,24[8C58%[50;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1HSVM_NCV.fit(x_train, y_train.ravel(), name_list=name_list, phenfile=phenfile, set_size=set_size, snps=snps, model_name=[31m'SVM'[0m, goal_dict=svm_goal_dict, time_dict=svm_time_dict)[52;194H[K[52;194H248,24[8C58%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H249,0-1[7C58%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H250,0-1[7C58%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hncv_results([31m'SVM'[0m, SVM_NCV)[52;194H[K[52;194H251,24[8C59%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31m'''[0m[52;194H[K[52;194H252,3[9C59%[51;3H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mprint[0m[31m[46m([0m[31m"Performing RBG"[0m[31m[46m)[0m[52;194H[K[52;194H253,23[8C59%[51;23H[34h[?25h[?25l[1;51r[1;1H[2M[1;52r[49;6H[31m([16C)
RBG_NCV = NestedCV(model_name='RBG', name_list=name_list, model=SVR(), params_grid=svm_random_grid, outer_kfolds=4, inner_kfolds=4, n_jobs = 8,cv_options={'randomized_search':True, 'randomized_search_iter':50, ''[51;1Hsqrt_of_score':False,'recursive_feature_elimination':False, 'metric':sklearn.metrics.r2_score, 'metric_score_indicator_lower':False})[0m[52;194H[K[52;194H254,24[8C60%[50;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mRBG_NCV.fit(x_train, y_train.ravel(), name_list=name_list, phenfile=phenfile, set_size=set_size, snps=snps, model_name='RBG')[0m[52;194H[K[52;194H255,24[8C60%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mncv_results('RBG', RBG_NCV)[0m[52;194H[K[52;194H256,24[8C60%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H257,0-1[7C61%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mprint("Performing LASSO")[0m[52;194H[K[52;194H258,24[8C61%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31malpha = [0.0001, 0.001, 0.01, 0.1, 0.5, 1, 10, 100][0m[52;194H[K[52;194H259,24[8C61%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31malpha_dict = {'alpha':alpha}[0m[52;194H[K[52;194H260,24[8C62%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mprint[0m[31m[46m([0m[31malpha_dict[0m[31m[46m)[0m[52;194H[K[52;194H261,17[8C62%[51;17H[34h[?25h[?25l[1;51r[51;1H
[1;52r[50;6H[31m([10C)
alpha_name_dict = {'alpha':"Alpha"}[0m[52;194H[K[52;194H262,24[8C62%[51;24H[34h[?25h[?25l[1;51r[1;1H[2M[1;52r[50;1H[31mLASS_NCV = NestedCV(model_name='LASS', name_list=name_list, model=Lasso(), params_grid=alpha_dict, outer_kfolds=4, inner_kfolds=4, n_jobs = 8,cv_options={'randomized_search':True, 'randomized_search_iter':50, 'ss[51;1Hqrt_of_score':False,'recursive_feature_elimination':False, 'metric':sklearn.metrics.r2_score, 'metric_score_indicator_lower':False})[0m[52;194H[K[52;194H263,24[8C63%[50;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mLASS_NCV.fit(x_train, y_train.ravel(), name_list=name_list, phenfile=phenfile, set_size=set_size, snps=snps, model_name='LASS')[0m[52;194H[K[52;194H264,24[8C63%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mncv_results('LASS', LASS_NCV)[0m[52;194H[K[52;194H265,24[8C63%[51;24H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mprint("Performing Ridge")[0m[52;194H[K[52;194H266,24[8C63%[51;24H[34h[?25h[?25l[52;196H5[50;24H[34h[?25h[?25l[52;196H4[49;24H[34h[?25h[?25l[52;196H3[47;24H[34h[?25h[?25l[52;196H2[46;24H[34h[?25h[?25l[45;6H[31m[46m([10C)[0m[52;196H1,17[45;17H[34h[?25h[?25l[45;6H[31m([10C)[0m[52;196H0,24[44;24H[34h[?25h[?25l[52;195H59[43;24H[34h[?25h[?25l[52;196H8[42;24H[34h[?25h[?25l[52;196H7,0-1[41;1H[34h[?25h[?25l[52;196H6,24 [40;24H[34h[?25h[?25l[52;196H5[39;24H[34h[?25h[?25l[52;196H4[37;24H[34h[?25h[?25l[36;6H[31m[46m([16C)[0m[52;196H3,23[36;23H[34h[?25h[?25l[36;6H[31m([16C)[0m[52;196H2,3 [35;3H[34h[?25h[?25l[52;196H1,24[34;24H[34h[?25h[?25l[52;196H0,0-1[33;1H[34h[?25h[?25l[52;195H49[32;1H[34h[?25h[?25l[52;196H8,24 [31;24H[34h[?25h[?25l[52;196H7[29;24H[34h[?25h[?25l[52;196H6[28;24H[34h[?25h[?25l[52;196H5[27;24H[34h[?25h[?25l[26;6H[46m([16C)[0m[52;196H4,23[26;23H[34h[?25h[?25l[26;6H([16C)[52;196H3,24[25;24H[34h[?25h[?25l[24;6H[46m([15C)[0m[52;196H2,22[24;22H[34h[?25h[?25l[24;6H([15C)[52;196H1,24[23;24H[34h[?25h[?25l[22;10H[46m[[5C][0m[52;196H0,16[22;16H[34h[?25h[?25l[21;16H[46m[[6C][0m[22;10H[[5C][52;195H39,23[21;23H[34h[?25h[?25l[21;16H[[6C][52;196H8,24[20;24H[34h[?25h[?25l[52;196H7[19;24H[34h[?25h[?25l[52;196H6,0-1[18;1H[34h[?25h[?25l[52;196H7,24 [19;24H[34h[?25h[?25l[52;196H8[20;24H[34h[?25h[?25l[21;16H[46m[[6C][0m[52;196H9,23[21;23H[34h[?25h[?25l[21;16H[[6C][22;10H[46m[[5C][0m[52;195H40,16[22;16H[34h[?25h[?25l[[5C][52;196H1,24[23;24H[34h[?25h[?25l[24;6H[46m([15C)[0m[52;196H2,22[24;22H[34h[?25h[?25l[24;6H([15C)[52;196H3,24[25;24H[34h[?25h[?25l[26;6H[46m([16C)[0m[52;196H4,23[26;23H[34h[?25h[?25l[26;6H([16C)[52;196H5,24[27;24H[34h[?25h[?25l[52;196H6[28;24H[34h[?25h[?25l[52;196H7[29;24H[34h[?25h[?25l[52;196H8[31;24H[34h[?25h[?25l[52;196H9,0-1[32;1H[34h[?25h[?25l[52;195H50[33;1H[34h[?25h[?25l[52;196H1,24 [34;24H[34h[?25h[?25l[52;196H2,3 [35;3H[34h[?25h[?25l[36;6H[31m[46m([16C)[0m[52;196H3,23[36;23H[34h[?25h[?25l[36;6H[31m([16C)[0m[52;196H4,24[37;24H[34h[?25h[?25l[52;196H5[39;24H[34h[?25h[?25l[52;196H6[40;24H[34h[?25h[?25l[52;196H7,0-1[41;1H[34h[?25h[?25l[52;196H8,24 [42;24H[34h[?25h[?25l[52;196H9[43;24H[34h[?25h[?25l[52;195H60[44;24H[34h[?25h[?25l[52;195H59[43;24H[34h[?25h[?25l[52;196H8[42;24H[34h[?25h[?25l[52;196H7,0-1[41;1H[34h[?25h[?25l[52;196H6,24 [40;24H[34h[?25h[?25l[52;196H5[39;24H[34h[?25h[?25l[52;196H4[37;24H[34h[?25h[?25l[36;6H[31m[46m([16C)[0m[52;196H3,23[36;23H[34h[?25h[?25l[36;6H[31m([16C)[0m[52;196H2,3 [35;3H[34h[?25h[?25l[52;196H1,24[34;24H[34h[?25h[?25l[52;196H0,0-1[33;1H[34h[?25h[?25l[52;195H49[32;1H[34h[?25h[?25l[52;196H8,24 [31;24H[34h[?25h[?25l[52;196H7[29;24H[34h[?25h[?25l[52;196H6[28;24H[34h[?25h[?25l[52;196H7[29;24H[34h[?25h[?25l[52;196H8[31;24H[34h[?25h[?25l[52;196H9,0-1[32;1H[34h[?25h[?25l[52;195H50[33;1H[34h[?25h[?25l[52;195H49[32;1H[34h[?25h[?25l[52;1H[1m-- INSERT --[0m[52;194H[K[52;194H249,1[9C63%[32;1H[34h[?25h[?25lmake_param_box_plot(goal_dict, time_dict)[32;20H[46m([20C)[0m[52;198H42[32;42H[34h[?25h[?25l[32;20H([20C)[52;196H8[31;42H[34h[?25h[?25l[52;196H7[29;42H[34h[?25h[?25l[52;196H6[28;42H[34h[?25h[?25l[52;196H5[27;42H[34h[?25h[?25l[26;6H[46m([16C)[0m[52;196H4,24[26;24H[34h[?25h[?25l[26;6H([16C)[52;196H3,42[25;42H[34h[?25h[?25l[24;6H[46m([15C)[0m[52;196H2,23[24;23H[34h[?25h[?25l[24;6H([15C)[52;196H1,42[23;42H[34h[?25h[?25l[22;10H[46m[[5C][0m[52;196H0,17[22;17H[34h[?25h[?25l[21;16H[46m[[6C][0m[22;10H[[5C][52;195H39,24[21;24H[34h[?25h[?25l[21;16H[[6C][52;196H8,42[20;42H[34h[?25h[?25l[52;196H7[19;42H[34h[?25h[?25l[52;196H6,1 [18;1H[34h[?25h[?25l[52;196H5[17;1H[34h[?25h[?25l[52;196H4,42[16;42H[34h[?25h[?25l[15;11H[46m[[3C][0m[52;196H3,16[15;16H[34h[?25h[?25l[14;6H[46m([16C)[0m[15;11H[[3C][52;196H2,24[14;24H[34h[?25h[?25l[14;6H([16C)[52;196H1,1 [13;1H[34h[?25h[?25l[52;196H0[12;1H[34h[?25h[?25l[52;195H29,42[11;42H[34h[?25h[?25l[52;196H8[10;42H[34h[?25h[?25l[52;196H7[9;42H[34h[?25h[?25l[52;196H6[8;42H[34h[?25h[?25l[7;41H[46m([29C)[0m[52;196H5[7;42H[34h[?25h[?25l([29C)[52;196H4[6;42H[34h[?25h[?25l[52;196H3[5;42H[34h[?25h[?25l[52;196H2,38[4;38H[34h[?25h[?25l[52;196H1,1 [3;1H[34h[?25h[?25l[52;196H0,42[2;42H[34h[?25h[?25l[52;195H19[1;42H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;9H[36mprint[0m([31m"Variance of %s is %s "[0m % (analysis, ncv_object.variance))[52;194H[K[52;194H218,42[8C63%[1;41H[46m([29C)[1;42H[34h[?25h[?25l[1;51r[0m[1;1H[L[1;52r[1;9H[36mprint[0m([31m"Outer scores of %s is %s and mean is %s"[0m % (analysis, ncv_object.outer_scores, np.mean(ncv_object.outer_scores)))[52;194H[K[52;194H217,42[8C63%[2;41H([29C)[1;42H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;9H[36mprint[0m([31m"Best Params of %s is %s "[0m % (analysis, ncv_object.best_params))[52;194H[K[52;194H216,42[8C63%[1;42H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;1H[33mdef[0m [36mncv_results[0m(analysis, ncv_object):[51;1H[1m[34m@                                                                                                                                                                                                                  [0m[52;194H[K[52;194H215,39[8C62%[1;39H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;1H[34m#################################################SVM####SVM#####SVM####################################################################[0m[52;194H[K[52;194H214,42[8C62%[1;42H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;1Hmy_cv = sklearn.model_selection.KFold(n_splits=[31m10[0m, shuffle=[36mTrue[0m, random_state=[31m42[0m)[52;194H[K[52;194H213,42[8C62%[1;42H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;1Hn_snps = x_train.shape[[31m1[0m][52;194H[K[52;194H212,26[8C62%[1;23H[46m[[1C][34h[?25h[?25l[1;51r[0m[1;1H[L[1;52r[52;194H[K[52;194H211,1[9C61%[2;23H[[1C][1;1H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;1Hy_train = scaler.transform(y_train)[52;194H[K[52;194H210,36[8C61%[1;27H[46m([7C)[34h[?25h[?25l[1;51r[0m[1;1H[L[1;52r[52;194H[K[52;194H209,1[9C61%[2;27H([7C)[1;1H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;1H[34m#scaler = pickle.load(open('scaler.pkl', 'rb'))[0m[52;194H[K[52;194H208,42[8C60%[1;42H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;1H[34m#pickle.dump(scaler, open('scaler.pkl', 'wb'))[0m[52;194H[K[52;194H207,42[8C60%[1;42H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;1Hscaler = preprocessing.StandardScaler().fit(y_train)[52;194H[K[52;194H206,42[8C60%[1;42H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[51;1H[1m[34m@                                                                                                                                                                                                                  [0m[52;194H[K[52;194H205,1[9C60%[1;1H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;1Hname_list = np.loadtxt(data, skiprows=[31m1[0m, usecols=([31m0[0m,), dtype=[31m'str'[0m)[52;194H[K[52;194H204,42[8C59%[1;42H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;1Hx_train, y_train = load_data(data)[52;194H[K[52;194H203,35[8C59%[1;29H[46m([0mdata[46m)[34h[?25h[?25l[1;51r[0m[1;1H[L[1;52r[52;194H[K[52;194H202,1[9C59%[2;29H(data)[1;1H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;9H[33mreturn[0m goal_dict, time_dict[52;194H[K[52;194H201,29-36     58%[1;36H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;25Htime_dict[key][item] = [][52;194H[K[52;194H200,42[8C58%[1;42H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;17H[33mfor[0m item [33min[0m whole_dict[key]:[52;194H[K[52;194H199,42[8C58%[1;42H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;9H[33mfor[0m key [33min[0m whole_dict:[52;194H[K[52;194H198,24-31     58%[1;31H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;9Htime_dict = {key:{} [33mfor[0m key [33min[0m whole_dict} [34m#both empty[0m[51;1H[1m[34m@                                                                                                                                                                                                                  [0m[52;194H[K[52;194H197,35-42     57%[1;42H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;25Hgoal_dict[key][item] = [][52;194H[K[52;194H196,21-42     57%[1;42H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;17H[33mfor[0m item [33min[0m whole_dict[key]:[52;194H[K[52;194H195,28-42     57%[1;42H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;9H[33mfor[0m key [33min[0m whole_dict:[52;194H[K[52;194H194,24-31     57%[1;31H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;9Hgoal_dict = {key:{} [33mfor[0m key [33min[0m whole_dict}[52;194H[K[52;194H193,35-42     56%[1;42H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;9H[36mprint[0m(whole_dict)[52;194H[K[52;194H192,19-26     56%[1;14H[46m([10C)[34h[?25h[?25l[1;51r[0m[1;1H[L[1;52r[1;1H[33mdef[0m [36mmake_goal_dict[0m(whole_dict):[52;194H[K[52;194H191,32[8C56%[2;14H([10C)[1;32H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[52;194H[K[52;194H190,3-17      55%[1;17H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;17Hplt.close()[52;194H[K[52;194H189,28[8C55%[1;26H[46m()[34h[?25h[?25l[1;51r[0m[1;1H[L[1;52r[1;17Hplt.clf()[52;194H[K[52;194H188,26[8C55%[1;24H[46m()[0m[2;26H()[1;26H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;17Hplt.show()[52;194H[K[52;194H187,27[8C55%[1;25H[46m()[0m[2;24H()[1;27H[34h[?25h[?25l[1;51r[1;1H[L[1;52r[1;17Hplt.savefig(my_fig_name, dpi=[31m300[0m)[52;194H[K[52;194H186,42[8C54%[2;25H()[1;42H[34h[?25h[?25l[2;25H[46m()[0m[52;196H7,27[2;27H[34h[?25h[?25l()[3;24H[46m()[0m[52;196H8,26[3;26H[34h[?25h[?25l()[4;26H[46m()[0m[52;196H9,28[4;28H[34h[?25h[?25l()[52;195H90,3-17[5;17H[34h[?25h[?25l[52;196H1,32  [6;32H[34h[?25h[?25l[7;14H[46m([10C)[0m[52;196H2,19-26[7;26H[34h[?25h[?25l[7;14H([10C)[52;196H3,35-42[8;42H[34h[?25h[?25l[52;196H4,24-31[9;31H[34h[?25h[?25l[52;196H5,28-42[10;42H[34h[?25h[?25l[52;196H6,21[11;42H[34h[?25h[?25l[52;196H7,35[12;42H[34h[?25h[?25l[52;196H8,24-31[13;31H[34h[?25h[?25l[52;196H9,42   [14;42H[34h[?25h[?25l[52;194H200[15;42H[34h[?25h[?25l[52;196H1,29-36[16;36H[34h[?25h[?25l[52;196H2,1    [17;1H[34h[?25h[?25l[18;29H[46m([0mdata[46m)[0m[52;196H3,35[18;35H[34h[?25h[?25l(data)[52;196H4,42[19;42H[34h[?25h[?25l[52;196H5,1 [20;1H[34h[?25h[?25l[52;196H6,42[21;42H[34h[?25h[?25l[52;196H7[22;42H[34h[?25h[?25l[52;196H8[23;42H[34h[?25h[?25l[52;196H9,1 [24;1H[34h[?25h[?25l[25;27H[46m([7C)[0m[52;195H10,36[25;36H[34h[?25h[?25l[25;27H([7C)[52;196H1,1 [26;1H[34h[?25h[?25l[27;23H[46m[[1C][0m[52;196H2,26[27;26H[34h[?25h[?25l[[1C][52;196H3,42[28;42H[34h[?25h[?25l[52;196H4[29;42H[34h[?25h[?25l[52;196H5,39[30;39H[34h[?25h[?25l[52;196H6,42[31;42H[34h[?25h[?25l[52;196H7[32;42H[34h[?25h[?25l[33;41H[46m([29C)[0m[52;196H8[33;42H[34h[?25h[?25l([29C)[52;199H3[33;43H[34h[?25h[?25l[52;199H4[33;44H[34h[?25h[?25l[52;199H5[33;45H[34h[?25h[?25l[52;199H6[33;46H[34h[?25h[?25l[52;199H7[33;47H[34h[?25h[?25l[52;199H8[33;48H[34h[?25h[?25l[52;199H9[33;49H[34h[?25h[?25l[52;198H50[33;50H[34h[?25h[?25l[52;199H1[33;51H[34h[?25h[?25l[52;199H2[33;52H[34h[?25h[?25l[52;199H3[33;53H[34h[?25h[?25l[52;199H4[33;54H[34h[?25h[?25l[52;199H5[33;55H[34h[?25h[?25l[52;199H6[33;56H[34h[?25h[?25l[52;199H7[33;57H[34h[?25h[?25l[52;199H8[33;58H[34h[?25h[?25l[52;199H9[33;59H[34h[?25h[?25l[52;198H60[33;60H[34h[?25h[?25l[52;199H1[33;61H[34h[?25h[?25l[52;199H2[33;62H[34h[?25h[?25l[52;199H3[33;63H[34h[?25h[?25l[52;199H4[33;64H[34h[?25h[?25l[52;199H5[33;65H[34h[?25h[?25l[52;199H6[33;66H[34h[?25h[?25l[52;199H7[33;67H[34h[?25h[?25l[52;199H8[33;68H[34h[?25h[?25l[52;199H9[33;69H[34h[?25h[?25l[52;198H70[33;70H[34h[?25h[?25l[33;41H[46m([29C)[0m[52;199H1[33;71H[34h[?25h[?25l[33;14H[46m([0m[26C([29C)[46m)[0m[52;199H2[33;72H[34h[?25h[?25l[52;199H3[33;73H[34h[?25h[?25li[33;14H([57C)[52;199H4[33;74H[34h[?25h[?25l[33;73H[K[33;14H[46m([57C)[0m[52;199H3[33;73H[34h[?25h[?25l[34;51r[34;1H[L[1;52r[52;194H[K[52;194H219,1[9C54%[33;14H([57C)
[34h[?25h[?25l[52;198H2[34;2H[34h[?25h[?25l[52;198H3[34;3H[34h[?25h[?25l[52;198H4[34;4H[34h[?25h[?25l[52;198H5[34;5H[34h[?25h[?25l[52;198H6[34;6H[34h[?25h[?25l[52;198H7[34;7H[34h[?25h[?25l[52;198H8[34;8H[34h[?25h[?25l[52;198H9[34;9H[34h[?25h[?25l[35;51r[35;1H[L[1;52r[34;9H[36mprint[0m([31m"Goal dict of %s is %s "[0m % (analysis, ncv_object.goal_dict))[35;9Hmake_param_box_plot(ncv_object.goal_dict, ncv_object.time_dict)[52;194H[K[52;194H220,72[8C54%[35;28H[46m([42C)[34h[?25h[?25l[0m[52;199H1[35;71H[34h[?25h[?25l[35;28H([42C)[52;199H0[35;70H[34h[?25h[?25l[52;198H69[35;69H[34h[?25h[?25l[52;199H8[35;68H[34h[?25h[?25l[52;199H7[35;67H[34h[?25h[?25l[52;199H6[35;66H[34h[?25h[?25l[52;199H5[35;65H[34h[?25h[?25l[52;199H4[35;64H[34h[?25h[?25l[52;199H3[35;63H[34h[?25h[?25l[52;199H2[35;62H[34h[?25h[?25l[52;199H1[35;61H[34h[?25h[?25l[52;199H0[35;60H[34h[?25h[?25l[52;198H59[35;59H[34h[?25h[?25l[52;199H8[35;58H[34h[?25h[?25l[52;199H7[35;57H[34h[?25h[?25l[52;199H6[35;56H[34h[?25h[?25l[52;199H5[35;55H[34h[?25h[?25l[52;199H4[35;54H[34h[?25h[?25l[52;199H3[35;53H[34h[?25h[?25l[52;199H2[35;52H[34h[?25h[?25l[52;199H1[35;51H[34h[?25h[?25l[52;199H0[35;50H[34h[?25h[?25l[52;198H49[35;49H[34h[?25h[?25l[52;199H8[35;48H[34h[?25h[?25l[52;199H7[35;47H[34h[?25h[?25l[52;199H6[35;46H[34h[?25h[?25l[52;199H5[35;45H[34h[?25h[?25l[52;199H4[35;44H[34h[?25h[?25l[52;199H3[35;43H[34h[?25h[?25l[52;199H2[35;42H[34h[?25h[?25l[52;199H1[35;41H[34h[?25h[?25l[52;199H0[35;40H[34h[?25h[?25l[52;198H39[35;39H[34h[?25h[?25l[52;199H8[35;38H[34h[?25h[?25l[52;199H7[35;37H[34h[?25h[?25l[52;199H6[35;36H[34h[?25h[?25l[52;199H5[35;35H[34h[?25h[?25l[52;199H4[35;34H[34h[?25h[?25l[52;199H3[35;33H[34h[?25h[?25l[52;199H2[35;32H[34h[?25h[?25l[52;199H1[35;31H[34h[?25h[?25l[52;199H0[35;30H[34h[?25h[?25l[46m([42C)[0m[52;198H29[35;29H[34h[?25h[?25l[52;199H8[35;28H[34h[?25h[?25l([42C)[52;199H7[35;27H[34h[?25h[?25l[52;199H6[35;26H[34h[?25h[?25l[52;199H5[35;25H[34h[?25h[?25l[52;199H4[35;24H[34h[?25h[?25l[52;199H3[35;23H[34h[?25h[?25l[52;199H2[35;22H[34h[?25h[?25l[52;199H1[35;21H[34h[?25h[?25l[52;199H0[35;20H[34h[?25h[?25l[52;198H19[35;19H[34h[?25h[?25l[52;199H8[35;18H[34h[?25h[?25l[52;199H7[35;17H[34h[?25h[?25l[52;199H6[35;16H[34h[?25h[?25l[52;199H5[35;15H[34h[?25h[?25l[52;199H4[35;14H[34h[?25h[?25l[52;199H3[35;13H[34h[?25h[?25l[52;199H2[35;12H[34h[?25h[?25l[52;199H1[35;11H[34h[?25h[?25l[52;199H0[35;10H[34h[?25h[?25l[52;198H9 [35;9H[34h[?25h[?25l[52;198H8[35;8H[34h[?25h[?25l[52;198H7[35;7H[34h[?25h[?25l[52;198H6[35;6H[34h[?25h[?25l[52;198H5[35;5H[34h[?25h[?25l[52;198H4[35;4H[34h[?25h[?25l[52;198H3[35;3H[34h[?25h[?25l[52;198H2[35;2H[34h[?25h[?25l[52;198H1[35;1H[34h[?25h[?25l[52;195H19[34;1H[34h[?25h[?25l[52;198H2[34;2H[34h[?25h[?25l[52;198H3[34;3H[34h[?25h[?25l[52;198H4[34;4H[34h[?25h[?25l[52;198H5[34;5H[34h[?25h[?25l[52;198H6[34;6H[34h[?25h[?25l[52;198H7[34;7H[34h[?25h[?25l[52;198H8[34;8H[34h[?25h[?25l[52;198H9[34;9H[34h[?25h[?25l[52;198H10[34;10H[34h[?25h[?25l[52;199H1[34;11H[34h[?25h[?25l[52;199H2[34;12H[34h[?25h[?25l[52;199H3[34;13H[34h[?25h[?25l[1C[46m([59C)[0m[52;199H4[34;14H[34h[?25h[?25l[52;199H5[34;15H[34h[?25h[?25l([59C)[52;199H6[34;16H[34h[?25h[?25l[52;199H7[34;17H[34h[?25h[?25l[52;199H8[34;18H[34h[?25h[?25l[52;199H9[34;19H[34h[?25h[?25l[52;198H20[34;20H[34h[?25h[?25l[52;195H20[35;20H[34h[?25h[?25l[52;199H1[35;21H[34h[?25h[?25l[52;199H2[35;22H[34h[?25h[?25l[52;199H3[35;23H[34h[?25h[?25l[52;199H4[35;24H[34h[?25h[?25l[52;199H5[35;25H[34h[?25h[?25l[52;199H6[35;26H[34h[?25h[?25l[52;199H7[35;27H[34h[?25h[?25lt[46m([42C)[0m[52;199H8[35;28H[34h[?25h[?25l[52;199H9[35;29H[34h[?25h[?25l([42C)[52;198H30[35;30H[34h[?25h[?25l[52;199H1[35;31H[34h[?25h[?25l[52;199H2[35;32H[34h[?25h[?25l[52;199H3[35;33H[34h[?25h[?25l[52;199H4[35;34H[34h[?25h[?25l[52;199H5[35;35H[34h[?25h[?25l[52;199H6[35;36H[34h[?25h[?25l[52;199H7[35;37H[34h[?25h[?25l[52;199H8[35;38H[34h[?25h[?25l[52;199H9[35;39H[34h[?25h[?25l[52;198H40[35;40H[34h[?25h[?25l[52;199H1[35;41H[34h[?25h[?25l[52;199H2[35;42H[34h[?25h[?25l[52;199H3[35;43H[34h[?25h[?25l[52;199H4[35;44H[34h[?25h[?25l[52;199H5[35;45H[34h[?25h[?25l[52;199H6[35;46H[34h[?25h[?25l[52;199H7[35;47H[34h[?25h[?25l[52;199H8[35;48H[34h[?25h[?25l[52;199H7[35;47H[34h[?25h[?25l[52;199H6[35;46H[34h[?25h[?25l[52;199H5[35;45H[34h[?25h[?25l[52;199H4[35;44H[34h[?25h[?25l[52;199H3[35;43H[34h[?25h[?25l[52;199H2[35;42H[34h[?25h[?25l[52;199H1[35;41H[34h[?25h[?25l[52;199H0[35;40H[34h[?25h[?25l[52;198H39[35;39H[34h[?25h[?25l[52;199H8[35;38H[34h[?25h[?25l[52;199H7[35;37H[34h[?25h[?25l[52;199H6[35;36H[34h[?25h[?25l[52;199H5[35;35H[34h[?25h[?25l[52;199H4[35;34H[34h[?25h[?25l[52;199H3[35;33H[34h[?25h[?25l[52;199H2[35;32H[34h[?25h[?25l[52;199H1[35;31H[34h[?25h[?25l[52;199H0[35;30H[34h[?25h[?25l[46m([42C)[0m[52;198H29[35;29H[34h[?25h[?25l[52;199H8[35;28H[34h[?25h[?25l([42C)[52;199H7[35;27H[34h[?25h[?25l[52;199H6[35;26H[34h[?25h[?25l[52;199H5[35;25H[34h[?25h[?25l[52;199H4[35;24H[34h[?25h[?25l[52;199H3[35;23H[34h[?25h[?25l[52;199H2[35;22H[34h[?25h[?25l[52;195H19[34;22H[34h[?25h[?25l[52;195H20[35;22H[34h[?25h[?25l[52;195H19[34;22H[34h[?25h[?25l[52;199H1[34;21H[34h[?25h[?25l[52;199H0[34;20H[34h[?25h[?25l[52;198H19[34;19H[34h[?25h[?25l[52;199H8[34;18H[34h[?25h[?25l[52;199H7[34;17H[34h[?25h[?25l[52;199H6[34;16H[34h[?25h[?25l[46m([59C)[0m[52;199H5[34;15H[34h[?25h[?25l[52;199H4[34;14H[34h[?25h[?25l([59C)[52;199H3[34;13H[34h[?25h[?25l[52;199H2[34;12H[34h[?25h[?25l[52;199H1[34;11H[34h[?25h[?25l[52;199H0[34;10H[34h[?25h[?25l[52;198H9 [34;9H[34h[?25h[?25l[52;198H8[34;8H[34h[?25h[?25l[52;198H7[34;7H[34h[?25h[?25l[52;198H6[34;6H[34h[?25h[?25l[52;198H5[34;5H[34h[?25h[?25l[52;198H6[34;6H[34h[?25h[?25l[52;198H7[34;7H[34h[?25h[?25l[52;198H8[34;8H[34h[?25h[?25l[52;198H9[34;9H[34h[?25h[?25l[52;195H20[35;9H[34h[?25h[?25l[52;198H10[35;10H[34h[?25h[?25l[52;199H1[35;11H[34h[?25h[?25l[52;199H2[35;12H[34h[?25h[?25l[52;199H3[35;13H[34h[?25h[?25l[52;199H4[35;14H[34h[?25h[?25l[52;199H5[35;15H[34h[?25h[?25l[52;199H6[35;16H[34h[?25h[?25l[52;199H7[35;17H[34h[?25h[?25l[52;199H8[35;18H[34h[?25h[?25l[52;199H9[35;19H[34h[?25h[?25l[52;198H20[35;20H[34h[?25h[?25l[52;199H1[35;21H[34h[?25h[?25l[52;199H2[35;22H[34h[?25h[?25l[52;199H3[35;23H[34h[?25h[?25l[52;199H4[35;24H[34h[?25h[?25l[52;199H5[35;25H[34h[?25h[?25l[52;199H6[35;26H[34h[?25h[?25l[52;199H7[35;27H[34h[?25h[?25lt[46m([42C)[0m[52;199H8[35;28H[34h[?25h[?25l[52;199H9[35;29H[34h[?25h[?25l([42C)[52;198H30[35;30H[34h[?25h[?25l[52;196H1[36;30H[34h[?25h[?25l[52;196H2[37;30H[34h[?25h[52;1H[K[37;29H[?25l[52;194H222,29[8C54%[37;29H[34h[?25h[?25l[52;194H[K[52;1H:[34h[?25hx[?25l[34h[?25h[?25l"cv_grid_all_ml.py" 391L, 23144C written
[?1l>[34h[?25h[?1049l(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ git add . 
(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ git commit -m [K[K[K[K[K[K[K[K[K[K[K[K[K[Kgit add . vi cv_grid_all_ml.pynested_cv_new_name.py~/j[Kvi cv_grid_all_ml.pypython cv_grid_all_ml.py 31 /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del /home/ciaran/completed_big_matrix_binary_new_snps_ids__first_10k.raw shuf FT10cv_new_10k 10000
Please remember to set the right set size in the nested_cv code
/external_storage/ciaran/machine_learning2
WARNING THIS IS AN EDITED SCRIPT - Ciaran Kelly 2021 
 Edited with permission under liscence 
 Flex version
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
31
/home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del
/home/ciaran/completed_big_matrix_binary_new_snps_ids_first_10k.raw
shuf
FT10cv_new_10k
10000
2021-12-19 17:27:00
Performing SVM
{'gamma': [0.1, 0.4, 0.7, 1.0], 'degree': [1, 2, 3], 'kernel': ['poly'], 'C': [1, 2]}
{'loss': ['epsilon_insensitive', 'squared_epsilon_insensitive'], 'C': [1, 2]}
{'gamma': [0.1, 0.4, 0.7, 1.0], 'degree': [1, 2, 3], 'kernel': ['poly'], 'C': [1, 2]}
{'loss': ['epsilon_insensitive', 'squared_epsilon_insensitive'], 'C': [1, 2]}
ADBUCE
INNER COUNT NO.  1
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_1_in_1_in.raw
Set size set to 10000
Set size set to 10000
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.
  % (grid_size, self.n_iter, grid_size), UserWarning)
{'loss': 'epsilon_insensitive', 'C': 1}
Blue
Red
0.18792127539665793
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 1}
Blue
Red
0.19045690858041564
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 2}
Blue
Red
0.18790166683552134
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 2}
Blue
Red
0.18919788585587005
Typer LinearSVR
INNER COUNT NO.  2
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_1_in_2_in.raw
Set size set to 10000
Set size set to 10000
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.
  % (grid_size, self.n_iter, grid_size), UserWarning)
{'loss': 'epsilon_insensitive', 'C': 1}
Blue
Red
0.40008104174475545
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 1}
Blue
Red
0.4022339448274854
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 2}
Blue
Red
0.4000845655932088
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 2}
Blue
Red
0.4011758971096884
Typer LinearSVR
OUTER COUNT NO.  1
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_1_in_2_out.raw
Set size set to 10000
Set size set to 10000
0.39627252602127194
INNER COUNT NO.  1
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_2_in_1_in.raw
Set size set to 10000
Set size set to 10000
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.
  % (grid_size, self.n_iter, grid_size), UserWarning)
{'loss': 'epsilon_insensitive', 'C': 1}
Blue
Red
0.4010008986820497
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 1}
Blue
Red
0.40277905514675605
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 2}
Blue
Red
0.4010099789165076
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 2}
Blue
Red
0.4019212341253958
Typer LinearSVR
INNER COUNT NO.  2
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_2_in_2_in.raw
Set size set to 10000
Set size set to 10000
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.
  % (grid_size, self.n_iter, grid_size), UserWarning)
{'loss': 'epsilon_insensitive', 'C': 1}
Blue
Red
0.4208473339232729
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 1}
Blue
Red
0.42290897756659884
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 2}
Blue
Red
0.4208482290311941
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 2}
Blue
Red
0.4218944795665194
Typer LinearSVR
OUTER COUNT NO.  2
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_2_in_2_out.raw
Set size set to 10000
Set size set to 10000
0.34398193775214114
Traceback (most recent call last):
  File "cv_grid_all_ml.py", line 251, in <module>
    make_param_box_plot(goal_dict, time_dict)
NameError: name 'goal_dict' is not defined
(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ d[K~/j
total 428K
-rw-rw-r-- 1 ciaran ciaran 200K Dec 19 17:27 test_boxplotting.txt
-rw-rw-r-- 1 ciaran ciaran  23K Dec 19 17:26 cv_grid_all_ml.py
-rw-rw-r-- 1 ciaran ciaran  18K Dec 19 17:19 nested_cv_new_name.py
-rw-rw-r-- 1 ciaran ciaran  222 Dec 10 17:40 done
-rw-rw-r-- 1 ciaran ciaran 1.9K Dec  9 11:12 blup_cv.sh
-rwxrwxrwx 1 ciaran ciaran  367 Dec  6 13:19 r2_score.py
(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ cv_grid_all_ml.pyvcv_grid_all_ml.pyicv_grid_all_ml.py cv_grid_all_ml.py
[?1049h[?1h=[1;52r[34l[34h[?25h[23m[24m[0m[H[J[?25l[52;1H"cv_grid_all_ml.py" 391L, 23144C[1;1H[34m#Warning : best model selected by NMAE and R2 might not be the same
#performs linear regression, linear regression, neural network, svm and random forest, LASSO, RIDGE, CNN
#source ~/venv/bin/activate #in python 3.5.2
#print a log to a .txt file!
#model = pickle.load(open('FILEPATH', 'rb')) 
#dependencies = {'coeff_determination':coeff_determination}
#model = tf.keras.models.load_model('FILEPATH', custom_objects=dependencies)
#

#import tensorflow
#import numpy as np; import scipy #need to do this before path insert
#import sys
#sys.path.insert(1, '/external_storage/ciaran/Library/Python/3.7/python/site-packages/')
#import dill as pickle
#sys.path.insert(1, '/external_storage/ciaran/Library/Python/3.7/python/site-packages/nested_cv')
#from nested_cv import NestedCV
#with open('NCV_NN.pkl', 'rb') as f:
#     red = pickle.load(f)[0m

[36mprint[0m([31m"Please remember to set the right set size in the nested_cv code"[0m)
[35mimport[0m sys
sys.path.insert([31m1[0m, [31m'/external_storage/ciaran/Library/Python/3.7/python/site-packages/nested_cv'[0m)
num = sys.argv[[31m1[0m] [34m#script number for saving out[0m
phenfile = [36mstr[0m(sys.argv[[31m2[0m]) [34m#txt file with phenotypes[0m
data = [36mstr[0m(sys.argv[[31m3[0m]) [34m#needs to be same size as set_size[0m
snps = [36mstr[0m(sys.argv[[31m4[0m]) [34m#top or shuf[0m
phenotype = [36mstr[0m(sys.argv[[31m5[0m]) [34m#make a directory for the results[0m
set_size = [36mint[0m(sys.argv[[31m6[0m]) [34m#how many SNPs[0m
[35mfrom[0m nested_cv [35mimport[0m NestedCV
[35mimport[0m statistics
[35mimport[0m numpy [33mas[0m np
[35mimport[0m sklearn
[35mimport[0m seaborn [33mas[0m sns
[35mfrom[0m sklearn [35mimport[0m preprocessing
[35mfrom[0m sklearn.pipeline [35mimport[0m make_pipeline, Pipeline
[35mfrom[0m sklearn.preprocessing [35mimport[0m StandardScaler
[35mimport[0m datetime
[35mimport[0m time
[35mfrom[0m statistics [35mimport[0m mean
[35mimport[0m matplotlib.pyplot [33mas[0m plt
[35mfrom[0m matplotlib [35mimport[0m style
[35mfrom[0m sklearn.svm [35mimport[0m LinearSVR
[35mfrom[0m sklearn.svm [35mimport[0m SVR
[35mfrom[0m sklearn.model_selection [35mimport[0m train_test_split
[35mfrom[0m sklearn.model_selection [35mimport[0m RandomizedSearchCV
[35mfrom[0m sklearn.linear_model [35mimport[0m LinearRegression, Lasso, Ridge
[34m#import pickle #use dill instead below[0m
[35mfrom[0m sklearn.ensemble [35mimport[0m RandomForestRegressor [34m#based in part on https://towardsdatascience.com/random-forest-in-python-24d0893d51c0[0m
[35mfrom[0m statistics [35mimport[0m mean
[35mimport[0m os
[35mimport[0m pandas [33mas[0m pd [34m# data processing, CSV file I/O (e.g. pd.read_csv)[0m[52;194H1,1[11CTop[1;1H[34h[?25h[?25l[52;1H[K[52;1H/[34h[?25ha[?25l[34h[?25hk[?25l[34h[?25he[?25l[34h[?25h_[?25l[34h[?25hp[?25l[34h[?25h[?25l[52;6H[K[52;6H[34h[?25h[?25l[52;5H[K[52;5H[34h[?25h[?25l[52;4H[K[52;4H[34h[?25h[?25l[52;3H[K[52;3H[34h[?25h[?25l[52;2H[K[52;2H[34h[?25hm[?25l[34h[?25ha[?25l[34h[?25hk[?25l[34h[?25he[?25l[34h[?25h_[?25l[34h[?25hp[?25l[34h[?25ha[?25l[34h[?25hr[?25l[34h[?25ha[?25l[34h[?25hm[?25l[34h[?25h[?25l[23m[24m[0m[H[J[1;5Hrestored_model = deserialize(model)
    [33mif[0m training_config [33mis[0m [33mnot[0m [36mNone[0m:[3;9Hrestored_model.compile([4;13H**saving_utils.compile_args_from_training_config([5;17Htraining_config[6;13H)[7;9H)
    restored_model.set_weights(weights)
    [33mreturn[0m restored_model

[34m# Hotfix function[0m
[33mdef[0m [36mmake_keras_picklable[0m():[14;5H[33mdef[0m [36m__reduce__[0m(self):[15;9Hmodel_metadata = saving_utils.model_metadata(self)[16;9Htraining_config = model_metadata.get([31m"training_config"[0m, [36mNone[0m)[17;9Hmodel = serialize(self)[18;9Hweights = self.get_weights()[19;9H[33mreturn[0m (unpack, (model, training_config, weights))[21;5Hcls = Model
    cls.__reduce__ = __reduce__


[33mdef[0m [36mmake_param_box_plot[0m(goal_dict, time_dict):[26;9H[33mfor[0m param [33min[0m goal_dict:[27;17Hplt.subplots([31m1[0m,[31m2[0m,figsize=([31m12[0m,[31m8[0m))[28;17Hplt.subplot([31m121[0m)[29;17Hplt.boxplot(goal_dict[param].values(), bootstrap=[36mNone[0m,showmeans=[36mFalse[0m, meanline=[36mFalse[0m, notch=[36mTrue[0m,labels=goal_dict[param].keys()) [34m#orange line is median, green dotted line is mean[0m[30;17Hplt.xlabel([36mstr[0m(param).upper(), fontsize=[31m10[0m, fontweight=[31m'bold'[0m)[31;17Hplt.ylabel([31m'R^2'[0m, fontsize=[31m10[0m,fontweight=[31m'bold'[0m)[32;17Hplt.title([31m'R^2 Score vs %s'[0m % param, fontsize=[31m14[0m, fontweight=[31m'bold'[0m)[33;17H[33mif[0m param == [31m'initialization'[0m:[34;25Hplt.xticks(fontsize=[31m6[0m)[35;17Hplt.subplot([31m122[0m)[36;17Hplt.boxplot(time_dict[param].values(), bootstrap=[36mNone[0m,showmeans=[36mFalse[0m, meanline=[36mFalse[0m, notch=[36mFalse[0m,labels=time_dict[param].keys())[37;17Hplt.xlabel([36mstr[0m(param).upper(), fontsize=[31m10[0m, fontweight=[31m'bold'[0m)[38;17Hplt.ylabel([31m'Training Time'[0m, fontsize=[31m10[0m,fontweight=[31m'bold'[0m)[39;17Hplt.title([31m'Training Time vs %s'[0m % param, fontsize=[31m14[0m, fontweight=[31m'bold'[0m)[40;17Hplt.tight_layout(pad=[31m4[0m)[41;17H[33mif[0m param == [31m'initialization'[0m:[42;25Hplt.xticks(fontsize=[31m6[0m)[43;17Hmy_fig_name = [31m"plots_of_"[0m + [36mstr[0m(param) + [31m'_'[0m + [36mstr[0m([31m"{:%Y_%m_%d}"[0m.format(datetime.datetime.now())) + [31m'_'[0m +[36mstr[0m(snps) +[36mstr[0m(num)+ [31m".png"[0m[44;17Hplt.savefig(my_fig_name, dpi=[31m300[0m)[45;17Hplt.show()[46;17Hplt.clf()[47;17Hplt.close()

[33mdef[0m [36mmake_goal_dict[0m(whole_dict):[50;9H[36mprint[0m(whole_dict)[51;9Hgoal_dict = {key:{} [33mfor[0m key [33min[0m whole_dict}[52;194H167,5[9C41%[25;5H[34h[?25h[?25l[52;1H/make_param[52;194H[K[52;1H[23m[24m[0m[H[J[1;17H[33mfor[0m item [33min[0m whole_dict[key]:[2;25Hgoal_dict[key][item] = [][3;9Htime_dict = {key:{} [33mfor[0m key [33min[0m whole_dict} [34m#both empty[0m[4;9H[33mfor[0m key [33min[0m whole_dict:[5;17H[33mfor[0m item [33min[0m whole_dict[key]:[6;25Htime_dict[key][item] = [][7;9H[33mreturn[0m goal_dict, time_dict

x_train, y_train = load_data(data)
name_list = np.loadtxt(data, skiprows=[31m1[0m, usecols=([31m0[0m,), dtype=[31m'str'[0m)

scaler = preprocessing.StandardScaler().fit(y_train)
[34m#pickle.dump(scaler, open('scaler.pkl', 'wb'))
#scaler = pickle.load(open('scaler.pkl', 'rb'))[0m

y_train = scaler.transform(y_train)

n_snps = x_train.shape[[31m1[0m]
my_cv = sklearn.model_selection.KFold(n_splits=[31m10[0m, shuffle=[36mTrue[0m, random_state=[31m42[0m)
[34m#################################################SVM####SVM#####SVM####################################################################[0m
[33mdef[0m [36mncv_results[0m(analysis, ncv_object):[22;9H[36mprint[0m([31m"Best Params of %s is %s "[0m % (analysis, ncv_object.best_params))[23;9H[36mprint[0m([31m"Outer scores of %s is %s and mean is %s"[0m % (analysis, ncv_object.outer_scores, np.mean(ncv_object.outer_scores)))[24;9H[36mprint[0m([31m"Variance of %s is %s "[0m % (analysis, ncv_object.variance))[25;9H[36mprint[0m([31m"Goal dict of %s is %s "[0m % (analysis, ncv_object.goal_dict))[26;9Hmake_param_box_plot(ncv_object.goal_dict, ncv_object.time_dict)[27;9H[33mwith[0m [36mopen[0m([31m'NCV_'[0m + [36mstr[0m(analysis) + [31m'_'[0m +  [36mstr[0m(snps) + [31m'_'[0m + [36mstr[0m(phenotype) + [31m'_'[0m + [36mstr[0m(num) + [31m'.pkl'[0m, [31m'wb'[0m) [33mas[0m ncvfile: [34m#with open("fname.pkl", 'rb') as ncvfile:[0m[28;17Hpickle.dump(ncv_object, ncvfile) [34m#ncv_object = pickle.load(ncvfile)[0m

[33mdef[0m [36mnn_results[0m(analysis, ncv_object):[31;9H[36mprint[0m([31m"Best Params of %s is %s "[0m % (analysis, ncv_object.best_params))[32;9H[36mprint[0m([31m"Outer scores of %s is %s and mean is %s"[0m % (analysis, ncv_object.outer_scores, np.mean(ncv_object.outer_scores)))[33;9H[36mprint[0m([31m"Variance of %s is %s "[0m % (analysis, ncv_object.variance))[34;9Hnn_list = [ncv_object.best_inner_params_list, ncv_object.best_inner_score_list, ncv_object.best_params, ncv_object.metric, ncv_object.outer_scores, ncv_object.variance][35;9H[33mwith[0m [36mopen[0m([31m'NCV_'[0m + [36mstr[0m(analysis) + [31m'_'[0m +  [36mstr[0m(snps) + [31m'_'[0m + [36mstr[0m(phenotype) + [31m'_'[0m + [36mstr[0m(num) + [31m'.pkl'[0m, [31m'wb'[0m) [33mas[0m ncvfile:[36;17Hpickle.dump(nn_list, ncvfile) [34m#ncv_object = pickle.load(ncvfile)[0m[37;9Hncv_object.model.model.save([31m"model_"[0m + [36mstr[0m(analysis) + [31m'_'[0m +  [36mstr[0m(snps) + [31m'_'[0m + [36mstr[0m(phenotype) + [31m'_'[0m + [36mstr[0m(num) + [31m".h5"[0m)


[36mprint[0m([31m"Performing SVM"[0m)
c_param = [[31m1[0m,[31m2[0m]
gamma_param = [[36mfloat[0m(x) [33mfor[0m x [33min[0m np.linspace([31m0.1[0m, [31m1[0m, [31m4[0m)]


epsilon_param = [[36mfloat[0m(x) [33mfor[0m x [33min[0m np.linspace([31m0.1[0m, [31m1[0m, [31m4[0m)]
loss_param = [[31m'epsilon_insensitive'[0m, [31m'squared_epsilon_insensitive'[0m]
kernel_param = [[31m'poly'[0m]
degree = [[31m1[0m,[31m2[0m,[31m3[0m]
svm_random_grid = {[31m'gamma'[0m:gamma_param, [31m'C'[0m:c_param,[31m'kernel'[0m:kernel_param, [31m"degree"[0m:degree}
[36mprint[0m(svm_random_grid)
svm_random_grid2 = {[31m'C'[0m : c_param, [31m'loss'[0m:loss_param}[52;194H220,9[9C57%[26;9H[34h[?25h[?25l[52;198H10[26;10H[34h[?25h[?25l[52;196H1[27;10H[34h[?25h[?25l[52;196H2[28;10H[34h[?25h[?25l[52;196H3,0-1[29;1H[34h[?25h[?25l[52;196H4,10 [30;10H[34h[?25h[?25l[52;196H5[31;10H[34h[?25h[?25l[52;196H4[30;10H[34h[?25h[?25l[52;196H3,0-1[29;1H[34h[?25h[?25l[52;196H2,10 [28;10H[34h[?25h[?25l[52;196H1[27;10H[34h[?25h[?25l[52;196H0[26;10H[34h[?25h[?25l[52;195H19[25;10H[34h[?25h[?25l[52;195H20[26;10H[34h[?25h[?25l[52;196H1[27;10H[34h[?25h[?25l[52;1H/make_param[52;194H[K[52;1H[1;51r[1;1H[7M[1;52r[45;1H[36mprint[0m(svm_random_grid2)
rbg_goal_dict, rbg_time_dict = make_goal_dict(svm_random_grid)
svm_goal_dict, svm_time_dict = make_goal_dict(svm_random_grid2)
SVM_NCV = NestedCV(model_name=[31m'LinearSVR'[0m, name_list = name_list, model=LinearSVR(), goal_dict=svm_goal_dict, time_dict=svm_time_dict, params_grid=svm_random_grid2, outer_kfolds=[31m2[0m, inner_kfolds=[31m2[0m, n_jobs = [31m8[0m,cv__[49;1Hoptions={[31m'randomized_search'[0m:[36mTrue[0m, [31m'randomized_search_iter'[0m:[31m10[0m, [31m'sqrt_of_score'[0m:[36mFalse[0m,[31m'recursive_feature_elimination'[0m:[36mFalse[0m, [31m'metric'[0m:sklearn.metrics.r2_score, [31m'metric_score_indicator_lower'[0m:[36mFalse[0m})
SVM_NCV.fit(x_train, y_train.ravel(), name_list=name_list, phenfile=phenfile, set_size=set_size, snps=snps, model_name=[31m'SVM'[0m, goal_dict=svm_goal_dict, time_dict=svm_time_dict)
make_param_box_plot(goal_dict, time_dict)[52;1H[K[52;194H251,1[9C58%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H252,0-1[7C59%[51;1H[34h[?25h[?25l[52;196H1,1  [50;1H[34h[?25h[?25l[52;196H2,0-1[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hncv_results([31m'SVM'[0m, SVM_NCV)[52;194H[K[52;194H253,1[9C59%[51;1H[34h[?25h[?25l[52;196H2,0-1[50;1H[34h[?25h[?25l[52;196H1,1  [49;1H[34h[?25h[?25l[52;196H0[48;1H[34h[?25h[?25l[52;196H1[49;1H[34h[?25h[?25l[52;196H2,0-1[50;1H[34h[?25h[?25l[52;196H3,1  [51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31m'''[0m[52;194H[K[52;194H254,1[9C59%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mprint("Performing RBG")[0m[52;194H[K[52;194H255,1[9C60%[51;1H[34h[?25h[?25l[1;51r[1;1H[2M[1;52r[50;1H[31mRBG_NCV = NestedCV(model_name='RBG', name_list=name_list, model=SVR(), params_grid=svm_random_grid, outer_kfolds=4, inner_kfolds=4, n_jobs = 8,cv_options={'randomized_search':True, 'randomized_search_iter':50, ''[51;1Hsqrt_of_score':False,'recursive_feature_elimination':False, 'metric':sklearn.metrics.r2_score, 'metric_score_indicator_lower':False})[0m[52;194H[K[52;194H256,1[9C60%[50;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mRBG_NCV.fit(x_train, y_train.ravel(), name_list=name_list, phenfile=phenfile, set_size=set_size, snps=snps, model_name='RBG')[0m[52;194H[K[52;194H257,1[9C60%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mncv_results('RBG', RBG_NCV)[0m[52;194H[K[52;194H258,1[9C61%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H259,0-1[7C61%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mprint("Performing LASSO")[0m[52;194H[K[52;194H260,1[9C61%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31malpha = [0.0001, 0.001, 0.01, 0.1, 0.5, 1, 10, 100][0m[52;194H[K[52;194H261,1[9C61%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31malpha_dict = {'alpha':alpha}[0m[52;194H[K[52;194H262,1[9C62%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mprint(alpha_dict)[0m[52;194H[K[52;194H263,1[9C62%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31malpha_name_dict = {'alpha':"Alpha"}[0m[52;194H[K[52;194H264,1[9C62%[51;1H[34h[?25h[?25l[1;51r[1;1H[2M[1;52r[50;1H[31mLASS_NCV = NestedCV(model_name='LASS', name_list=name_list, model=Lasso(), params_grid=alpha_dict, outer_kfolds=4, inner_kfolds=4, n_jobs = 8,cv_options={'randomized_search':True, 'randomized_search_iter':50, 'ss[51;1Hqrt_of_score':False,'recursive_feature_elimination':False, 'metric':sklearn.metrics.r2_score, 'metric_score_indicator_lower':False})[0m[52;194H[K[52;194H265,1[9C63%[50;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mLASS_NCV.fit(x_train, y_train.ravel(), name_list=name_list, phenfile=phenfile, set_size=set_size, snps=snps, model_name='LASS')[0m[52;194H[K[52;194H266,1[9C63%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mncv_results('LASS', LASS_NCV)[0m[52;194H[K[52;194H267,1[9C63%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mprint("Performing Ridge")[0m[52;194H[K[52;194H268,1[9C64%[51;1H[34h[?25h[?25l[1;51r[1;1H[2M[1;52r[50;1H[31mRIDGE_NCV = NestedCV(model_name='RIDGE', name_list=name_list, model=Ridge(), params_grid=alpha_dict, outer_kfolds=4, inner_kfolds=4, n_jobs = 8,cv_options={'randomized_search':True, 'randomized_search_iter':50,  [51;1H'sqrt_of_score':False,'recursive_feature_elimination':False, 'metric':sklearn.metrics.r2_score, 'metric_score_indicator_lower':False})[0m[52;194H[K[52;194H269,1[9C64%[50;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mRIDGE_NCV.fit(x_train, y_train.ravel(), name_list=name_list, phenfile=phenfile, set_size=set_size, snps=snps, model_name='RIDGE')[0m[52;194H[K[52;194H270,1[9C64%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mncv_results('RIDGE', RIDGE_NCV)[0m[52;194H[K[52;194H271,1[9C65%[51;1H[34h[?25h[?25l[52;196H0[50;1H[34h[?25h[?25l[52;195H69[48;1H[34h[?25h[?25l[52;196H8[47;1H[34h[?25h[?25l[52;196H7[46;1H[34h[?25h[?25l[52;196H6[45;1H[34h[?25h[?25l[52;196H5[43;1H[34h[?25h[?25l[52;196H4[42;1H[34h[?25h[?25l[52;196H3[41;1H[34h[?25h[?25l[52;196H2[40;1H[34h[?25h[?25l[52;196H1[39;1H[34h[?25h[?25l[52;196H0[38;1H[34h[?25h[?25l[52;195H59,0-1[37;1H[34h[?25h[?25l[52;196H8,1  [36;1H[34h[?25h[?25l[52;196H7[35;1H[34h[?25h[?25l[52;196H6[33;1H[34h[?25h[?25l[52;196H5[32;1H[34h[?25h[?25l[52;196H4[31;1H[34h[?25h[?25l[52;196H3[30;1H[34h[?25h[?25l[52;196H4[31;1H[34h[?25h[?25l[52;196H3[30;1H[34h[?25h[?25l[52;196H2,0-1[29;1H[34h[?25h[?25l[52;196H3,1  [30;1H[34h[?25h[?25l[52;196H4[31;1H[34h[?25h[?25l[52;196H3[30;1H[34h[?25h[?25l[52;196H2,0-1[29;1H[34h[?25h[?25l[52;196H3,1  [30;1H[34h[?25h[?25l[52;196H4[31;1H[34h[?25h[?25l[52;196H3[30;1H[34h[?25h[?25l[52;196H2,0-1[29;1H[34h[?25h[?25l[52;196H1,1  [28;1H[34h[?25h[?25l[52;196H2,0-1[29;1H[34h[?25h[?25l[52;196H3,1  [30;1H[34h[?25h[?25l[52;196H4[31;1H[34h[?25h[?25l[52;196H5[32;1H[34h[?25h[?25l[52;196H6[33;1H[34h[?25h[?25l[52;196H7[35;1H[34h[?25h[?25l[52;196H6[33;1H[34h[?25h[?25l[52;196H5[32;1H[34h[?25h[?25l[52;196H4[31;1H[34h[?25h[?25l[52;196H5[32;1H[34h[?25h[?25l[52;196H4[31;1H[34h[?25h[?25l[52;196H3[30;1H[34h[?25h[?25l[52;196H2,0-1[29;1H[34h[?25h[?25l[52;196H1,1  [28;1H[34h[?25h[?25l[52;196H0[27;1H[34h[?25h[?25l[52;195H49[25;1H[34h[?25h[?25l[52;196H8[24;1H[34h[?25h[?25l[52;196H7[23;1H[34h[?25h[?25l[52;196H6[22;1H[34h[?25h[?25l[52;196H5[21;1H[34h[?25h[?25l[52;196H4[20;1H[34h[?25h[?25l[52;196H3[19;1H[34h[?25h[?25l[52;196H4[20;1H[34h[?25h[?25l[52;196H5[21;1H[34h[?25h[?25l[52;196H6[22;1H[34h[?25h[?25l[52;196H7[23;1H[34h[?25h[?25l[52;196H8[24;1H[34h[?25h[?25l[52;196H9[25;1H[34h[?25h[?25l[52;195H50[27;1H[34h[?25h[?25l[52;196H1[28;1H[34h[?25h[?25l[52;196H2,0-1[29;1H[34h[?25h[?25l[52;196H3,1  [30;1H[34h[?25h[?25l[52;196H4[31;1H[34h[?25h[?25l[52;196H5[32;1H[34h[?25h[?25l[52;196H4[31;1H[34h[?25h[?25l[52;196H3[30;1H[34h[?25h[?25l[52;196H2,0-1[29;1H[34h[?25h[?25l[52;196H1,1  [28;1H[34h[?25h[?25l[52;196H0[27;1H[34h[?25h[?25l[52;195H49[25;1H[34h[?25h[?25l[52;196H8[24;1H[34h[?25h[?25l[52;196H7[23;1H[34h[?25h[?25l[52;194H[K[52;1H:[34h[?25hw[?25l[34h[?25h[?25l[52;2H[K[52;2H[34h[?25hq[?25l[34h[?25h![?25l[34h[?25h[?25l[52;1H[K[52;1H[?1l>[34h[?25h[?1049l(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ vi cv_grid_all_ml.py
[?1049h[?1h=[1;52r[34l[34h[?25h[23m[24m[0m[H[J[?25l[52;1H"cv_grid_all_ml.py" 391L, 23144C[1;1H[34m#Warning : best model selected by NMAE and R2 might not be the same
#performs linear regression, linear regression, neural network, svm and random forest, LASSO, RIDGE, CNN
#source ~/venv/bin/activate #in python 3.5.2
#print a log to a .txt file!
#model = pickle.load(open('FILEPATH', 'rb')) 
#dependencies = {'coeff_determination':coeff_determination}
#model = tf.keras.models.load_model('FILEPATH', custom_objects=dependencies)
#

#import tensorflow
#import numpy as np; import scipy #need to do this before path insert
#import sys
#sys.path.insert(1, '/external_storage/ciaran/Library/Python/3.7/python/site-packages/')
#import dill as pickle
#sys.path.insert(1, '/external_storage/ciaran/Library/Python/3.7/python/site-packages/nested_cv')
#from nested_cv import NestedCV
#with open('NCV_NN.pkl', 'rb') as f:
#     red = pickle.load(f)[0m

[36mprint[0m([31m"Please remember to set the right set size in the nested_cv code"[0m)
[35mimport[0m sys
sys.path.insert([31m1[0m, [31m'/external_storage/ciaran/Library/Python/3.7/python/site-packages/nested_cv'[0m)
num = sys.argv[[31m1[0m] [34m#script number for saving out[0m
phenfile = [36mstr[0m(sys.argv[[31m2[0m]) [34m#txt file with phenotypes[0m
data = [36mstr[0m(sys.argv[[31m3[0m]) [34m#needs to be same size as set_size[0m
snps = [36mstr[0m(sys.argv[[31m4[0m]) [34m#top or shuf[0m
phenotype = [36mstr[0m(sys.argv[[31m5[0m]) [34m#make a directory for the results[0m
set_size = [36mint[0m(sys.argv[[31m6[0m]) [34m#how many SNPs[0m
[35mfrom[0m nested_cv [35mimport[0m NestedCV
[35mimport[0m statistics
[35mimport[0m numpy [33mas[0m np
[35mimport[0m sklearn
[35mimport[0m seaborn [33mas[0m sns
[35mfrom[0m sklearn [35mimport[0m preprocessing
[35mfrom[0m sklearn.pipeline [35mimport[0m make_pipeline, Pipeline
[35mfrom[0m sklearn.preprocessing [35mimport[0m StandardScaler
[35mimport[0m datetime
[35mimport[0m time
[35mfrom[0m statistics [35mimport[0m mean
[35mimport[0m matplotlib.pyplot [33mas[0m plt
[35mfrom[0m matplotlib [35mimport[0m style
[35mfrom[0m sklearn.svm [35mimport[0m LinearSVR
[35mfrom[0m sklearn.svm [35mimport[0m SVR
[35mfrom[0m sklearn.model_selection [35mimport[0m train_test_split
[35mfrom[0m sklearn.model_selection [35mimport[0m RandomizedSearchCV
[35mfrom[0m sklearn.linear_model [35mimport[0m LinearRegression, Lasso, Ridge
[34m#import pickle #use dill instead below[0m
[35mfrom[0m sklearn.ensemble [35mimport[0m RandomForestRegressor [34m#based in part on https://towardsdatascience.com/random-forest-in-python-24d0893d51c0[0m
[35mfrom[0m statistics [35mimport[0m mean
[35mimport[0m os
[35mimport[0m pandas [33mas[0m pd [34m# data processing, CSV file I/O (e.g. pd.read_csv)[0m[52;194H1,1[11CTop[1;1H[34h[?25h[?25l[52;194H2[2;1H[34h[?25h[?25l[52;1H[K[52;1H/[34h[?25hm[?25l[34h[?25ha[?25l[34h[?25hk[?25l[34h[?25he[?25l[34h[?25h_[?25l[34h[?25hb[?25l[34h[?25ho[?25l[34h[?25hx[?25l[34h[?25h[?25l[31msearch hit BOTTOM, continuing at TOP[0m[1m[37m[41mE486: Pattern not found: make_box[0m[52;34H[K[52;194H2,1[11CTop[2;1H[34h[?25h[?25l[52;1H[K[52;1H/[34h[?25hm[?25l[34h[?25ha[?25l[34h[?25hk[?25l[34h[?25he[?25l[34h[?25h_[?25l[34h[?25hp[?25l[34h[?25ha[?25l[34h[?25hr[?25l[34h[?25ha[?25l[34h[?25hm[?25l[34h[?25h[?25l[23m[24m[0m[H[J[1;5Hrestored_model = deserialize(model)
    [33mif[0m training_config [33mis[0m [33mnot[0m [36mNone[0m:[3;9Hrestored_model.compile([4;13H**saving_utils.compile_args_from_training_config([5;17Htraining_config[6;13H)[7;9H)
    restored_model.set_weights(weights)
    [33mreturn[0m restored_model

[34m# Hotfix function[0m
[33mdef[0m [36mmake_keras_picklable[0m():[14;5H[33mdef[0m [36m__reduce__[0m(self):[15;9Hmodel_metadata = saving_utils.model_metadata(self)[16;9Htraining_config = model_metadata.get([31m"training_config"[0m, [36mNone[0m)[17;9Hmodel = serialize(self)[18;9Hweights = self.get_weights()[19;9H[33mreturn[0m (unpack, (model, training_config, weights))[21;5Hcls = Model
    cls.__reduce__ = __reduce__


[33mdef[0m [36mmake_param_box_plot[0m(goal_dict, time_dict):[26;9H[33mfor[0m param [33min[0m goal_dict:[27;17Hplt.subplots([31m1[0m,[31m2[0m,figsize=([31m12[0m,[31m8[0m))[28;17Hplt.subplot([31m121[0m)[29;17Hplt.boxplot(goal_dict[param].values(), bootstrap=[36mNone[0m,showmeans=[36mFalse[0m, meanline=[36mFalse[0m, notch=[36mTrue[0m,labels=goal_dict[param].keys()) [34m#orange line is median, green dotted line is mean[0m[30;17Hplt.xlabel([36mstr[0m(param).upper(), fontsize=[31m10[0m, fontweight=[31m'bold'[0m)[31;17Hplt.ylabel([31m'R^2'[0m, fontsize=[31m10[0m,fontweight=[31m'bold'[0m)[32;17Hplt.title([31m'R^2 Score vs %s'[0m % param, fontsize=[31m14[0m, fontweight=[31m'bold'[0m)[33;17H[33mif[0m param == [31m'initialization'[0m:[34;25Hplt.xticks(fontsize=[31m6[0m)[35;17Hplt.subplot([31m122[0m)[36;17Hplt.boxplot(time_dict[param].values(), bootstrap=[36mNone[0m,showmeans=[36mFalse[0m, meanline=[36mFalse[0m, notch=[36mFalse[0m,labels=time_dict[param].keys())[37;17Hplt.xlabel([36mstr[0m(param).upper(), fontsize=[31m10[0m, fontweight=[31m'bold'[0m)[38;17Hplt.ylabel([31m'Training Time'[0m, fontsize=[31m10[0m,fontweight=[31m'bold'[0m)[39;17Hplt.title([31m'Training Time vs %s'[0m % param, fontsize=[31m14[0m, fontweight=[31m'bold'[0m)[40;17Hplt.tight_layout(pad=[31m4[0m)[41;17H[33mif[0m param == [31m'initialization'[0m:[42;25Hplt.xticks(fontsize=[31m6[0m)[43;17Hmy_fig_name = [31m"plots_of_"[0m + [36mstr[0m(param) + [31m'_'[0m + [36mstr[0m([31m"{:%Y_%m_%d}"[0m.format(datetime.datetime.now())) + [31m'_'[0m +[36mstr[0m(snps) +[36mstr[0m(num)+ [31m".png"[0m[44;17Hplt.savefig(my_fig_name, dpi=[31m300[0m)[45;17Hplt.show()[46;17Hplt.clf()[47;17Hplt.close()

[33mdef[0m [36mmake_goal_dict[0m(whole_dict):[50;9H[36mprint[0m(whole_dict)[51;9Hgoal_dict = {key:{} [33mfor[0m key [33min[0m whole_dict}[52;194H167,5[9C41%[25;5H[34h[?25h[?25l[52;1H/make_param[52;194H[K[52;1H[23m[24m[0m[H[J[1;17H[33mfor[0m item [33min[0m whole_dict[key]:[2;25Hgoal_dict[key][item] = [][3;9Htime_dict = {key:{} [33mfor[0m key [33min[0m whole_dict} [34m#both empty[0m[4;9H[33mfor[0m key [33min[0m whole_dict:[5;17H[33mfor[0m item [33min[0m whole_dict[key]:[6;25Htime_dict[key][item] = [][7;9H[33mreturn[0m goal_dict, time_dict

x_train, y_train = load_data(data)
name_list = np.loadtxt(data, skiprows=[31m1[0m, usecols=([31m0[0m,), dtype=[31m'str'[0m)

scaler = preprocessing.StandardScaler().fit(y_train)
[34m#pickle.dump(scaler, open('scaler.pkl', 'wb'))
#scaler = pickle.load(open('scaler.pkl', 'rb'))[0m

y_train = scaler.transform(y_train)

n_snps = x_train.shape[[31m1[0m]
my_cv = sklearn.model_selection.KFold(n_splits=[31m10[0m, shuffle=[36mTrue[0m, random_state=[31m42[0m)
[34m#################################################SVM####SVM#####SVM####################################################################[0m
[33mdef[0m [36mncv_results[0m(analysis, ncv_object):[22;9H[36mprint[0m([31m"Best Params of %s is %s "[0m % (analysis, ncv_object.best_params))[23;9H[36mprint[0m([31m"Outer scores of %s is %s and mean is %s"[0m % (analysis, ncv_object.outer_scores, np.mean(ncv_object.outer_scores)))[24;9H[36mprint[0m([31m"Variance of %s is %s "[0m % (analysis, ncv_object.variance))[25;9H[36mprint[0m([31m"Goal dict of %s is %s "[0m % (analysis, ncv_object.goal_dict))[26;9Hmake_param_box_plot(ncv_object.goal_dict, ncv_object.time_dict)[27;9H[33mwith[0m [36mopen[0m([31m'NCV_'[0m + [36mstr[0m(analysis) + [31m'_'[0m +  [36mstr[0m(snps) + [31m'_'[0m + [36mstr[0m(phenotype) + [31m'_'[0m + [36mstr[0m(num) + [31m'.pkl'[0m, [31m'wb'[0m) [33mas[0m ncvfile: [34m#with open("fname.pkl", 'rb') as ncvfile:[0m[28;17Hpickle.dump(ncv_object, ncvfile) [34m#ncv_object = pickle.load(ncvfile)[0m

[33mdef[0m [36mnn_results[0m(analysis, ncv_object):[31;9H[36mprint[0m([31m"Best Params of %s is %s "[0m % (analysis, ncv_object.best_params))[32;9H[36mprint[0m([31m"Outer scores of %s is %s and mean is %s"[0m % (analysis, ncv_object.outer_scores, np.mean(ncv_object.outer_scores)))[33;9H[36mprint[0m([31m"Variance of %s is %s "[0m % (analysis, ncv_object.variance))[34;9Hnn_list = [ncv_object.best_inner_params_list, ncv_object.best_inner_score_list, ncv_object.best_params, ncv_object.metric, ncv_object.outer_scores, ncv_object.variance][35;9H[33mwith[0m [36mopen[0m([31m'NCV_'[0m + [36mstr[0m(analysis) + [31m'_'[0m +  [36mstr[0m(snps) + [31m'_'[0m + [36mstr[0m(phenotype) + [31m'_'[0m + [36mstr[0m(num) + [31m'.pkl'[0m, [31m'wb'[0m) [33mas[0m ncvfile:[36;17Hpickle.dump(nn_list, ncvfile) [34m#ncv_object = pickle.load(ncvfile)[0m[37;9Hncv_object.model.model.save([31m"model_"[0m + [36mstr[0m(analysis) + [31m'_'[0m +  [36mstr[0m(snps) + [31m'_'[0m + [36mstr[0m(phenotype) + [31m'_'[0m + [36mstr[0m(num) + [31m".h5"[0m)


[36mprint[0m([31m"Performing SVM"[0m)
c_param = [[31m1[0m,[31m2[0m]
gamma_param = [[36mfloat[0m(x) [33mfor[0m x [33min[0m np.linspace([31m0.1[0m, [31m1[0m, [31m4[0m)]


epsilon_param = [[36mfloat[0m(x) [33mfor[0m x [33min[0m np.linspace([31m0.1[0m, [31m1[0m, [31m4[0m)]
loss_param = [[31m'epsilon_insensitive'[0m, [31m'squared_epsilon_insensitive'[0m]
kernel_param = [[31m'poly'[0m]
degree = [[31m1[0m,[31m2[0m,[31m3[0m]
svm_random_grid = {[31m'gamma'[0m:gamma_param, [31m'C'[0m:c_param,[31m'kernel'[0m:kernel_param, [31m"degree"[0m:degree}
[36mprint[0m(svm_random_grid)
svm_random_grid2 = {[31m'C'[0m : c_param, [31m'loss'[0m:loss_param}[52;194H220,9[9C57%[26;9H[34h[?25h[?25l[52;1H/make_param[52;194H[K[52;1H[1;51r[1;1H[7M[1;52r[45;1H[36mprint[0m(svm_random_grid2)
rbg_goal_dict, rbg_time_dict = make_goal_dict(svm_random_grid)
svm_goal_dict, svm_time_dict = make_goal_dict(svm_random_grid2)
SVM_NCV = NestedCV(model_name=[31m'LinearSVR'[0m, name_list = name_list, model=LinearSVR(), goal_dict=svm_goal_dict, time_dict=svm_time_dict, params_grid=svm_random_grid2, outer_kfolds=[31m2[0m, inner_kfolds=[31m2[0m, n_jobs = [31m8[0m,cv__[49;1Hoptions={[31m'randomized_search'[0m:[36mTrue[0m, [31m'randomized_search_iter'[0m:[31m10[0m, [31m'sqrt_of_score'[0m:[36mFalse[0m,[31m'recursive_feature_elimination'[0m:[36mFalse[0m, [31m'metric'[0m:sklearn.metrics.r2_score, [31m'metric_score_indicator_lower'[0m:[36mFalse[0m})
SVM_NCV.fit(x_train, y_train.ravel(), name_list=name_list, phenfile=phenfile, set_size=set_size, snps=snps, model_name=[31m'SVM'[0m, goal_dict=svm_goal_dict, time_dict=svm_time_dict)
make_param_box_plot(goal_dict, time_dict)[52;1H[K[52;194H251,1[9C58%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H252,0-1[7C59%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hncv_results([31m'SVM'[0m, SVM_NCV)[52;194H[K[52;194H253,1[9C59%[51;1H[34h[?25h[?25l[52;196H2,0-1[50;1H[34h[?25h[?25l[52;196H1,1  [49;1H[34h[?25h[?25l


[1m-- INSERT --[0m[52;194H[K[52;194H251,1[9C59%[49;1H[34h[?25h[?25l[34m#make_param_box_plot(goal_dict, time_dict)[0m[52;198H2[49;2H[34h[?25h[52;1H[K[49;1H[?25l[52;194H251,1[9C59%[49;1H[34h[?25h[?25l[52;194H[K[52;1H:[34h[?25hx[?25l[34h[?25h[?25l"cv_grid_all_ml.py" 391L, 23145C written
[?1l>[34h[?25h[?1049l(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ vi cv_grid_all_ml.py~/j[Kpython cv_grid_all_ml.py 31 /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del /home/ciaran/completed_big_matrix_binary_new_snps_ids__first_10k.raw shuf FT10cv_new_10k 10000M[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cgit add . [K
[KM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cpython cv_grid_all_ml.py 31 /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del /home/ciaran/completed_big_matrix_binary_new_snps_ids__first_10k.raw shuf FT10cv_new_10k 10000
Please remember to set the right set size in the nested_cv code
/external_storage/ciaran/machine_learning2
WARNING THIS IS AN EDITED SCRIPT - Ciaran Kelly 2021 
 Edited with permission under liscence 
 Flex version
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
31
/home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del
/home/ciaran/completed_big_matrix_binary_new_snps_ids_first_10k.raw
shuf
FT10cv_new_10k
10000
2021-12-19 17:43:00
Performing SVM
{'gamma': [0.1, 0.4, 0.7, 1.0], 'C': [1, 2], 'degree': [1, 2, 3], 'kernel': ['poly']}
{'loss': ['epsilon_insensitive', 'squared_epsilon_insensitive'], 'C': [1, 2]}
{'gamma': [0.1, 0.4, 0.7, 1.0], 'C': [1, 2], 'degree': [1, 2, 3], 'kernel': ['poly']}
{'loss': ['epsilon_insensitive', 'squared_epsilon_insensitive'], 'C': [1, 2]}
ADBUCE
INNER COUNT NO.  1
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_1_in_1_in.raw
Set size set to 10000
Set size set to 10000
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.
  % (grid_size, self.n_iter, grid_size), UserWarning)
{'C': 1, 'loss': 'epsilon_insensitive'}
Blue
Red
0.18789261559723158
Typer LinearSVR
{'C': 1, 'loss': 'squared_epsilon_insensitive'}
Blue
Red
0.19044756476145774
Typer LinearSVR
{'C': 2, 'loss': 'epsilon_insensitive'}
Blue
Red
0.18789066563009682
Typer LinearSVR
{'C': 2, 'loss': 'squared_epsilon_insensitive'}
Blue
Red
0.18918761470495593
Typer LinearSVR
INNER COUNT NO.  2
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_1_in_2_in.raw
Set size set to 10000
Set size set to 10000
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.
  % (grid_size, self.n_iter, grid_size), UserWarning)
{'C': 1, 'loss': 'epsilon_insensitive'}
Blue
Red
0.4000789887837073
Typer LinearSVR
{'C': 1, 'loss': 'squared_epsilon_insensitive'}
Blue
Red
0.40223646317261896
Typer LinearSVR
{'C': 2, 'loss': 'epsilon_insensitive'}
Blue
Red
0.40006000582810186
Typer LinearSVR
{'C': 2, 'loss': 'squared_epsilon_insensitive'}
Blue
Red
0.40117290672085093
Typer LinearSVR
OUTER COUNT NO.  1
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_1_in_2_out.raw
Set size set to 10000
Set size set to 10000
0.3962699962327251
INNER COUNT NO.  1
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_2_in_1_in.raw
Set size set to 10000
Set size set to 10000
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.
  % (grid_size, self.n_iter, grid_size), UserWarning)
{'C': 1, 'loss': 'epsilon_insensitive'}
Blue
Red
0.4010067015404032
Typer LinearSVR
{'C': 1, 'loss': 'squared_epsilon_insensitive'}
Blue
Red
0.4027837323682467
Typer LinearSVR
{'C': 2, 'loss': 'epsilon_insensitive'}
Blue
Red
0.4010115793212359
Typer LinearSVR
{'C': 2, 'loss': 'squared_epsilon_insensitive'}
Blue
Red
0.4018973472012837
Typer LinearSVR
INNER COUNT NO.  2
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_2_in_2_in.raw
Set size set to 10000
Set size set to 10000
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.
  % (grid_size, self.n_iter, grid_size), UserWarning)
{'C': 1, 'loss': 'epsilon_insensitive'}
Blue
Red
0.4208475029472114
Typer LinearSVR
{'C': 1, 'loss': 'squared_epsilon_insensitive'}
Blue
Red
0.42289799710756226
Typer LinearSVR
{'C': 2, 'loss': 'epsilon_insensitive'}
Blue
Red
0.42084457420213695
Typer LinearSVR
{'C': 2, 'loss': 'squared_epsilon_insensitive'}
Blue
Red
0.4219020242071133
Typer LinearSVR
OUTER COUNT NO.  2
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_2_in_2_out.raw
Set size set to 10000
Set size set to 10000
0.3439801215021385
Best Params of SVM is {'loss': ['squared_epsilon_insensitive'], 'C': [1]} 
Outer scores of SVM is [0.3962699962327251, 0.3439801215021385] and mean is 0.3701250588674318
Variance of SVM is [0.5401413417212235, 0.4136356791252838] 
Goal dict of SVM is {'C': {1: [0.18789261559723158, 0.19044756476145774, 0.4000789887837073, 0.40223646317261896, 0.4010067015404032, 0.4027837323682467, 0.4208475029472114, 0.42289799710756226], 2: [0.18789066563009682, 0.18918761470495593, 0.40006000582810186, 0.40117290672085093, 0.4010115793212359, 0.4018973472012837, 0.42084457420213695, 0.4219020242071133]}, 'loss': {'epsilon_insensitive': [0.18789261559723158, 0.18789066563009682, 0.4000789887837073, 0.40006000582810186, 0.4010067015404032, 0.4010115793212359, 0.4208475029472114, 0.42084457420213695], 'squared_epsilon_insensitive': [0.19044756476145774, 0.18918761470495593, 0.40223646317261896, 0.40117290672085093, 0.4027837323682467, 0.4018973472012837, 0.42289799710756226, 0.4219020242071133]}} 
Performing Neural Network
Traceback (most recent call last):
  File "cv_grid_all_ml.py", line 343, in <module>
    NN_NCV = NestedCV(model_name='nn_model', name_list = name_list, model=nn_model, params_grid=param_grid, outer_kfolds=4, inner_kfolds=4, n_jobs = 32,cv_options={'randomized_search':True, 'randomized_search_iter':100, 'sqrt_of_score':False,'recursive_feature_elimination':False, 'metric':sklearn.metrics.r2_score, 'metric_score_indicator_lower':False})
TypeError: __init__() missing 2 required positional arguments: 'goal_dict' and 'time_dict'
(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ python cv_grid_all_ml.py 31 /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del /home/ciaran/completed_big_matrix_binary_new_snps_ids__first_10k.raw shuf FT10cv_new_10k 10000M[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cvi cv_grid_all_ml.py[K
[KM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C
[?1049h[?1h=[1;52r[34l[34h[?25h[23m[24m[0m[H[J[?25l[52;1H"cv_grid_all_ml.py" 391L, 23145C[1;1H[34m#Warning : best model selected by NMAE and R2 might not be the same
#performs linear regression, linear regression, neural network, svm and random forest, LASSO, RIDGE, CNN
#source ~/venv/bin/activate #in python 3.5.2
#print a log to a .txt file!
#model = pickle.load(open('FILEPATH', 'rb')) 
#dependencies = {'coeff_determination':coeff_determination}
#model = tf.keras.models.load_model('FILEPATH', custom_objects=dependencies)
#

#import tensorflow
#import numpy as np; import scipy #need to do this before path insert
#import sys
#sys.path.insert(1, '/external_storage/ciaran/Library/Python/3.7/python/site-packages/')
#import dill as pickle
#sys.path.insert(1, '/external_storage/ciaran/Library/Python/3.7/python/site-packages/nested_cv')
#from nested_cv import NestedCV
#with open('NCV_NN.pkl', 'rb') as f:
#     red = pickle.load(f)[0m

[36mprint[0m([31m"Please remember to set the right set size in the nested_cv code"[0m)
[35mimport[0m sys
sys.path.insert([31m1[0m, [31m'/external_storage/ciaran/Library/Python/3.7/python/site-packages/nested_cv'[0m)
num = sys.argv[[31m1[0m] [34m#script number for saving out[0m
phenfile = [36mstr[0m(sys.argv[[31m2[0m]) [34m#txt file with phenotypes[0m
data = [36mstr[0m(sys.argv[[31m3[0m]) [34m#needs to be same size as set_size[0m
snps = [36mstr[0m(sys.argv[[31m4[0m]) [34m#top or shuf[0m
phenotype = [36mstr[0m(sys.argv[[31m5[0m]) [34m#make a directory for the results[0m
set_size = [36mint[0m(sys.argv[[31m6[0m]) [34m#how many SNPs[0m
[35mfrom[0m nested_cv [35mimport[0m NestedCV
[35mimport[0m statistics
[35mimport[0m numpy [33mas[0m np
[35mimport[0m sklearn
[35mimport[0m seaborn [33mas[0m sns
[35mfrom[0m sklearn [35mimport[0m preprocessing
[35mfrom[0m sklearn.pipeline [35mimport[0m make_pipeline, Pipeline
[35mfrom[0m sklearn.preprocessing [35mimport[0m StandardScaler
[35mimport[0m datetime
[35mimport[0m time
[35mfrom[0m statistics [35mimport[0m mean
[35mimport[0m matplotlib.pyplot [33mas[0m plt
[35mfrom[0m matplotlib [35mimport[0m style
[35mfrom[0m sklearn.svm [35mimport[0m LinearSVR
[35mfrom[0m sklearn.svm [35mimport[0m SVR
[35mfrom[0m sklearn.model_selection [35mimport[0m train_test_split
[35mfrom[0m sklearn.model_selection [35mimport[0m RandomizedSearchCV
[35mfrom[0m sklearn.linear_model [35mimport[0m LinearRegression, Lasso, Ridge
[34m#import pickle #use dill instead below[0m
[35mfrom[0m sklearn.ensemble [35mimport[0m RandomForestRegressor [34m#based in part on https://towardsdatascience.com/random-forest-in-python-24d0893d51c0[0m
[35mfrom[0m statistics [35mimport[0m mean
[35mimport[0m os
[35mimport[0m pandas [33mas[0m pd [34m# data processing, CSV file I/O (e.g. pd.read_csv)[0m[52;194H1,1[11CTop[1;1H[34h[?25h[?25l[52;194H2[2;1H[34h[?25h[?25l[52;194H3[3;1H[34h[?25h[?25l[52;194H4[4;1H[34h[?25h[?25l[52;194H5[5;1H[34h[?25h[?25l[52;194H6[6;1H[34h[?25h[?25l[52;194H7[7;1H[34h[?25h[?25l[52;194H8[8;1H[34h[?25h[?25l[52;194H9,0-1[9;1H[34h[?25h[?25l[52;194H10,1 [10;1H[34h[?25h[?25l[52;195H1[11;1H[34h[?25h[?25l[52;195H2[12;1H[34h[?25h[?25l[52;195H3[13;1H[34h[?25h[?25l[52;195H4[14;1H[34h[?25h[?25l[52;195H5[15;1H[34h[?25h[?25l[52;195H6[16;1H[34h[?25h[?25l[52;195H7[17;1H[34h[?25h[?25l[52;195H8[18;1H[34h[?25h[?25l[52;195H9,0-1[19;1H[34h[?25h[?25l[52;194H20,1  [20;1H[34h[?25h[?25l[52;195H1[21;1H[34h[?25h[?25l[52;195H2[22;1H[34h[?25h[?25l[52;195H3[23;1H[34h[?25h[?25l[52;195H4[24;1H[34h[?25h[?25l[52;195H5[25;1H[34h[?25h[?25l[52;195H6[26;1H[34h[?25h[?25l[52;195H7[27;1H[34h[?25h[?25l[52;195H8[28;1H[34h[?25h[?25l[52;195H9[29;1H[34h[?25h[?25l[52;194H30[30;1H[34h[?25h[?25l[52;195H1[31;1H[34h[?25h[?25l[52;195H2[32;1H[34h[?25h[?25l[52;195H3[33;1H[34h[?25h[?25l[52;195H4[34;1H[34h[?25h[?25l[52;195H5[35;1H[34h[?25h[?25l[52;195H6[36;1H[34h[?25h[?25l[52;195H7[37;1H[34h[?25h[?25l[52;195H8[38;1H[34h[?25h[?25l[52;195H9[39;1H[34h[?25h[?25l[52;194H40[40;1H[34h[?25h[?25l[52;195H1[41;1H[34h[?25h[?25l[52;195H2[42;1H[34h[?25h[?25l[52;195H3[43;1H[34h[?25h[?25l[52;195H4[44;1H[34h[?25h[?25l[52;195H5[45;1H[34h[?25h[?25l[52;195H6[46;1H[34h[?25h[?25l[52;195H7[47;1H[34h[?25h[?25l[52;195H8[48;1H[34h[?25h[?25l[52;195H9[49;1H[34h[?25h[?25l[52;194H50[50;1H[34h[?25h[?25l[52;195H1[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m sklearn.metrics [35mimport[0m make_scorer[52;1H[K[52;194H52,1[11C0%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mimport[0m tensorflow [33mas[0m tf[52;194H[K[52;194H53,1[11C0%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.python.keras [35mimport[0m backend [33mas[0m K[52;194H[K[52;194H54,1[11C0%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.layers [35mimport[0m Dense, Dropout, Flatten [34m#.core[0m[52;194H[K[52;194H55,1[11C1%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.optimizers [35mimport[0m SGD[52;194H[K[52;194H56,1[11C1%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.layers [35mimport[0m Dropout[52;194H[K[52;194H57,1[11C1%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m sklearn.externals [35mimport[0m joblib[52;194H[K[52;194H58,1[11C2%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.models [35mimport[0m Sequential, Model[52;194H[K[52;194H59,1[11C2%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.layers [35mimport[0m Dense[52;194H[K[52;194H60,1[11C2%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.wrappers.scikit_learn [35mimport[0m KerasRegressor [34m# or Classifier[0m[52;194H[K[52;194H61,1[11C2%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m sklearn.model_selection [35mimport[0m KFold[52;194H[K[52;194H62,1[11C3%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.optimizers [35mimport[0m SGD[52;194H[K[52;194H63,1[11C3%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.layers [35mimport[0m Dropout[52;194H[K[52;194H64,1[11C3%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mimport[0m random[52;194H[K[52;194H65,1[11C4%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb[0m[52;194H[K[52;194H66,1[11C4%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorboard.plugins.hparams [35mimport[0m api [33mas[0m hp[52;194H[K[52;194H67,1[11C4%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mimport[0m random[52;194H[K[52;194H68,1[11C5%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.python.keras.layers [35mimport[0m deserialize, serialize[52;194H[K[52;194H69,1[11C5%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.python.keras.saving [35mimport[0m saving_utils[52;194H[K[52;194H70,1[11C5%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H71,0-1[9C5%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H72,0-1[9C6%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H73,0-1[9C6%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#if snps == 'shuf' :[0m[52;194H[K[52;194H74,1[11C6%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#       print("Shuf nestedCV in usage")[0m[52;194H[K[52;194H75,1[11C7%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#       from nested_cv_shuf import NestedCV[0m[52;194H[K[52;194H76,1[11C7%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#elif snps == 'top':[0m[52;194H[K[52;194H77,1[11C7%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#       from nested_cv import NestedCV[0m[52;194H[K[52;194H78,1[11C7%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#else:[0m[52;194H[K[52;194H79,1[11C8%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#       print("snnps must be top or shuf")[0m[52;194H[K[52;194H80,1[11C8%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H81,1-8[9C8%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H82,0-1[9C9%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hsys.path.insert([31m1[0m, [31m'/external_storage/ciaran/Library/Python/3.7/python/site-packages/'[0m)[52;194H[K[52;194H83,1[11C9%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mimport[0m dill [33mas[0m pickle[52;194H[K[52;194H84,1[11C9%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mfor[0m i [33min[0m [36mrange[0m([31m1[0m,[36mlen[0m(sys.argv)):[52;194H[K[52;194H85,1[10C10%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m(sys.argv[i])[52;194H[K[52;194H86,1-8[8C10%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H87,0-1[8C10%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mif[0m [33mnot[0m os.path.exists([31m'/external_storage/ciaran/arabadopsis/'[0m + phenotype+ [31m'/'[0m + snps):[52;194H[K[52;194H88,1[10C10%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5Hos.makedirs([31m'/external_storage/ciaran/arabadopsis/'[0m + phenotype+ [31m'/'[0m + snps)[52;194H[K[52;194H89,1[10C11%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H90,0-1[8C11%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hos.chdir([31m'/external_storage/ciaran/arabadopsis/'[0m + phenotype+ [31m'/'[0m + snps)[52;194H[K[52;194H91,1[10C11%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hdate_object = datetime.datetime.now().replace(second=[31m0[0m,microsecond=[31m0[0m)[52;194H[K[52;194H92,1[10C12%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[36mprint[0m(date_object)[52;194H[K[52;194H93,1[10C12%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H94,0-1[8C12%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mcoeff_determination[0m(y_true, y_pred):[52;194H[K[52;194H95,1[10C12%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9HSS_res = K.sum(K.square( y_true-y_pred ))[52;194H[K[52;194H96,1[10C13%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9HSS_tot = K.sum(K.square( y_true - K.mean(y_true)))[52;194H[K[52;194H97,1[10C13%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mreturn[0m ([31m1[0m-SS_res/SS_tot)[52;194H[K[52;194H98,1[10C13%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H99,0-1[8C14%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H100,0-1[7C14%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mload_data[0m(data):[52;194H[K[52;194H101,1[9C14%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hdataset = np.loadtxt(data, skiprows=[31m1[0m)[52;194H[K[52;194H102,1[9C15%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hx = dataset[: , [31m6[0m:set_size+[31m6[0m]/[31m2[0m[52;194H[K[52;194H103,1[9C15%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hy = dataset[: , [31m5[0m ][52;194H[K[52;194H104,1[9C15%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hy = y.reshape(-[31m1[0m,[31m1[0m)[52;194H[K[52;194H105,1[9C15%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[34m#print("Performing split of raw data....")[0m[52;194H[K[52;194H106,1[9C16%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[34m#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.8, random_state=42)[0m[52;194H[K[52;194H107,1[9C16%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mreturn[0m x, y [34m#x_train, y_train, x_test, y_test[0m[52;194H[K[52;194H108,1[9C16%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H109,0-1[7C17%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H110,0-1[7C17%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H111,0-1[7C17%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mwith[0m [36mopen[0m(([31m'validation_results_'[0m+ [36mstr[0m(snps) +[36mstr[0m(num) + phenotype + [36mstr[0m([31m"{:%Y_%m_%d}"[0m.format(datetime.datetime.now())) + [31m'.vallog'[0m ), [31m'a'[0m) [33mas[0m f:[52;194H[K[52;194H112,1[9C17%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Horiginal_stdout = sys.stdout [34m# Save a reference to the original standard output[0m[52;194H[K[52;194H113,1[9C18%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hsys.stdout = f [34m# Change the standard output to the file we created.[0m[52;194H[K[52;194H114,1[9C18%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m(datetime.datetime.now())[52;194H[K[52;194H115,1[9C18%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hsys.stdout = original_stdout[52;194H[K[52;194H116,1[9C19%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H117,0-1[7C19%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mbaseline[0m(x, y):[52;194H[K[52;194H118,1[9C19%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hmodel = LinearRegression()[52;194H[K[52;194H119,1[9C20%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hmodel.fit(x, y)[52;194H[K[52;194H120,1[9C20%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mreturn[0m model[52;194H[K[52;194H121,1[9C20%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H122,0-1[7C20%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H123,0-1[7C21%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mavg_cv_result[0m(measure,cv_result):[52;194H[K[52;194H124,1[9C21%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hmy_var_name = [k [33mfor[0m k,v [33min[0m [36mlocals[0m().items() [33mif[0m v == measure][[31m0[0m] [34m#just to print out the name[0m[52;194H[K[52;194H125,1-8[7C21%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m(my_var_name)[52;194H[K[52;194H126,1-8[7C22%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hn_combos = [36mlen[0m(cv_result.cv_results_[[31m'split0_test_neg_mean_absolute_error'[0m])[52;194H[K[52;194H127,1-8[7C22%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hnamed_dict = {} [34m#dictonary will have a list of results PER grid combination across all CV results and this will return the average result. [0m[52;194H[K[52;194H128,1-8[7C22%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Havg_list = [][52;194H[K[52;194H129,1-8[7C22%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mfor[0m combo [33min[0m [36mrange[0m([31m0[0m, n_combos):[52;194H[K[52;194H130,1-8[7C23%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hnamed_dict[[36mstr[0m(combo)] = [][52;194H[K[52;194H131,1-8[7C23%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17H[33mfor[0m split [33min[0m measure:[52;194H[K[52;194H132,1-8[7C23%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;25Hnamed_dict[[36mstr[0m(combo)].append(cv_result.cv_results_[split][combo])[52;194H[K[52;194H133,1-8[7C24%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Havg_list.append(statistics.mean(named_dict[[36mstr[0m(combo)]))[52;194H[K[52;194H134,1-8[7C24%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17H[36mprint[0m(combo, statistics.mean(named_dict[[36mstr[0m(combo)]))[52;194H[K[52;194H135,1-8[7C24%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H136,0-1[7C25%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m'Max'[0m, np.nanmax(avg_list), np.where(avg_list == np.nanmax(avg_list)))[52;194H[K[52;194H137,1-8[7C25%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m'Min'[0m, np.nanmin(avg_list), np.where(avg_list == np.nanmin(avg_list)))[52;194H[K[52;194H138,1-8[7C25%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mreturn[0m avg_list[52;194H[K[52;194H139,1-8[7C25%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H140,0-1[7C26%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H141,0-1[7C26%[51;1H[34h[?25h[?25l[1;51r[1;1H[2M[1;52r[50;1H[33mdef[0m [36munpack[0m(model, training_config, weights): [34m##https://github.com/tensorflow/tensorflow/issues/34697 #fixes an error that the early stopping callback throws up in the nested cv #something about the parralele fitt[51;1Hting step needing everything to be pickle-able and the callback isnt [0m[52;194H[K[52;194H142,1[9C26%[50;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5Hrestored_model = deserialize(model)[52;194H[K[52;194H143,1[9C27%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5H[33mif[0m training_config [33mis[0m [33mnot[0m [36mNone[0m:[52;194H[K[52;194H144,1[9C27%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hrestored_model.compile([52;194H[K[52;194H145,1[9C27%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;13H**saving_utils.compile_args_from_training_config([52;194H[K[52;194H146,1[9C28%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Htraining_config[52;194H[K[52;194H147,1[9C28%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;13H)[52;194H[K[52;194H148,1[9C28%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H)[52;194H[K[52;194H149,1[9C29%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5Hrestored_model.set_weights(weights)[52;194H[K[52;194H150,1[9C29%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5H[33mreturn[0m restored_model[52;194H[K[52;194H151,1[9C29%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H152,0-1[7C29%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m# Hotfix function[0m[52;194H[K[52;194H153,1[9C30%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mmake_keras_picklable[0m():[52;194H[K[52;194H154,1[9C30%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H155,0-1[7C30%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5H[33mdef[0m [36m__reduce__[0m(self):[52;194H[K[52;194H156,1[9C31%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hmodel_metadata = saving_utils.model_metadata(self)[52;194H[K[52;194H157,1[9C31%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Htraining_config = model_metadata.get([31m"training_config"[0m, [36mNone[0m)[52;194H[K[52;194H158,1[9C31%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hmodel = serialize(self)[52;194H[K[52;194H159,1[9C31%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hweights = self.get_weights()[52;194H[K[52;194H160,1[9C32%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mreturn[0m (unpack, (model, training_config, weights))[52;194H[K[52;194H161,1[9C32%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H162,0-1[7C32%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5Hcls = Model[52;194H[K[52;194H163,1[9C33%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5Hcls.__reduce__ = __reduce__[52;194H[K[52;194H164,1[9C33%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H165,0-1[7C33%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H166,0-1[7C34%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mmake_param_box_plot[0m(goal_dict, time_dict):[52;194H[K[52;194H167,1[9C34%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mfor[0m param [33min[0m goal_dict:[52;194H[K[52;194H168,1-8[7C34%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.subplots([31m1[0m,[31m2[0m,figsize=([31m12[0m,[31m8[0m))[52;194H[K[52;194H169,1[9C34%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.subplot([31m121[0m)[52;194H[K[52;194H170,1[9C35%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.boxplot(goal_dict[param].values(), bootstrap=[36mNone[0m,showmeans=[36mFalse[0m, meanline=[36mFalse[0m, notch=[36mTrue[0m,labels=goal_dict[param].keys()) [34m#orange line is median, green dotted line is mean[0m[52;194H[K[52;194H171,1[9C35%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.xlabel([36mstr[0m(param).upper(), fontsize=[31m10[0m, fontweight=[31m'bold'[0m)[52;194H[K[52;194H172,1[9C35%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.ylabel([31m'R^2'[0m, fontsize=[31m10[0m,fontweight=[31m'bold'[0m)[52;194H[K[52;194H173,1[9C36%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.title([31m'R^2 Score vs %s'[0m % param, fontsize=[31m14[0m, fontweight=[31m'bold'[0m)[52;194H[K[52;194H174,1[9C36%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17H[33mif[0m param == [31m'initialization'[0m:[52;194H[K[52;194H175,1[9C36%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;25Hplt.xticks(fontsize=[31m6[0m)[52;194H[K[52;194H176,1[9C36%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.subplot([31m122[0m)[52;194H[K[52;194H177,1[9C37%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.boxplot(time_dict[param].values(), bootstrap=[36mNone[0m,showmeans=[36mFalse[0m, meanline=[36mFalse[0m, notch=[36mFalse[0m,labels=time_dict[param].keys())[52;194H[K[52;194H178,1[9C37%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.xlabel([36mstr[0m(param).upper(), fontsize=[31m10[0m, fontweight=[31m'bold'[0m)[52;194H[K[52;194H179,1[9C37%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.ylabel([31m'Training Time'[0m, fontsize=[31m10[0m,fontweight=[31m'bold'[0m)[52;194H[K[52;194H180,1[9C38%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.title([31m'Training Time vs %s'[0m % param, fontsize=[31m14[0m, fontweight=[31m'bold'[0m)[52;194H[K[52;194H181,1[9C38%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.tight_layout(pad=[31m4[0m)[52;194H[K[52;194H182,1[9C38%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17H[33mif[0m param == [31m'initialization'[0m:[52;194H[K[52;194H183,1[9C39%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;25Hplt.xticks(fontsize=[31m6[0m)[52;194H[K[52;194H184,1[9C39%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hmy_fig_name = [31m"plots_of_"[0m + [36mstr[0m(param) + [31m'_'[0m + [36mstr[0m([31m"{:%Y_%m_%d}"[0m.format(datetime.datetime.now())) + [31m'_'[0m +[36mstr[0m(snps) +[36mstr[0m(num)+ [31m".png"[0m[52;194H[K[52;194H185,1[9C39%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.savefig(my_fig_name, dpi=[31m300[0m)[52;194H[K[52;194H186,1[9C39%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.show()[52;194H[K[52;194H187,1[9C40%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.clf()[52;194H[K[52;194H188,1[9C40%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.close()[52;194H[K[52;194H189,1[9C40%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H190,1-8[7C41%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mmake_goal_dict[0m(whole_dict):[52;194H[K[52;194H191,1[9C41%[51;1H[34h[?25h[?25l[1;51r[1;1H[2M[1;52r[50;9H[36mprint[0m(whole_dict)[51;9Hgoal_dict = {key:{} [33mfor[0m key [33min[0m whole_dict}[52;194H[K[52;194H192,1-8[7C41%[50;8H[34h[?25h[?25l[52;196H3[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mfor[0m key [33min[0m whole_dict:[52;194H[K[52;194H194,1-8[7C42%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17H[33mfor[0m item [33min[0m whole_dict[key]:[52;194H[K[52;194H195,1-8[7C42%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;25Hgoal_dict[key][item] = [][52;194H[K[52;194H196,1-8[7C42%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Htime_dict = {key:{} [33mfor[0m key [33min[0m whole_dict} [34m#both empty[0m[52;194H[K[52;194H197,1-8[7C42%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mfor[0m key [33min[0m whole_dict:[52;194H[K[52;194H198,1-8[7C43%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17H[33mfor[0m item [33min[0m whole_dict[key]:[52;194H[K[52;194H199,1[9C43%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;25Htime_dict[key][item] = [][52;194H[K[52;194H200,1[9C43%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mreturn[0m goal_dict, time_dict[52;194H[K[52;194H201,1-8[7C44%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H202,0-1[7C44%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hx_train, y_train = load_data(data)[52;194H[K[52;194H203,1[9C44%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hname_list = np.loadtxt(data, skiprows=[31m1[0m, usecols=([31m0[0m,), dtype=[31m'str'[0m)[52;194H[K[52;194H204,1[9C45%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H205,0-1[7C45%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hscaler = preprocessing.StandardScaler().fit(y_train)[52;194H[K[52;194H206,1[9C45%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#pickle.dump(scaler, open('scaler.pkl', 'wb'))[0m[52;194H[K[52;194H207,1[9C45%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#scaler = pickle.load(open('scaler.pkl', 'rb'))[0m[52;194H[K[52;194H208,1[9C46%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H209,0-1[7C46%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hy_train = scaler.transform(y_train)[52;194H[K[52;194H210,1[9C46%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H211,0-1[7C47%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hn_snps = x_train.shape[[31m1[0m][52;194H[K[52;194H212,1[9C47%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hmy_cv = sklearn.model_selection.KFold(n_splits=[31m10[0m, shuffle=[36mTrue[0m, random_state=[31m42[0m)[52;194H[K[52;194H213,1[9C47%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#################################################SVM####SVM#####SVM####################################################################[0m[52;194H[K[52;194H214,1[9C47%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mncv_results[0m(analysis, ncv_object):[52;194H[K[52;194H215,1[9C48%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Best Params of %s is %s "[0m % (analysis, ncv_object.best_params))[52;194H[K[52;194H216,1[9C48%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Outer scores of %s is %s and mean is %s"[0m % (analysis, ncv_object.outer_scores, np.mean(ncv_object.outer_scores)))[52;194H[K[52;194H217,1[9C48%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Variance of %s is %s "[0m % (analysis, ncv_object.variance))[52;194H[K[52;194H218,1[9C49%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Goal dict of %s is %s "[0m % (analysis, ncv_object.goal_dict))[52;194H[K[52;194H219,1[9C49%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hmake_param_box_plot(ncv_object.goal_dict, ncv_object.time_dict)[52;194H[K[52;194H220,1[9C49%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mwith[0m [36mopen[0m([31m'NCV_'[0m + [36mstr[0m(analysis) + [31m'_'[0m +  [36mstr[0m(snps) + [31m'_'[0m + [36mstr[0m(phenotype) + [31m'_'[0m + [36mstr[0m(num) + [31m'.pkl'[0m, [31m'wb'[0m) [33mas[0m ncvfile: [34m#with open("fname.pkl", 'rb') as ncvfile:[0m[52;194H[K[52;194H221,1[9C50%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hpickle.dump(ncv_object, ncvfile) [34m#ncv_object = pickle.load(ncvfile)[0m[52;194H[K[52;194H222,1[9C50%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H223,0-1[7C50%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mnn_results[0m(analysis, ncv_object):[52;194H[K[52;194H224,1[9C50%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Best Params of %s is %s "[0m % (analysis, ncv_object.best_params))[52;194H[K[52;194H225,1[9C51%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Outer scores of %s is %s and mean is %s"[0m % (analysis, ncv_object.outer_scores, np.mean(ncv_object.outer_scores)))[52;194H[K[52;194H226,1[9C51%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Variance of %s is %s "[0m % (analysis, ncv_object.variance))[52;194H[K[52;194H227,1[9C51%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hnn_list = [ncv_object.best_inner_params_list, ncv_object.best_inner_score_list, ncv_object.best_params, ncv_object.metric, ncv_object.outer_scores, ncv_object.variance][52;194H[K[52;194H228,1[9C52%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mwith[0m [36mopen[0m([31m'NCV_'[0m + [36mstr[0m(analysis) + [31m'_'[0m +  [36mstr[0m(snps) + [31m'_'[0m + [36mstr[0m(phenotype) + [31m'_'[0m + [36mstr[0m(num) + [31m'.pkl'[0m, [31m'wb'[0m) [33mas[0m ncvfile:[52;194H[K[52;194H229,1[9C52%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hpickle.dump(nn_list, ncvfile) [34m#ncv_object = pickle.load(ncvfile)[0m[52;194H[K[52;194H230,1[9C52%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hncv_object.model.model.save([31m"model_"[0m + [36mstr[0m(analysis) + [31m'_'[0m +  [36mstr[0m(snps) + [31m'_'[0m + [36mstr[0m(phenotype) + [31m'_'[0m + [36mstr[0m(num) + [31m".h5"[0m)[52;194H[K[52;194H231,1[9C52%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H232,0-1[7C53%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H233,0-1[7C53%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[36mprint[0m([31m"Performing SVM"[0m)[52;194H[K[52;194H234,1[9C53%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hc_param = [[31m1[0m,[31m2[0m][52;194H[K[52;194H235,1[9C54%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hgamma_param = [[36mfloat[0m(x) [33mfor[0m x [33min[0m np.linspace([31m0.1[0m, [31m1[0m, [31m4[0m)][52;194H[K[52;194H236,1[9C54%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H237,0-1[7C54%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H238,0-1[7C55%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hepsilon_param = [[36mfloat[0m(x) [33mfor[0m x [33min[0m np.linspace([31m0.1[0m, [31m1[0m, [31m4[0m)][52;194H[K[52;194H239,1[9C55%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hloss_param = [[31m'epsilon_insensitive'[0m, [31m'squared_epsilon_insensitive'[0m][52;194H[K[52;194H240,1[9C55%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hkernel_param = [[31m'poly'[0m][52;194H[K[52;194H241,1[9C55%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hdegree = [[31m1[0m,[31m2[0m,[31m3[0m][52;194H[K[52;194H242,1[9C56%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hsvm_random_grid = {[31m'gamma'[0m:gamma_param, [31m'C'[0m:c_param,[31m'kernel'[0m:kernel_param, [31m"degree"[0m:degree}[52;194H[K[52;194H243,1[9C56%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[36mprint[0m(svm_random_grid)[52;194H[K[52;194H244,1[9C56%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hsvm_random_grid2 = {[31m'C'[0m : c_param, [31m'loss'[0m:loss_param}[52;194H[K[52;194H245,1[9C57%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[36mprint[0m(svm_random_grid2)[52;194H[K[52;194H246,1[9C57%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hrbg_goal_dict, rbg_time_dict = make_goal_dict(svm_random_grid)[52;194H[K[52;194H247,1[9C57%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hsvm_goal_dict, svm_time_dict = make_goal_dict(svm_random_grid2)[52;194H[K[52;194H248,1[9C57%[51;1H[34h[?25h[?25l[1;51r[1;1H[2M[1;52r[50;1HSVM_NCV = NestedCV(model_name=[31m'LinearSVR'[0m, name_list = name_list, model=LinearSVR(), goal_dict=svm_goal_dict, time_dict=svm_time_dict, params_grid=svm_random_grid2, outer_kfolds=[31m2[0m, inner_kfolds=[31m2[0m, n_jobs = [31m8[0m,cv__[51;1Hoptions={[31m'randomized_search'[0m:[36mTrue[0m, [31m'randomized_search_iter'[0m:[31m10[0m, [31m'sqrt_of_score'[0m:[36mFalse[0m,[31m'recursive_feature_elimination'[0m:[36mFalse[0m, [31m'metric'[0m:sklearn.metrics.r2_score, [31m'metric_score_indicator_lower'[0m:[36mFalse[0m})[52;194H[K[52;194H249,1[9C58%[50;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1HSVM_NCV.fit(x_train, y_train.ravel(), name_list=name_list, phenfile=phenfile, set_size=set_size, snps=snps, model_name=[31m'SVM'[0m, goal_dict=svm_goal_dict, time_dict=svm_time_dict)[52;194H[K[52;194H250,1[9C58%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#make_param_box_plot(goal_dict, time_dict)[0m[52;194H[K[52;194H251,1[9C58%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H252,0-1[7C59%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hncv_results([31m'SVM'[0m, SVM_NCV)[52;194H[K[52;194H253,1[9C59%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31m'''[0m[52;194H[K[52;194H254,1[9C59%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mprint("Performing RBG")[0m[52;194H[K[52;194H255,1[9C60%[51;1H[34h[?25h[?25l[1;51r[1;1H[2M[1;52r[50;1H[31mRBG_NCV = NestedCV(model_name='RBG', name_list=name_list, model=SVR(), params_grid=svm_random_grid, outer_kfolds=4, inner_kfolds=4, n_jobs = 8,cv_options={'randomized_search':True, 'randomized_search_iter':50, ''[51;1Hsqrt_of_score':False,'recursive_feature_elimination':False, 'metric':sklearn.metrics.r2_score, 'metric_score_indicator_lower':False})[0m[52;194H[K[52;194H256,1[9C60%[50;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mRBG_NCV.fit(x_train, y_train.ravel(), name_list=name_list, phenfile=phenfile, set_size=set_size, snps=snps, model_name='RBG')[0m[52;194H[K[52;194H257,1[9C60%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mncv_results('RBG', RBG_NCV)[0m[52;194H[K[52;194H258,1[9C61%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H259,0-1[7C61%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mprint("Performing LASSO")[0m[52;194H[K[52;194H260,1[9C61%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31malpha = [0.0001, 0.001, 0.01, 0.1, 0.5, 1, 10, 100][0m[52;194H[K[52;194H261,1[9C61%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31malpha_dict = {'alpha':alpha}[0m[52;194H[K[52;194H262,1[9C62%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mprint(alpha_dict)[0m[52;194H[K[52;194H263,1[9C62%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31malpha_name_dict = {'alpha':"Alpha"}[0m[52;194H[K[52;194H264,1[9C62%[51;1H[34h[?25h[?25l[1;51r[1;1H[2M[1;52r[50;1H[31mLASS_NCV = NestedCV(model_name='LASS', name_list=name_list, model=Lasso(), params_grid=alpha_dict, outer_kfolds=4, inner_kfolds=4, n_jobs = 8,cv_options={'randomized_search':True, 'randomized_search_iter':50, 'ss[51;1Hqrt_of_score':False,'recursive_feature_elimination':False, 'metric':sklearn.metrics.r2_score, 'metric_score_indicator_lower':False})[0m[52;194H[K[52;194H265,1[9C63%[50;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mLASS_NCV.fit(x_train, y_train.ravel(), name_list=name_list, phenfile=phenfile, set_size=set_size, snps=snps, model_name='LASS')[0m[52;194H[K[52;194H266,1[9C63%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mncv_results('LASS', LASS_NCV)[0m[52;194H[K[52;194H267,1[9C63%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mprint("Performing Ridge")[0m[52;194H[K[52;194H268,1[9C64%[51;1H[34h[?25h[?25l[1;51r[1;1H[2M[1;52r[50;1H[31mRIDGE_NCV = NestedCV(model_name='RIDGE', name_list=name_list, model=Ridge(), params_grid=alpha_dict, outer_kfolds=4, inner_kfolds=4, n_jobs = 8,cv_options={'randomized_search':True, 'randomized_search_iter':50,  [51;1H'sqrt_of_score':False,'recursive_feature_elimination':False, 'metric':sklearn.metrics.r2_score, 'metric_score_indicator_lower':False})[0m[52;194H[K[52;194H269,1[9C64%[50;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mRIDGE_NCV.fit(x_train, y_train.ravel(), name_list=name_list, phenfile=phenfile, set_size=set_size, snps=snps, model_name='RIDGE')[0m[52;194H[K[52;194H270,1[9C64%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mncv_results('RIDGE', RIDGE_NCV)[0m[52;194H[K[52;194H271,1[9C65%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mprint("Performing Random Forests")[0m[52;194H[K[52;194H272,1[9C65%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mn_estimators = [int(x) for x in np.linspace(start = 2000, stop = 9000, num = 50)] # Number of features to consider at every split[0m[52;194H[K[52;194H273,1[9C65%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mmax_features = ['auto', 'sqrt', 'log2'] # Maximum number of levels in tree[0m[52;194H[K[52;194H274,1[9C65%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mmax_depth = [int(x) for x in np.linspace(1, 100, num = 20)][0m[52;194H[K[52;194H275,1[9C66%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mmax_depth.append(None) # Minimum number of samples required to split a node[0m[52;194H[K[52;194H276,1[9C66%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31m#min_samples_split = [int(x) for x in np.linspace(2, 2000, num = 100)]; min_samples_split.extend((5,10,20))[0m[52;194H[K[52;194H277,1[9C66%[51;1H[34h[?25h[?25l[52;196H6[50;1H[34h[?25h[?25l[52;196H5[49;1H[34h[?25h[?25l[52;196H4[48;1H[34h[?25h[?25l[52;196H3[47;1H[34h[?25h[?25l[52;196H2[46;1H[34h[?25h[?25l[52;196H1[45;1H[34h[?25h[?25l[52;196H0[44;1H[34h[?25h[?25l[52;195H69[42;1H[34h[?25h[?25l[52;196H8[41;1H[34h[?25h[?25l[52;196H7[40;1H[34h[?25h[?25l[52;196H6[39;1H[34h[?25h[?25l[52;196H5[37;1H[34h[?25h[?25l[52;196H4[36;1H[34h[?25h[?25l[52;196H3[35;1H[34h[?25h[?25l[52;196H2[34;1H[34h[?25h[?25l[52;196H1[33;1H[34h[?25h[?25l[52;196H0[32;1H[34h[?25h[?25l[52;195H59,0-1[31;1H[34h[?25h[?25l[52;196H8,1  [30;1H[34h[?25h[?25l[52;196H7[29;1H[34h[?25h[?25l[52;196H6[27;1H[34h[?25h[?25l[52;196H5[26;1H[34h[?25h[?25l[52;196H4[25;1H[34h[?25h[?25l[52;196H3[24;1H[34h[?25h[?25l[52;196H2,0-1[23;1H[34h[?25h[?25l[52;196H1,1  [22;1H[34h[?25h[?25l[52;196H0[21;1H[34h[?25h[?25l[52;195H49[19;1H[34h[?25h[?25l[52;198H2[19;2H[34h[?25h[?25l[52;198H3[19;3H[34h[?25h[?25l[52;198H4[19;4H[34h[?25h[?25l[52;198H5[19;5H[34h[?25h[?25l[52;198H6[19;6H[34h[?25h[?25l[52;198H7[19;7H[34h[?25h[?25l[52;198H8[19;8H[34h[?25h[?25l[52;198H9[19;9H[34h[?25h[?25l[52;198H10[19;10H[34h[?25h[?25l[52;199H1[19;11H[34h[?25h[?25l[52;199H2[19;12H[34h[?25h[?25l[52;199H3[19;13H[34h[?25h[?25l[52;199H4[19;14H[34h[?25h[?25l[52;199H5[19;15H[34h[?25h[?25l[52;199H6[19;16H[34h[?25h[?25l[52;199H7[19;17H[34h[?25h[?25l[52;199H8[19;18H[34h[?25h[?25lV[46m([0m[191C__[20;1Ho[196C[46m)[0m[52;199H9[19;19H[34h[?25h[?25l([191C__[20;1Ho[196C)[52;198H20[19;20H[34h[?25h[?25l[52;199H1[19;21H[34h[?25h[?25l[52;199H2[19;22H[34h[?25h[?25l[52;199H3[19;23H[34h[?25h[?25l[52;199H4[19;24H[34h[?25h[?25l[52;199H5[19;25H[34h[?25h[?25l[52;199H6[19;26H[34h[?25h[?25l[52;199H7[19;27H[34h[?25h[?25l[52;199H8[19;28H[34h[?25h[?25l[52;199H9[19;29H[34h[?25h[?25l[52;198H30[19;30H[34h[?25h[?25l[52;199H1[19;31H[34h[?25h[?25l[52;199H2[19;32H[34h[?25h[?25l[52;199H3[19;33H[34h[?25h[?25l[52;199H4[19;34H[34h[?25h[?25l[52;199H5[19;35H[34h[?25h[?25l[52;199H6[19;36H[34h[?25h[?25l[52;199H7[19;37H[34h[?25h[?25l[52;199H8[19;38H[34h[?25h[?25l[52;199H9[19;39H[34h[?25h[?25l[52;198H40[19;40H[34h[?25h[?25l[52;199H1[19;41H[34h[?25h[?25l[52;199H2[19;42H[34h[?25h[?25l[52;199H3[19;43H[34h[?25h[?25l[52;199H4[19;44H[34h[?25h[?25l[52;199H5[19;45H[34h[?25h[?25l[52;199H6[19;46H[34h[?25h[?25l[52;199H7[19;47H[34h[?25h[?25l[52;199H8[19;48H[34h[?25h[?25l[52;199H9[19;49H[34h[?25h[?25l[52;198H50[19;50H[34h[?25h[?25l[52;199H1[19;51H[34h[?25h[?25l[52;199H2[19;52H[34h[?25h[?25l[52;199H3[19;53H[34h[?25h[?25l[52;199H4[19;54H[34h[?25h[?25l[52;199H5[19;55H[34h[?25h[?25l[52;199H6[19;56H[34h[?25h[?25l[52;199H7[19;57H[34h[?25h[?25l[52;199H8[19;58H[34h[?25h[?25l[52;199H9[19;59H[34h[?25h[?25l[52;198H60[19;60H[34h[?25h[?25l[52;199H1[19;61H[34h[?25h[?25l[52;199H2[19;62H[34h[?25h[?25l[52;199H3[19;63H[34h[?25h[?25l[52;199H4[19;64H[34h[?25h[?25l[52;199H5[19;65H[34h[?25h[?25l[52;199H6[19;66H[34h[?25h[?25l[52;199H7[19;67H[34h[?25h[?25l[52;199H8[19;68H[34h[?25h[?25l[52;199H9[19;69H[34h[?25h[?25l[52;198H70[19;70H[34h[?25h[?25l[52;199H1[19;71H[34h[?25h[?25l[52;199H2[19;72H[34h[?25h[?25l[52;199H3[19;73H[34h[?25h[?25l[52;199H4[19;74H[34h[?25h[?25l[52;199H5[19;75H[34h[?25h[?25l[52;199H6[19;76H[34h[?25h[?25l[52;199H7[19;77H[34h[?25h[?25l[52;199H8[19;78H[34h[?25h[?25l[52;199H9[19;79H[34h[?25h[?25l[52;198H80[19;80H[34h[?25h[?25l[52;199H1[19;81H[34h[?25h[?25lR[46m()[0m[127C__[20;1Ho[52;199H2[19;82H[34h[?25h[?25l[129C__[20;1Ho[52;199H3[19;83H[34h[?25h[?25l()[127C__[20;1Ho[52;199H4[19;84H[34h[?25h[?25l[52;199H5[19;85H[34h[?25h[?25l[52;199H6[19;86H[34h[?25h[?25l[52;199H7[19;87H[34h[?25h[?25l[52;199H8[19;88H[34h[?25h[?25l[52;199H9[19;89H[34h[?25h[?25l[52;198H90[19;90H[34h[?25h[?25l[52;199H1[19;91H[34h[?25h[?25l[52;199H2[19;92H[34h[?25h[?25l[52;199H3[19;93H[34h[?25h[?25l[52;199H4[19;94H[34h[?25h[?25l[52;199H5[19;95H[34h[?25h[?25l[52;199H6[19;96H[34h[?25h[?25l[52;199H7[19;97H[34h[?25h[?25l[52;199H8[19;98H[34h[?25h[?25l[52;199H9[19;99H[34h[?25h[?25l[52;198H100[19;100H[34h[?25h[?25l[52;200H1[19;101H[34h[?25h[?25l[52;200H2[19;102H[34h[?25h[?25l[52;200H3[19;103H[34h[?25h[?25l[52;200H4[19;104H[34h[?25h[?25l[52;200H5[19;105H[34h[?25h[?25l[52;200H6[19;106H[34h[?25h[?25l[52;200H7[19;107H[34h[?25h[?25l[52;200H8[19;108H[34h[?25h[?25l[52;200H9[19;109H[34h[?25h[?25l[52;199H10[19;110H[34h[?25h[?25l[52;200H1[19;111H[34h[?25h[?25l[52;200H2[19;112H[34h[?25h[?25l[52;200H3[19;113H[34h[?25h[?25l[52;200H4[19;114H[34h[?25h[?25l[52;200H5[19;115H[34h[?25h[?25l[52;200H6[19;116H[34h[?25h[?25l[52;200H7[19;117H[34h[?25h[?25l[52;200H8[19;118H[34h[?25h[?25l[52;200H9[19;119H[34h[?25h[?25l[52;199H20[19;120H[34h[?25h[?25l[52;200H1[19;121H[34h[?25h[?25l[52;200H2[19;122H[34h[?25h[?25l[52;200H3[19;123H[34h[?25h[?25l[52;200H4[19;124H[34h[?25h[?25l[52;200H5[19;125H[34h[?25h[?25l[52;200H6[19;126H[34h[?25h[?25l[52;200H7[19;127H[34h[?25h[?25l[52;200H8[19;128H[34h[?25h[?25l[52;200H9[19;129H[34h[?25h[?25l[52;199H30[19;130H[34h[?25h[?25l[52;200H1[19;131H[34h[?25h[?25l[52;200H2[19;132H[34h[?25h[?25l[52;200H3[19;133H[34h[?25h[?25l[52;200H4[19;134H[34h[?25h[?25l[52;200H5[19;135H[34h[?25h[?25l[52;200H6[19;136H[34h[?25h[?25l[52;200H7[19;137H[34h[?25h[?25l[52;200H8[19;138H[34h[?25h[?25l[52;200H9[19;139H[34h[?25h[?25l[52;199H40[19;140H[34h[?25h[?25l[52;200H1[19;141H[34h[?25h[?25l[52;200H2[19;142H[34h[?25h[?25l[52;200H3[19;143H[34h[?25h[?25l[52;200H4[19;144H[34h[?25h[?25l[52;200H5[19;145H[34h[?25h[?25l[52;200H6[19;146H[34h[?25h[?25l[52;200H7[19;147H[34h[?25h[?25l[52;200H8[19;148H[34h[?25h[?25l[52;200H9[19;149H[34h[?25h[?25l[52;199H50[19;150H[34h[?25h[?25l[52;200H1[19;151H[34h[?25h[?25l[52;200H2[19;152H[34h[?25h[?25l[52;200H3[19;153H[34h[?25h[?25l[52;200H4[19;154H[34h[?25h[?25l[52;200H5[19;155H[34h[?25h[?25l[52;200H6[19;156H[34h[?25h[?25l[52;200H7[19;157H[34h[?25h[?25l[52;200H8[19;158H[34h[?25h[?25l[52;200H9[19;159H[34h[?25h[?25l[52;199H60[19;160H[34h[?25h[?25l[52;200H1[19;161H[34h[?25h[?25l[52;200H2[19;162H[34h[?25h[?25l[52;200H3[19;163H[34h[?25h[?25l[52;200H4[19;164H[34h[?25h[?25l[52;200H5[19;165H[34h[?25h[?25l[52;200H6[19;166H[34h[?25h[?25l[52;200H7[19;167H[34h[?25h[?25l[52;200H8[19;168H[34h[?25h[?25l[52;200H9[19;169H[34h[?25h[?25l[52;199H70[19;170H[34h[?25h[?25l[52;200H1[19;171H[34h[?25h[?25l[52;200H2[19;172H[34h[?25h[?25l[52;200H3[19;173H[34h[?25h[?25l[52;200H4[19;174H[34h[?25h[?25l[52;200H5[19;175H[34h[?25h[?25l[52;200H6[19;176H[34h[?25h[?25l[52;200H7[19;177H[34h[?25h[?25l[52;200H8[19;178H[34h[?25h[?25l[52;200H9[19;179H[34h[?25h[?25l[52;199H80[19;180H[34h[?25h[?25l[52;200H1[19;181H[34h[?25h[?25l[52;200H2[19;182H[34h[?25h[?25l[52;200H3[19;183H[34h[?25h[?25l[52;200H4[19;184H[34h[?25h[?25l[52;200H5[19;185H[34h[?25h[?25l[52;200H6[19;186H[34h[?25h[?25l[52;200H7[19;187H[34h[?25h[?25l[52;200H8[19;188H[34h[?25h[?25l[52;200H9[19;189H[34h[?25h[?25l[52;199H90[19;190H[34h[?25h[?25l[52;200H1[19;191H[34h[?25h[?25l[52;200H2[19;192H[34h[?25h[?25l[52;200H3[19;193H[34h[?25h[?25l[52;200H4[19;194H[34h[?25h[?25l[52;200H5[19;195H[34h[?25h[?25l[52;200H6[19;196H[34h[?25h[?25l[52;200H7[19;197H[34h[?25h[?25l[52;200H8[19;198H[34h[?25h[?25l[52;200H9[19;199H[34h[?25h[?25l[52;198H200[19;200H[34h[?25h[?25l[52;200H1[19;201H[34h[?25h[?25l[52;200H2[19;202H[34h[?25h[?25l[52;200H3[19;203H[34h[?25h[?25l[52;200H4[19;204H[34h[?25h[?25l[52;200H5[19;205H[34h[?25h[?25l[52;200H6[19;206H[34h[?25h[?25l[52;200H7[19;207H[34h[?25h[?25l[52;200H8[19;208H[34h[?25h[?25l[52;200H9[19;209H[34h[?25h[?25l[52;199H10[19;210H[34h[?25h[?25l[52;200H1[19;211H[34h[?25h[?25l[52;200H2[20;1H[34h[?25h[?25l[52;200H3[20;2H[34h[?25h[?25l[52;200H4[20;3H[34h[?25h[?25l[52;200H5[20;4H[34h[?25h[?25l[52;200H6[20;5H[34h[?25h[?25l[52;200H7[20;6H[34h[?25h[?25l[52;200H8[20;7H[34h[?25h[?25l[52;200H9[20;8H[34h[?25h[?25l=[46m{[187C}[0m[52;199H20[20;9H[34h[?25h[?25l{[187C}[52;200H1[20;10H[34h[?25h[?25l[52;200H2[20;11H[34h[?25h[?25l[52;200H3[20;12H[34h[?25h[?25l[52;200H4[20;13H[34h[?25h[?25l[52;200H5[20;14H[34h[?25h[?25l[52;200H6[20;15H[34h[?25h[?25l[52;200H7[20;16H[34h[?25h[?25l[52;200H8[20;17H[34h[?25h[?25l[52;200H9[20;18H[34h[?25h[?25l[52;199H30[20;19H[34h[?25h[?25l[52;200H1[20;20H[34h[?25h[?25l[52;200H2[20;21H[34h[?25h[?25l[52;200H3[20;22H[34h[?25h[?25l[52;200H4[20;23H[34h[?25h[?25l[52;200H5[20;24H[34h[?25h[?25l[52;200H6[20;25H[34h[?25h[?25l[52;200H7[20;26H[34h[?25h[?25l[52;200H8[20;27H[34h[?25h[?25l[52;200H9[20;28H[34h[?25h[?25l[52;199H40[20;29H[34h[?25h[?25l[52;200H1[20;30H[34h[?25h[?25l[52;200H2[20;31H[34h[?25h[?25l[52;200H3[20;32H[34h[?25h[?25l[52;200H4[20;33H[34h[?25h[?25l[52;200H5[20;34H[34h[?25h[?25l[52;200H6[20;35H[34h[?25h[?25l[52;200H7[20;36H[34h[?25h[?25l[52;200H8[20;37H[34h[?25h[?25l[52;200H9[20;38H[34h[?25h[?25l[52;199H50[20;39H[34h[?25h[?25l[52;200H1[20;40H[34h[?25h[?25l[52;200H2[20;41H[34h[?25h[?25l[52;200H3[20;42H[34h[?25h[?25l[52;200H4[20;43H[34h[?25h[?25l[52;200H5[20;44H[34h[?25h[?25l[52;200H6[20;45H[34h[?25h[?25l[52;200H7[20;46H[34h[?25h[?25l[52;200H8[20;47H[34h[?25h[?25l[52;200H9[20;48H[34h[?25h[?25l[52;199H60[20;49H[34h[?25h[?25l[52;200H1[20;50H[34h[?25h[?25l[52;200H2[20;51H[34h[?25h[?25l[52;200H3[20;52H[34h[?25h[?25l[52;200H4[20;53H[34h[?25h[?25l[52;200H5[20;54H[34h[?25h[?25l[52;200H6[20;55H[34h[?25h[?25l[52;200H7[20;56H[34h[?25h[?25l[52;200H8[20;57H[34h[?25h[?25l[52;200H9[20;58H[34h[?25h[?25l[52;199H70[20;59H[34h[?25h[?25l[52;200H1[20;60H[34h[?25h[?25l[52;200H2[20;61H[34h[?25h[?25l[52;200H3[20;62H[34h[?25h[?25l[52;1H[1m-- INSERT --[0m[52;194H[K[52;194H249,273[7C66%[20;62H[34h[?25h[?25l[31m0[0m, [31m'sqrt_of_score'[0m:[36mFalse[0m,[31m'recursive_feature_elimination'[0m:[36mFalse[0m, [31m'metric'[0m:sklearn.metrics.r2_score, [31m'metric_score_indicator_lower'[0m:[36mFalse[0m})[20;198H[K[52;200H2[20;61H[34h[?25h[?25l[31m50[0m, [31m'sqrt_of_score'[0m:[36mFalse[0m,[31m'recursive_feature_elimination'[0m:[36mFalse[0m, [31m'metric'[0m:sklearn.metrics.r2_score, [31m'metric_score_indicator_lower'[0m:[36mFalse[0m})[52;200H3[20;62H[34h[?25h[52;1H[K[20;61H[?25l[52;194H249,272[7C66%[20;61H[34h[?25h[?25l[52;194H[K[52;1H:[34h[?25hx[?25l[34h[?25h[?25l"cv_grid_all_ml.py" 391L, 23145C written
[?1l>[34h[?25h[?1049l(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ vi cv_grid_all_ml.pypython cv_grid_all_ml.py 31 /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del /home/ciaran/completed_big_matrix_binary_new_snps_ids__first_10k.raw shuf FT10cv_new_10k 10000
Please remember to set the right set size in the nested_cv code
/external_storage/ciaran/machine_learning2
WARNING THIS IS AN EDITED SCRIPT - Ciaran Kelly 2021 
 Edited with permission under liscence 
 Flex version
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
31
/home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del
/home/ciaran/completed_big_matrix_binary_new_snps_ids_first_10k.raw
shuf
FT10cv_new_10k
10000
2021-12-19 23:23:00
Performing SVM
{'gamma': [0.1, 0.4, 0.7, 1.0], 'C': [1, 2], 'degree': [1, 2, 3], 'kernel': ['poly']}
{'C': [1, 2], 'loss': ['epsilon_insensitive', 'squared_epsilon_insensitive']}
{'gamma': [0.1, 0.4, 0.7, 1.0], 'C': [1, 2], 'degree': [1, 2, 3], 'kernel': ['poly']}
{'C': [1, 2], 'loss': ['epsilon_insensitive', 'squared_epsilon_insensitive']}
ADBUCE
INNER COUNT NO.  1
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_1_in_1_in.raw
Set size set to 10000
Set size set to 10000
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 4 is smaller than n_iter=50. Running 4 iterations. For exhaustive searches, use GridSearchCV.
  % (grid_size, self.n_iter, grid_size), UserWarning)
{'C': 1, 'loss': 'epsilon_insensitive'}
Blue
Red
0.18788891387594797
Typer LinearSVR
{'C': 1, 'loss': 'squared_epsilon_insensitive'}
Blue
Red
0.19045801795869732
Typer LinearSVR
{'C': 2, 'loss': 'epsilon_insensitive'}
Blue
Red
0.18789555344345532
Typer LinearSVR
{'C': 2, 'loss': 'squared_epsilon_insensitive'}
Blue
Red
0.1891854773194156
Typer LinearSVR
INNER COUNT NO.  2
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_1_in_2_in.raw
Set size set to 10000
Set size set to 10000
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 4 is smaller than n_iter=50. Running 4 iterations. For exhaustive searches, use GridSearchCV.
  % (grid_size, self.n_iter, grid_size), UserWarning)
{'C': 1, 'loss': 'epsilon_insensitive'}
Blue
Red
0.4000731350577079
Typer LinearSVR
{'C': 1, 'loss': 'squared_epsilon_insensitive'}
Blue
Red
0.40223841110872827
Typer LinearSVR
{'C': 2, 'loss': 'epsilon_insensitive'}
Blue
Red
0.4000682223246623
Typer LinearSVR
{'C': 2, 'loss': 'squared_epsilon_insensitive'}
Blue
Red
0.4011735035418117
Typer LinearSVR
OUTER COUNT NO.  1
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_1_in_2_out.raw
Set size set to 10000
Set size set to 10000
0.39628037361569746
INNER COUNT NO.  1
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_2_in_1_in.raw
Set size set to 10000
Set size set to 10000
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 4 is smaller than n_iter=50. Running 4 iterations. For exhaustive searches, use GridSearchCV.
  % (grid_size, self.n_iter, grid_size), UserWarning)
{'C': 1, 'loss': 'epsilon_insensitive'}
Blue
Red
0.40100472027018663
Typer LinearSVR
{'C': 1, 'loss': 'squared_epsilon_insensitive'}
Blue
Red
0.40278508366621346
Typer LinearSVR
{'C': 2, 'loss': 'epsilon_insensitive'}
Blue
Red
0.4009987152782948
Typer LinearSVR
{'C': 2, 'loss': 'squared_epsilon_insensitive'}
Blue
Red
0.40190705143175753
Typer LinearSVR
INNER COUNT NO.  2
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_2_in_2_in.raw
Set size set to 10000
Set size set to 10000
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 4 is smaller than n_iter=50. Running 4 iterations. For exhaustive searches, use GridSearchCV.
  % (grid_size, self.n_iter, grid_size), UserWarning)
{'C': 1, 'loss': 'epsilon_insensitive'}
Blue
Red
0.42083734976475784
Typer LinearSVR
{'C': 1, 'loss': 'squared_epsilon_insensitive'}
Blue
Red
0.42290827460648994
Typer LinearSVR
{'C': 2, 'loss': 'epsilon_insensitive'}
Blue
Red
0.42084849099240684
Typer LinearSVR
{'C': 2, 'loss': 'squared_epsilon_insensitive'}
Blue
Red
0.42190306917947873
Typer LinearSVR
OUTER COUNT NO.  2
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_2_in_2_out.raw
Set size set to 10000
Set size set to 10000
0.34398264040299265
Best Params of SVM is {'C': [1], 'loss': ['squared_epsilon_insensitive']} 
Outer scores of SVM is [0.39628037361569746, 0.34398264040299265] and mean is 0.37013150700934505
Variance of SVM is [0.5401563834310771, 0.41365577084058264] 
Goal dict of SVM is {'C': {1: [0.18788891387594797, 0.19045801795869732, 0.4000731350577079, 0.40223841110872827, 0.40100472027018663, 0.40278508366621346, 0.42083734976475784, 0.42290827460648994], 2: [0.18789555344345532, 0.1891854773194156, 0.4000682223246623, 0.4011735035418117, 0.4009987152782948, 0.40190705143175753, 0.42084849099240684, 0.42190306917947873]}, 'loss': {'squared_epsilon_insensitive': [0.19045801795869732, 0.1891854773194156, 0.40223841110872827, 0.4011735035418117, 0.40278508366621346, 0.40190705143175753, 0.42290827460648994, 0.42190306917947873], 'epsilon_insensitive': [0.18788891387594797, 0.18789555344345532, 0.4000731350577079, 0.4000682223246623, 0.40100472027018663, 0.4009987152782948, 0.42083734976475784, 0.42084849099240684]}} 
Performing Neural Network
Traceback (most recent call last):
  File "cv_grid_all_ml.py", line 343, in <module>
    NN_NCV = NestedCV(model_name='nn_model', name_list = name_list, model=nn_model, params_grid=param_grid, outer_kfolds=4, inner_kfolds=4, n_jobs = 32,cv_options={'randomized_search':True, 'randomized_search_iter':100, 'sqrt_of_score':False,'recursive_feature_elimination':False, 'metric':sklearn.metrics.r2_score, 'metric_score_indicator_lower':False})
TypeError: __init__() missing 2 required positional arguments: 'goal_dict' and 'time_dict'
(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ git add . 
(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ c[Kgit commit -m "boxplotting seems to be working"
[main b109816] boxplotting seems to be working
 3 files changed, 2450 insertions(+), 5 deletions(-)
 create mode 100644 test_boxplotting.txt
(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ git commit -m "boxplotting seems to be working"add . [Kpython cv_grid_all_ml.py 31 /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del /home/ciaran/completed_big_matrix_binary_new_snps_ids__first_10k.raw shuf FT10cv_new_10k 10000M[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cvi cv_grid_all_ml.py[K
[KM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C
[?1049h[?1h=[1;52r[34l[34h[?25h[23m[24m[0m[H[J[?25l[52;1H"cv_grid_all_ml.py" 391L, 23145C[1;1H[34m#Warning : best model selected by NMAE and R2 might not be the same
#performs linear regression, linear regression, neural network, svm and random forest, LASSO, RIDGE, CNN
#source ~/venv/bin/activate #in python 3.5.2
#print a log to a .txt file!
#model = pickle.load(open('FILEPATH', 'rb')) 
#dependencies = {'coeff_determination':coeff_determination}
#model = tf.keras.models.load_model('FILEPATH', custom_objects=dependencies)
#

#import tensorflow
#import numpy as np; import scipy #need to do this before path insert
#import sys
#sys.path.insert(1, '/external_storage/ciaran/Library/Python/3.7/python/site-packages/')
#import dill as pickle
#sys.path.insert(1, '/external_storage/ciaran/Library/Python/3.7/python/site-packages/nested_cv')
#from nested_cv import NestedCV
#with open('NCV_NN.pkl', 'rb') as f:
#     red = pickle.load(f)[0m

[36mprint[0m([31m"Please remember to set the right set size in the nested_cv code"[0m)
[35mimport[0m sys
sys.path.insert([31m1[0m, [31m'/external_storage/ciaran/Library/Python/3.7/python/site-packages/nested_cv'[0m)
num = sys.argv[[31m1[0m] [34m#script number for saving out[0m
phenfile = [36mstr[0m(sys.argv[[31m2[0m]) [34m#txt file with phenotypes[0m
data = [36mstr[0m(sys.argv[[31m3[0m]) [34m#needs to be same size as set_size[0m
snps = [36mstr[0m(sys.argv[[31m4[0m]) [34m#top or shuf[0m
phenotype = [36mstr[0m(sys.argv[[31m5[0m]) [34m#make a directory for the results[0m
set_size = [36mint[0m(sys.argv[[31m6[0m]) [34m#how many SNPs[0m
[35mfrom[0m nested_cv [35mimport[0m NestedCV
[35mimport[0m statistics
[35mimport[0m numpy [33mas[0m np
[35mimport[0m sklearn
[35mimport[0m seaborn [33mas[0m sns
[35mfrom[0m sklearn [35mimport[0m preprocessing
[35mfrom[0m sklearn.pipeline [35mimport[0m make_pipeline, Pipeline
[35mfrom[0m sklearn.preprocessing [35mimport[0m StandardScaler
[35mimport[0m datetime
[35mimport[0m time
[35mfrom[0m statistics [35mimport[0m mean
[35mimport[0m matplotlib.pyplot [33mas[0m plt
[35mfrom[0m matplotlib [35mimport[0m style
[35mfrom[0m sklearn.svm [35mimport[0m LinearSVR
[35mfrom[0m sklearn.svm [35mimport[0m SVR
[35mfrom[0m sklearn.model_selection [35mimport[0m train_test_split
[35mfrom[0m sklearn.model_selection [35mimport[0m RandomizedSearchCV
[35mfrom[0m sklearn.linear_model [35mimport[0m LinearRegression, Lasso, Ridge
[34m#import pickle #use dill instead below[0m
[35mfrom[0m sklearn.ensemble [35mimport[0m RandomForestRegressor [34m#based in part on https://towardsdatascience.com/random-forest-in-python-24d0893d51c0[0m
[35mfrom[0m statistics [35mimport[0m mean
[35mimport[0m os
[35mimport[0m pandas [33mas[0m pd [34m# data processing, CSV file I/O (e.g. pd.read_csv)[0m[52;194H1,1[11CTop[1;1H[34h[?25h[?25l[52;194H2[2;1H[34h[?25h[?25l[52;194H3[3;1H[34h[?25h[?25l[52;194H4[4;1H[34h[?25h[?25l[52;194H5[5;1H[34h[?25h[?25l[52;194H6[6;1H[34h[?25h[?25l[52;194H7[7;1H[34h[?25h[?25l[52;194H8[8;1H[34h[?25h[?25l[52;194H9,0-1[9;1H[34h[?25h[?25l[52;194H10,1 [10;1H[34h[?25h[?25l[52;195H1[11;1H[34h[?25h[?25l[52;195H2[12;1H[34h[?25h[?25l[52;195H3[13;1H[34h[?25h[?25l[52;195H4[14;1H[34h[?25h[?25l[52;195H5[15;1H[34h[?25h[?25l[52;195H6[16;1H[34h[?25h[?25l[52;195H7[17;1H[34h[?25h[?25l[52;195H8[18;1H[34h[?25h[?25l[52;195H9,0-1[19;1H[34h[?25h[?25l[52;194H20,1  [20;1H[34h[?25h[?25l[52;195H1[21;1H[34h[?25h[?25l[52;195H2[22;1H[34h[?25h[?25l[52;195H3[23;1H[34h[?25h[?25l[52;195H4[24;1H[34h[?25h[?25l[52;195H5[25;1H[34h[?25h[?25l[52;195H6[26;1H[34h[?25h[?25l[52;195H7[27;1H[34h[?25h[?25l[52;195H8[28;1H[34h[?25h[?25l[52;195H9[29;1H[34h[?25h[?25l[52;194H30[30;1H[34h[?25h[?25l[52;195H1[31;1H[34h[?25h[?25l[52;195H2[32;1H[34h[?25h[?25l[52;195H3[33;1H[34h[?25h[?25l[52;195H4[34;1H[34h[?25h[?25l[52;195H5[35;1H[34h[?25h[?25l[52;195H6[36;1H[34h[?25h[?25l[52;195H7[37;1H[34h[?25h[?25l[52;195H8[38;1H[34h[?25h[?25l[52;195H9[39;1H[34h[?25h[?25l[52;194H40[40;1H[34h[?25h[?25l[52;195H1[41;1H[34h[?25h[?25l[52;195H2[42;1H[34h[?25h[?25l[52;195H3[43;1H[34h[?25h[?25l[52;195H4[44;1H[34h[?25h[?25l[52;195H5[45;1H[34h[?25h[?25l[52;195H6[46;1H[34h[?25h[?25l[52;195H7[47;1H[34h[?25h[?25l[52;195H8[48;1H[34h[?25h[?25l[52;195H9[49;1H[34h[?25h[?25l[52;194H50[50;1H[34h[?25h[?25l[52;195H1[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m sklearn.metrics [35mimport[0m make_scorer[52;1H[K[52;194H52,1[11C0%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mimport[0m tensorflow [33mas[0m tf[52;194H[K[52;194H53,1[11C0%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.python.keras [35mimport[0m backend [33mas[0m K[52;194H[K[52;194H54,1[11C0%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.layers [35mimport[0m Dense, Dropout, Flatten [34m#.core[0m[52;194H[K[52;194H55,1[11C1%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.optimizers [35mimport[0m SGD[52;194H[K[52;194H56,1[11C1%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.layers [35mimport[0m Dropout[52;194H[K[52;194H57,1[11C1%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m sklearn.externals [35mimport[0m joblib[52;194H[K[52;194H58,1[11C2%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.models [35mimport[0m Sequential, Model[52;194H[K[52;194H59,1[11C2%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.layers [35mimport[0m Dense[52;194H[K[52;194H60,1[11C2%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.wrappers.scikit_learn [35mimport[0m KerasRegressor [34m# or Classifier[0m[52;194H[K[52;194H61,1[11C2%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m sklearn.model_selection [35mimport[0m KFold[52;194H[K[52;194H62,1[11C3%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.optimizers [35mimport[0m SGD[52;194H[K[52;194H63,1[11C3%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.layers [35mimport[0m Dropout[52;194H[K[52;194H64,1[11C3%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mimport[0m random[52;194H[K[52;194H65,1[11C4%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb[0m[52;194H[K[52;194H66,1[11C4%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorboard.plugins.hparams [35mimport[0m api [33mas[0m hp[52;194H[K[52;194H67,1[11C4%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mimport[0m random[52;194H[K[52;194H68,1[11C5%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.python.keras.layers [35mimport[0m deserialize, serialize[52;194H[K[52;194H69,1[11C5%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.python.keras.saving [35mimport[0m saving_utils[52;194H[K[52;194H70,1[11C5%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H71,0-1[9C5%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H72,0-1[9C6%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H73,0-1[9C6%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#if snps == 'shuf' :[0m[52;194H[K[52;194H74,1[11C6%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#       print("Shuf nestedCV in usage")[0m[52;194H[K[52;194H75,1[11C7%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#       from nested_cv_shuf import NestedCV[0m[52;194H[K[52;194H76,1[11C7%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#elif snps == 'top':[0m[52;194H[K[52;194H77,1[11C7%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#       from nested_cv import NestedCV[0m[52;194H[K[52;194H78,1[11C7%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#else:[0m[52;194H[K[52;194H79,1[11C8%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#       print("snnps must be top or shuf")[0m[52;194H[K[52;194H80,1[11C8%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H81,1-8[9C8%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H82,0-1[9C9%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hsys.path.insert([31m1[0m, [31m'/external_storage/ciaran/Library/Python/3.7/python/site-packages/'[0m)[52;194H[K[52;194H83,1[11C9%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mimport[0m dill [33mas[0m pickle[52;194H[K[52;194H84,1[11C9%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mfor[0m i [33min[0m [36mrange[0m([31m1[0m,[36mlen[0m(sys.argv)):[52;194H[K[52;194H85,1[10C10%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m(sys.argv[i])[52;194H[K[52;194H86,1-8[8C10%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H87,0-1[8C10%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mif[0m [33mnot[0m os.path.exists([31m'/external_storage/ciaran/arabadopsis/'[0m + phenotype+ [31m'/'[0m + snps):[52;194H[K[52;194H88,1[10C10%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5Hos.makedirs([31m'/external_storage/ciaran/arabadopsis/'[0m + phenotype+ [31m'/'[0m + snps)[52;194H[K[52;194H89,1[10C11%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H90,0-1[8C11%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hos.chdir([31m'/external_storage/ciaran/arabadopsis/'[0m + phenotype+ [31m'/'[0m + snps)[52;194H[K[52;194H91,1[10C11%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hdate_object = datetime.datetime.now().replace(second=[31m0[0m,microsecond=[31m0[0m)[52;194H[K[52;194H92,1[10C12%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[36mprint[0m(date_object)[52;194H[K[52;194H93,1[10C12%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H94,0-1[8C12%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mcoeff_determination[0m(y_true, y_pred):[52;194H[K[52;194H95,1[10C12%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9HSS_res = K.sum(K.square( y_true-y_pred ))[52;194H[K[52;194H96,1[10C13%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9HSS_tot = K.sum(K.square( y_true - K.mean(y_true)))[52;194H[K[52;194H97,1[10C13%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mreturn[0m ([31m1[0m-SS_res/SS_tot)[52;194H[K[52;194H98,1[10C13%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H99,0-1[8C14%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H100,0-1[7C14%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mload_data[0m(data):[52;194H[K[52;194H101,1[9C14%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hdataset = np.loadtxt(data, skiprows=[31m1[0m)[52;194H[K[52;194H102,1[9C15%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hx = dataset[: , [31m6[0m:set_size+[31m6[0m]/[31m2[0m[52;194H[K[52;194H103,1[9C15%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hy = dataset[: , [31m5[0m ][52;194H[K[52;194H104,1[9C15%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hy = y.reshape(-[31m1[0m,[31m1[0m)[52;194H[K[52;194H105,1[9C15%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[34m#print("Performing split of raw data....")[0m[52;194H[K[52;194H106,1[9C16%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[34m#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.8, random_state=42)[0m[52;194H[K[52;194H107,1[9C16%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mreturn[0m x, y [34m#x_train, y_train, x_test, y_test[0m[52;194H[K[52;194H108,1[9C16%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H109,0-1[7C17%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H110,0-1[7C17%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H111,0-1[7C17%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mwith[0m [36mopen[0m(([31m'validation_results_'[0m+ [36mstr[0m(snps) +[36mstr[0m(num) + phenotype + [36mstr[0m([31m"{:%Y_%m_%d}"[0m.format(datetime.datetime.now())) + [31m'.vallog'[0m ), [31m'a'[0m) [33mas[0m f:[52;194H[K[52;194H112,1[9C17%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Horiginal_stdout = sys.stdout [34m# Save a reference to the original standard output[0m[52;194H[K[52;194H113,1[9C18%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hsys.stdout = f [34m# Change the standard output to the file we created.[0m[52;194H[K[52;194H114,1[9C18%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m(datetime.datetime.now())[52;194H[K[52;194H115,1[9C18%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hsys.stdout = original_stdout[52;194H[K[52;194H116,1[9C19%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H117,0-1[7C19%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mbaseline[0m(x, y):[52;194H[K[52;194H118,1[9C19%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hmodel = LinearRegression()[52;194H[K[52;194H119,1[9C20%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hmodel.fit(x, y)[52;194H[K[52;194H120,1[9C20%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mreturn[0m model[52;194H[K[52;194H121,1[9C20%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H122,0-1[7C20%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H123,0-1[7C21%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mavg_cv_result[0m(measure,cv_result):[52;194H[K[52;194H124,1[9C21%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hmy_var_name = [k [33mfor[0m k,v [33min[0m [36mlocals[0m().items() [33mif[0m v == measure][[31m0[0m] [34m#just to print out the name[0m[52;194H[K[52;194H125,1-8[7C21%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m(my_var_name)[52;194H[K[52;194H126,1-8[7C22%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hn_combos = [36mlen[0m(cv_result.cv_results_[[31m'split0_test_neg_mean_absolute_error'[0m])[52;194H[K[52;194H127,1-8[7C22%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hnamed_dict = {} [34m#dictonary will have a list of results PER grid combination across all CV results and this will return the average result. [0m[52;194H[K[52;194H128,1-8[7C22%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Havg_list = [][52;194H[K[52;194H129,1-8[7C22%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mfor[0m combo [33min[0m [36mrange[0m([31m0[0m, n_combos):[52;194H[K[52;194H130,1-8[7C23%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hnamed_dict[[36mstr[0m(combo)] = [][52;194H[K[52;194H131,1-8[7C23%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17H[33mfor[0m split [33min[0m measure:[52;194H[K[52;194H132,1-8[7C23%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;25Hnamed_dict[[36mstr[0m(combo)].append(cv_result.cv_results_[split][combo])[52;194H[K[52;194H133,1-8[7C24%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Havg_list.append(statistics.mean(named_dict[[36mstr[0m(combo)]))[52;194H[K[52;194H134,1-8[7C24%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17H[36mprint[0m(combo, statistics.mean(named_dict[[36mstr[0m(combo)]))[52;194H[K[52;194H135,1-8[7C24%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H136,0-1[7C25%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m'Max'[0m, np.nanmax(avg_list), np.where(avg_list == np.nanmax(avg_list)))[52;194H[K[52;194H137,1-8[7C25%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m'Min'[0m, np.nanmin(avg_list), np.where(avg_list == np.nanmin(avg_list)))[52;194H[K[52;194H138,1-8[7C25%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mreturn[0m avg_list[52;194H[K[52;194H139,1-8[7C25%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H140,0-1[7C26%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H141,0-1[7C26%[51;1H[34h[?25h[?25l[1;51r[1;1H[2M[1;52r[50;1H[33mdef[0m [36munpack[0m(model, training_config, weights): [34m##https://github.com/tensorflow/tensorflow/issues/34697 #fixes an error that the early stopping callback throws up in the nested cv #something about the parralele fitt[51;1Hting step needing everything to be pickle-able and the callback isnt [0m[52;194H[K[52;194H142,1[9C26%[50;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5Hrestored_model = deserialize(model)[52;194H[K[52;194H143,1[9C27%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5H[33mif[0m training_config [33mis[0m [33mnot[0m [36mNone[0m:[52;194H[K[52;194H144,1[9C27%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hrestored_model.compile([52;194H[K[52;194H145,1[9C27%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;13H**saving_utils.compile_args_from_training_config([52;194H[K[52;194H146,1[9C28%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Htraining_config[52;194H[K[52;194H147,1[9C28%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;13H)[52;194H[K[52;194H148,1[9C28%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H)[52;194H[K[52;194H149,1[9C29%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5Hrestored_model.set_weights(weights)[52;194H[K[52;194H150,1[9C29%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5H[33mreturn[0m restored_model[52;194H[K[52;194H151,1[9C29%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H152,0-1[7C29%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m# Hotfix function[0m[52;194H[K[52;194H153,1[9C30%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mmake_keras_picklable[0m():[52;194H[K[52;194H154,1[9C30%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H155,0-1[7C30%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5H[33mdef[0m [36m__reduce__[0m(self):[52;194H[K[52;194H156,1[9C31%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hmodel_metadata = saving_utils.model_metadata(self)[52;194H[K[52;194H157,1[9C31%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Htraining_config = model_metadata.get([31m"training_config"[0m, [36mNone[0m)[52;194H[K[52;194H158,1[9C31%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hmodel = serialize(self)[52;194H[K[52;194H159,1[9C31%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hweights = self.get_weights()[52;194H[K[52;194H160,1[9C32%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mreturn[0m (unpack, (model, training_config, weights))[52;194H[K[52;194H161,1[9C32%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H162,0-1[7C32%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5Hcls = Model[52;194H[K[52;194H163,1[9C33%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5Hcls.__reduce__ = __reduce__[52;194H[K[52;194H164,1[9C33%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H165,0-1[7C33%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H166,0-1[7C34%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mmake_param_box_plot[0m(goal_dict, time_dict):[52;194H[K[52;194H167,1[9C34%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mfor[0m param [33min[0m goal_dict:[52;194H[K[52;194H168,1-8[7C34%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.subplots([31m1[0m,[31m2[0m,figsize=([31m12[0m,[31m8[0m))[52;194H[K[52;194H169,1[9C34%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.subplot([31m121[0m)[52;194H[K[52;194H170,1[9C35%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.boxplot(goal_dict[param].values(), bootstrap=[36mNone[0m,showmeans=[36mFalse[0m, meanline=[36mFalse[0m, notch=[36mTrue[0m,labels=goal_dict[param].keys()) [34m#orange line is median, green dotted line is mean[0m[52;194H[K[52;194H171,1[9C35%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.xlabel([36mstr[0m(param).upper(), fontsize=[31m10[0m, fontweight=[31m'bold'[0m)[52;194H[K[52;194H172,1[9C35%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.ylabel([31m'R^2'[0m, fontsize=[31m10[0m,fontweight=[31m'bold'[0m)[52;194H[K[52;194H173,1[9C36%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.title([31m'R^2 Score vs %s'[0m % param, fontsize=[31m14[0m, fontweight=[31m'bold'[0m)[52;194H[K[52;194H174,1[9C36%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17H[33mif[0m param == [31m'initialization'[0m:[52;194H[K[52;194H175,1[9C36%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;25Hplt.xticks(fontsize=[31m6[0m)[52;194H[K[52;194H176,1[9C36%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.subplot([31m122[0m)[52;194H[K[52;194H177,1[9C37%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.boxplot(time_dict[param].values(), bootstrap=[36mNone[0m,showmeans=[36mFalse[0m, meanline=[36mFalse[0m, notch=[36mFalse[0m,labels=time_dict[param].keys())[52;194H[K[52;194H178,1[9C37%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.xlabel([36mstr[0m(param).upper(), fontsize=[31m10[0m, fontweight=[31m'bold'[0m)[52;194H[K[52;194H179,1[9C37%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.ylabel([31m'Training Time'[0m, fontsize=[31m10[0m,fontweight=[31m'bold'[0m)[52;194H[K[52;194H180,1[9C38%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.title([31m'Training Time vs %s'[0m % param, fontsize=[31m14[0m, fontweight=[31m'bold'[0m)[52;194H[K[52;194H181,1[9C38%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.tight_layout(pad=[31m4[0m)[52;194H[K[52;194H182,1[9C38%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17H[33mif[0m param == [31m'initialization'[0m:[52;194H[K[52;194H183,1[9C39%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;25Hplt.xticks(fontsize=[31m6[0m)[52;194H[K[52;194H184,1[9C39%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hmy_fig_name = [31m"plots_of_"[0m + [36mstr[0m(param) + [31m'_'[0m + [36mstr[0m([31m"{:%Y_%m_%d}"[0m.format(datetime.datetime.now())) + [31m'_'[0m +[36mstr[0m(snps) +[36mstr[0m(num)+ [31m".png"[0m[52;194H[K[52;194H185,1[9C39%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.savefig(my_fig_name, dpi=[31m300[0m)[52;194H[K[52;194H186,1[9C39%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.show()[52;194H[K[52;194H187,1[9C40%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.clf()[52;194H[K[52;194H188,1[9C40%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.close()[52;194H[K[52;194H189,1[9C40%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H190,1-8[7C41%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mmake_goal_dict[0m(whole_dict):[52;194H[K[52;194H191,1[9C41%[51;1H[34h[?25h[?25l[1;51r[1;1H[2M[1;52r[50;9H[36mprint[0m(whole_dict)[51;9Hgoal_dict = {key:{} [33mfor[0m key [33min[0m whole_dict}[52;194H[K[52;194H192,1-8[7C41%[50;8H[34h[?25h[?25l[52;196H3[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mfor[0m key [33min[0m whole_dict:[52;194H[K[52;194H194,1-8[7C42%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17H[33mfor[0m item [33min[0m whole_dict[key]:[52;194H[K[52;194H195,1-8[7C42%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;25Hgoal_dict[key][item] = [][52;194H[K[52;194H196,1-8[7C42%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Htime_dict = {key:{} [33mfor[0m key [33min[0m whole_dict} [34m#both empty[0m[52;194H[K[52;194H197,1-8[7C42%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mfor[0m key [33min[0m whole_dict:[52;194H[K[52;194H198,1-8[7C43%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17H[33mfor[0m item [33min[0m whole_dict[key]:[52;194H[K[52;194H199,1[9C43%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;25Htime_dict[key][item] = [][52;194H[K[52;194H200,1[9C43%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mreturn[0m goal_dict, time_dict[52;194H[K[52;194H201,1-8[7C44%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H202,0-1[7C44%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hx_train, y_train = load_data(data)[52;194H[K[52;194H203,1[9C44%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hname_list = np.loadtxt(data, skiprows=[31m1[0m, usecols=([31m0[0m,), dtype=[31m'str'[0m)[52;194H[K[52;194H204,1[9C45%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H205,0-1[7C45%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hscaler = preprocessing.StandardScaler().fit(y_train)[52;194H[K[52;194H206,1[9C45%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#pickle.dump(scaler, open('scaler.pkl', 'wb'))[0m[52;194H[K[52;194H207,1[9C45%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#scaler = pickle.load(open('scaler.pkl', 'rb'))[0m[52;194H[K[52;194H208,1[9C46%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H209,0-1[7C46%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hy_train = scaler.transform(y_train)[52;194H[K[52;194H210,1[9C46%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H211,0-1[7C47%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hn_snps = x_train.shape[[31m1[0m][52;194H[K[52;194H212,1[9C47%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hmy_cv = sklearn.model_selection.KFold(n_splits=[31m10[0m, shuffle=[36mTrue[0m, random_state=[31m42[0m)[52;194H[K[52;194H213,1[9C47%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#################################################SVM####SVM#####SVM####################################################################[0m[52;194H[K[52;194H214,1[9C47%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mncv_results[0m(analysis, ncv_object):[52;194H[K[52;194H215,1[9C48%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Best Params of %s is %s "[0m % (analysis, ncv_object.best_params))[52;194H[K[52;194H216,1[9C48%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Outer scores of %s is %s and mean is %s"[0m % (analysis, ncv_object.outer_scores, np.mean(ncv_object.outer_scores)))[52;194H[K[52;194H217,1[9C48%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Variance of %s is %s "[0m % (analysis, ncv_object.variance))[52;194H[K[52;194H218,1[9C49%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Goal dict of %s is %s "[0m % (analysis, ncv_object.goal_dict))[52;194H[K[52;194H219,1[9C49%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hmake_param_box_plot(ncv_object.goal_dict, ncv_object.time_dict)[52;194H[K[52;194H220,1[9C49%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mwith[0m [36mopen[0m([31m'NCV_'[0m + [36mstr[0m(analysis) + [31m'_'[0m +  [36mstr[0m(snps) + [31m'_'[0m + [36mstr[0m(phenotype) + [31m'_'[0m + [36mstr[0m(num) + [31m'.pkl'[0m, [31m'wb'[0m) [33mas[0m ncvfile: [34m#with open("fname.pkl", 'rb') as ncvfile:[0m[52;194H[K[52;194H221,1[9C50%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hpickle.dump(ncv_object, ncvfile) [34m#ncv_object = pickle.load(ncvfile)[0m[52;194H[K[52;194H222,1[9C50%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H223,0-1[7C50%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mnn_results[0m(analysis, ncv_object):[52;194H[K[52;194H224,1[9C50%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Best Params of %s is %s "[0m % (analysis, ncv_object.best_params))[52;194H[K[52;194H225,1[9C51%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Outer scores of %s is %s and mean is %s"[0m % (analysis, ncv_object.outer_scores, np.mean(ncv_object.outer_scores)))[52;194H[K[52;194H226,1[9C51%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Variance of %s is %s "[0m % (analysis, ncv_object.variance))[52;194H[K[52;194H227,1[9C51%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hnn_list = [ncv_object.best_inner_params_list, ncv_object.best_inner_score_list, ncv_object.best_params, ncv_object.metric, ncv_object.outer_scores, ncv_object.variance][52;194H[K[52;194H228,1[9C52%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mwith[0m [36mopen[0m([31m'NCV_'[0m + [36mstr[0m(analysis) + [31m'_'[0m +  [36mstr[0m(snps) + [31m'_'[0m + [36mstr[0m(phenotype) + [31m'_'[0m + [36mstr[0m(num) + [31m'.pkl'[0m, [31m'wb'[0m) [33mas[0m ncvfile:[52;194H[K[52;194H229,1[9C52%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hpickle.dump(nn_list, ncvfile) [34m#ncv_object = pickle.load(ncvfile)[0m[52;194H[K[52;194H230,1[9C52%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hncv_object.model.model.save([31m"model_"[0m + [36mstr[0m(analysis) + [31m'_'[0m +  [36mstr[0m(snps) + [31m'_'[0m + [36mstr[0m(phenotype) + [31m'_'[0m + [36mstr[0m(num) + [31m".h5"[0m)[52;194H[K[52;194H231,1[9C52%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H232,0-1[7C53%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H233,0-1[7C53%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[36mprint[0m([31m"Performing SVM"[0m)[52;194H[K[52;194H234,1[9C53%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hc_param = [[31m1[0m,[31m2[0m][52;194H[K[52;194H235,1[9C54%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hgamma_param = [[36mfloat[0m(x) [33mfor[0m x [33min[0m np.linspace([31m0.1[0m, [31m1[0m, [31m4[0m)][52;194H[K[52;194H236,1[9C54%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H237,0-1[7C54%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H238,0-1[7C55%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hepsilon_param = [[36mfloat[0m(x) [33mfor[0m x [33min[0m np.linspace([31m0.1[0m, [31m1[0m, [31m4[0m)][52;194H[K[52;194H239,1[9C55%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hloss_param = [[31m'epsilon_insensitive'[0m, [31m'squared_epsilon_insensitive'[0m][52;194H[K[52;194H240,1[9C55%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hkernel_param = [[31m'poly'[0m][52;194H[K[52;194H241,1[9C55%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hdegree = [[31m1[0m,[31m2[0m,[31m3[0m][52;194H[K[52;194H242,1[9C56%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hsvm_random_grid = {[31m'gamma'[0m:gamma_param, [31m'C'[0m:c_param,[31m'kernel'[0m:kernel_param, [31m"degree"[0m:degree}[52;194H[K[52;194H243,1[9C56%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[36mprint[0m(svm_random_grid)[52;194H[K[52;194H244,1[9C56%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hsvm_random_grid2 = {[31m'C'[0m : c_param, [31m'loss'[0m:loss_param}[52;194H[K[52;194H245,1[9C57%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[36mprint[0m(svm_random_grid2)[52;194H[K[52;194H246,1[9C57%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hrbg_goal_dict, rbg_time_dict = make_goal_dict(svm_random_grid)[52;194H[K[52;194H247,1[9C57%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hsvm_goal_dict, svm_time_dict = make_goal_dict(svm_random_grid2)[52;194H[K[52;194H248,1[9C57%[51;1H[34h[?25h[?25l[1;51r[1;1H[2M[1;52r[50;1HSVM_NCV = NestedCV(model_name=[31m'LinearSVR'[0m, name_list = name_list, model=LinearSVR(), goal_dict=svm_goal_dict, time_dict=svm_time_dict, params_grid=svm_random_grid2, outer_kfolds=[31m2[0m, inner_kfolds=[31m2[0m, n_jobs = [31m8[0m,cv__[51;1Hoptions={[31m'randomized_search'[0m:[36mTrue[0m, [31m'randomized_search_iter'[0m:[31m50[0m, [31m'sqrt_of_score'[0m:[36mFalse[0m,[31m'recursive_feature_elimination'[0m:[36mFalse[0m, [31m'metric'[0m:sklearn.metrics.r2_score, [31m'metric_score_indicator_lower'[0m:[36mFalse[0m})[52;194H[K[52;194H249,1[9C58%[50;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1HSVM_NCV.fit(x_train, y_train.ravel(), name_list=name_list, phenfile=phenfile, set_size=set_size, snps=snps, model_name=[31m'SVM'[0m, goal_dict=svm_goal_dict, time_dict=svm_time_dict)[52;194H[K[52;194H250,1[9C58%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#make_param_box_plot(goal_dict, time_dict)[0m[52;194H[K[52;194H251,1[9C58%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H252,0-1[7C59%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hncv_results([31m'SVM'[0m, SVM_NCV)[52;194H[K[52;194H253,1[9C59%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31m'''[0m[52;194H[K[52;194H254,1[9C59%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mprint("Performing RBG")[0m[52;194H[K[52;194H255,1[9C60%[51;1H[34h[?25h[?25l[1;51r[1;1H[2M[1;52r[50;1H[31mRBG_NCV = NestedCV(model_name='RBG', name_list=name_list, model=SVR(), params_grid=svm_random_grid, outer_kfolds=4, inner_kfolds=4, n_jobs = 8,cv_options={'randomized_search':True, 'randomized_search_iter':50, ''[51;1Hsqrt_of_score':False,'recursive_feature_elimination':False, 'metric':sklearn.metrics.r2_score, 'metric_score_indicator_lower':False})[0m[52;194H[K[52;194H256,1[9C60%[50;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mRBG_NCV.fit(x_train, y_train.ravel(), name_list=name_list, phenfile=phenfile, set_size=set_size, snps=snps, model_name='RBG')[0m[52;194H[K[52;194H257,1[9C60%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mncv_results('RBG', RBG_NCV)[0m[52;194H[K[52;194H258,1[9C61%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H259,0-1[7C61%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mprint("Performing LASSO")[0m[52;194H[K[52;194H260,1[9C61%[51;1H[34h[?25h[?25l[52;195H59,0-1[50;1H[34h[?25h[?25l[52;196H8,1  [49;1H[34h[?25h[?25l[52;196H7[48;1H[34h[?25h[?25l[52;196H6[46;1H[34h[?25h[?25l[52;196H5[45;1H[34h[?25h[?25l[52;196H4[44;1H[34h[?25h[?25l[52;196H3[43;1H[34h[?25h[?25l[52;196H2,0-1[42;1H[34h[?25h[?25l[52;196H1,1  [41;1H[34h[?25h[?25l[52;196H0[40;1H[34h[?25h[?25l[52;195H49[38;1H[34h[?25h[?25l[52;196H8[37;1H[34h[?25h[?25l[52;196H7[36;1H[34h[?25h[?25l[52;196H8[37;1H[34h[?25h[?25l[52;196H9[38;1H[34h[?25h[?25l[52;198H2[38;2H[34h[?25h[?25l[52;198H3[38;3H[34h[?25h[?25l[52;198H4[38;4H[34h[?25h[?25l[52;198H5[38;5H[34h[?25h[?25l[52;198H6[38;6H[34h[?25h[?25l[52;198H7[38;7H[34h[?25h[?25l[52;198H8[38;8H[34h[?25h[?25l[52;198H9[38;9H[34h[?25h[?25l[52;198H10[38;10H[34h[?25h[?25l[52;199H1[38;11H[34h[?25h[?25l[52;199H2[38;12H[34h[?25h[?25l[52;199H3[38;13H[34h[?25h[?25l[52;199H4[38;14H[34h[?25h[?25l[52;199H5[38;15H[34h[?25h[?25l[52;199H6[38;16H[34h[?25h[?25l[52;199H7[38;17H[34h[?25h[?25l[52;199H8[38;18H[34h[?25h[?25lV[46m([0m[191C__[39;1Ho[196C[46m)[0m[52;199H9[38;19H[34h[?25h[?25l([191C__[39;1Ho[196C)[52;198H20[38;20H[34h[?25h[?25l[52;199H1[38;21H[34h[?25h[?25l[52;196H8[37;21H[34h[?25h[?25l[52;196H7[36;21H[34h[?25h[?25l[52;196H6[35;21H[34h[?25h[?25l[52;196H5[34;21H[34h[?25h[?25l[52;196H4[33;21H[34h[?25h[?25l[52;196H3[32;21H[34h[?25h[?25l[31;10H[46m[[5C][0m[52;196H2,16[31;16H[34h[?25h[?25l[[5C][52;196H1,21[30;21H[34h[?25h[?25l[31;10H[46m[[5C][0m[52;196H2,16[31;16H[34h[?25h[?25l[[5C][52;196H1,21[30;21H[34h[?25h[?25l[52;196H0[29;21H[34h[?25h[?25l[52;195H39[28;21H[34h[?25h[?25l[52;196H8,0-1[27;1H[34h[?25h[?25l[52;196H7[26;1H[34h[?25h[?25l[25;21H[46m([0mx[46m)[0m[52;196H6,21 [25;21H[34h[?25h[?25l[24;11H[46m[[3C][0m[25;21H(x)[52;196H5,15[24;15H[34h[?25h[?25l[[3C][52;199H4[24;14H[34h[?25h[?25l[46m[[3C][0m[52;199H5[24;15H[34h[?25h[?25l[52;1H[1m-- INSERT --[0m[52;194H[K[52;194H235,15[8C61%[24;15H[34h[?25h[?25l[46m,[0m],[46m][0m[52;199H6[24;16H[34h[?25h[?25l[46m [0m] [46m][0m[52;199H7[24;17H[34h[?25h[?25l[31m[46m1[0m][31m1[0m[46m][0m[52;199H8[24;18H[34h[?25h[?25l[31m[46m0[0m][31m0[0m[46m][0m[52;199H9[24;19H[34h[?25h[?25l[46m,[0m],[46m][0m[52;198H20[24;20H[34h[?25h[?25l[46m [0m] [46m][0m[52;199H1[24;21H[34h[?25h[?25l[31m[46m1[0m][31m1[0m[46m][0m[52;199H2[24;22H[34h[?25h[?25l[31m[46m0[0m][31m0[0m[46m][0m[52;199H3[24;23H[34h[?25h[?25l[31m[46m0[0m][31m0[0m[46m][0m[52;199H4[24;24H[34h[?25h[52;1H[K[24;23H[?25l[24;11H[[12C][52;194H235,23[8C61%[24;23H[34h[?25h[?25l[52;194H[K[52;1H:[34h[?25hx[?25l[34h[?25h[?25l"cv_grid_all_ml.py" 391L, 23154C written
[?1l>[34h[?25h[?1049l(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ #chnage vack c level
(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ #chnage vack c levelvi cv_grid_all_ml.pygit commit -m "boxplotting seems to be working"add . [Kpython cv_grid_all_ml.py 31 /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del /home/ciaran/completed_big_matrix_binary_new_snps_ids__first_10k.raw shuf FT10cv_new_10k 10000
Please remember to set the right set size in the nested_cv code
/external_storage/ciaran/machine_learning2
WARNING THIS IS AN EDITED SCRIPT - Ciaran Kelly 2021 
 Edited with permission under liscence 
 Flex version
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
31
/home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del
/home/ciaran/completed_big_matrix_binary_new_snps_ids_first_10k.raw
shuf
FT10cv_new_10k
10000
2021-12-19 23:29:00
Performing SVM
{'kernel': ['poly'], 'C': [1, 2, 10, 100], 'degree': [1, 2, 3], 'gamma': [0.1, 0.4, 0.7, 1.0]}
{'loss': ['epsilon_insensitive', 'squared_epsilon_insensitive'], 'C': [1, 2, 10, 100]}
{'kernel': ['poly'], 'C': [1, 2, 10, 100], 'degree': [1, 2, 3], 'gamma': [0.1, 0.4, 0.7, 1.0]}
{'loss': ['epsilon_insensitive', 'squared_epsilon_insensitive'], 'C': [1, 2, 10, 100]}
ADBUCE
INNER COUNT NO.  1
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_1_in_1_in.raw
Set size set to 10000
Set size set to 10000
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 8 is smaller than n_iter=50. Running 8 iterations. For exhaustive searches, use GridSearchCV.
  % (grid_size, self.n_iter, grid_size), UserWarning)
{'loss': 'epsilon_insensitive', 'C': 1}
Blue
Red
0.18791717601472946
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 1}
Blue
Red
0.19046308751729468
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 2}
Blue
Red
0.18788974358143196
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 2}
Blue
Red
0.18920236540301327
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 10}
Blue
Red
0.18789054022284923
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 10}
Blue
Red
0.18814941214403458
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 100}
Blue
Red
0.1879118086030317
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 100}
Blue
Red
0.18792261374755204
Typer LinearSVR
INNER COUNT NO.  2
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_1_in_2_in.raw
Set size set to 10000
Set size set to 10000
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 8 is smaller than n_iter=50. Running 8 iterations. For exhaustive searches, use GridSearchCV.
  % (grid_size, self.n_iter, grid_size), UserWarning)
{'loss': 'epsilon_insensitive', 'C': 1}
Blue
Red
0.4000805475558258
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 1}
Blue
Red
0.40226057629854783
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 2}
Blue
Red
0.40008057676609654
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 2}
Blue
Red
0.401178040592836
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 10}
Blue
Red
0.4000773469424147
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 10}
Blue
Red
0.4003031249907434
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 100}
Blue
Red
0.4000893555058771
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 100}
Blue
Red
0.400101673815595
Typer LinearSVR
OUTER COUNT NO.  1
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_1_in_2_out.raw
Set size set to 10000
Set size set to 10000
0.39627752901866853
INNER COUNT NO.  1
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_2_in_1_in.raw
Set size set to 10000
Set size set to 10000
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 8 is smaller than n_iter=50. Running 8 iterations. For exhaustive searches, use GridSearchCV.
  % (grid_size, self.n_iter, grid_size), UserWarning)
{'loss': 'epsilon_insensitive', 'C': 1}
Blue
Red
0.400995826323598
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 1}
Blue
Red
0.4027858958901791
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 2}
Blue
Red
0.4010075854174344
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 2}
Blue
Red
0.4019144764215986
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 10}
Blue
Red
0.40100241846028717
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 10}
Blue
Red
0.40118723420658287
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 100}
Blue
Red
0.4010060013856218
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 100}
Blue
Red
0.40102047307427247
Typer LinearSVR
INNER COUNT NO.  2
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_2_in_2_in.raw
Set size set to 10000
Set size set to 10000
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 8 is smaller than n_iter=50. Running 8 iterations. For exhaustive searches, use GridSearchCV.
  % (grid_size, self.n_iter, grid_size), UserWarning)
{'loss': 'epsilon_insensitive', 'C': 1}
Blue
Red
0.4208552124305588
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 1}
Blue
Red
0.4229094451907469
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 2}
Blue
Red
0.4208411940800002
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 2}
Blue
Red
0.4218994420403094
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 10}
Blue
Red
0.4208451270039272
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 10}
Blue
Red
0.42105429004628914
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 100}
Blue
Red
0.42084928148332845
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 100}
Blue
Red
0.42086069298261364
Typer LinearSVR
OUTER COUNT NO.  2
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_2_in_2_out.raw
Set size set to 10000
Set size set to 10000
0.3439745034910203
Best Params of SVM is {'loss': ['squared_epsilon_insensitive'], 'C': [1]} 
Outer scores of SVM is [0.39627752901866853, 0.3439745034910203] and mean is 0.3701260162548444
Variance of SVM is [0.5401588990195139, 0.4136399806826012] 
Goal dict of SVM is {'loss': {'squared_epsilon_insensitive': [0.19046308751729468, 0.18920236540301327, 0.18814941214403458, 0.18792261374755204, 0.40226057629854783, 0.401178040592836, 0.4003031249907434, 0.400101673815595, 0.4027858958901791, 0.4019144764215986, 0.40118723420658287, 0.40102047307427247, 0.4229094451907469, 0.4218994420403094, 0.42105429004628914, 0.42086069298261364], 'epsilon_insensitive': [0.18791717601472946, 0.18788974358143196, 0.18789054022284923, 0.1879118086030317, 0.4000805475558258, 0.40008057676609654, 0.4000773469424147, 0.4000893555058771, 0.400995826323598, 0.4010075854174344, 0.40100241846028717, 0.4010060013856218, 0.4208552124305588, 0.4208411940800002, 0.4208451270039272, 0.42084928148332845]}, 'C': {1: [0.18791717601472946, 0.19046308751729468, 0.4000805475558258, 0.40226057629854783, 0.400995826323598, 0.4027858958901791, 0.4208552124305588, 0.4229094451907469], 2: [0.18788974358143196, 0.18920236540301327, 0.40008057676609654, 0.401178040592836, 0.4010075854174344, 0.4019144764215986, 0.4208411940800002, 0.4218994420403094], 100: [0.1879118086030317, 0.18792261374755204, 0.4000893555058771, 0.400101673815595, 0.4010060013856218, 0.40102047307427247, 0.42084928148332845, 0.42086069298261364], 10: [0.18789054022284923, 0.18814941214403458, 0.4000773469424147, 0.4003031249907434, 0.40100241846028717, 0.40118723420658287, 0.4208451270039272, 0.42105429004628914]}} 
Performing Neural Network
Traceback (most recent call last):
  File "cv_grid_all_ml.py", line 343, in <module>
    NN_NCV = NestedCV(model_name='nn_model', name_list = name_list, model=nn_model, params_grid=param_grid, outer_kfolds=4, inner_kfolds=4, n_jobs = 32,cv_options={'randomized_search':True, 'randomized_search_iter':100, 'sqrt_of_score':False,'recursive_feature_elimination':False, 'metric':sklearn.metrics.r2_score, 'metric_score_indicator_lower':False})
TypeError: __init__() missing 2 required positional arguments: 'goal_dict' and 'time_dict'
(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ python cv_grid_all_ml.py 31 /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del /home/ciaran/completed_big_matrix_binary_new_snps_ids__first_10k.raw shuf FT10cv_new_10k 10000M[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C#chnage vack c level[K
[KM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cvi cv_grid_all_ml.py
[?1049h[?1h=[1;52r[34l[34h[?25h[23m[24m[0m[H[J[?25l[52;1H"cv_grid_all_ml.py" 391L, 23154C[1;1H[34m#Warning : best model selected by NMAE and R2 might not be the same
#performs linear regression, linear regression, neural network, svm and random forest, LASSO, RIDGE, CNN
#source ~/venv/bin/activate #in python 3.5.2
#print a log to a .txt file!
#model = pickle.load(open('FILEPATH', 'rb')) 
#dependencies = {'coeff_determination':coeff_determination}
#model = tf.keras.models.load_model('FILEPATH', custom_objects=dependencies)
#

#import tensorflow
#import numpy as np; import scipy #need to do this before path insert
#import sys
#sys.path.insert(1, '/external_storage/ciaran/Library/Python/3.7/python/site-packages/')
#import dill as pickle
#sys.path.insert(1, '/external_storage/ciaran/Library/Python/3.7/python/site-packages/nested_cv')
#from nested_cv import NestedCV
#with open('NCV_NN.pkl', 'rb') as f:
#     red = pickle.load(f)[0m

[36mprint[0m([31m"Please remember to set the right set size in the nested_cv code"[0m)
[35mimport[0m sys
sys.path.insert([31m1[0m, [31m'/external_storage/ciaran/Library/Python/3.7/python/site-packages/nested_cv'[0m)
num = sys.argv[[31m1[0m] [34m#script number for saving out[0m
phenfile = [36mstr[0m(sys.argv[[31m2[0m]) [34m#txt file with phenotypes[0m
data = [36mstr[0m(sys.argv[[31m3[0m]) [34m#needs to be same size as set_size[0m
snps = [36mstr[0m(sys.argv[[31m4[0m]) [34m#top or shuf[0m
phenotype = [36mstr[0m(sys.argv[[31m5[0m]) [34m#make a directory for the results[0m
set_size = [36mint[0m(sys.argv[[31m6[0m]) [34m#how many SNPs[0m
[35mfrom[0m nested_cv [35mimport[0m NestedCV
[35mimport[0m statistics
[35mimport[0m numpy [33mas[0m np
[35mimport[0m sklearn
[35mimport[0m seaborn [33mas[0m sns
[35mfrom[0m sklearn [35mimport[0m preprocessing
[35mfrom[0m sklearn.pipeline [35mimport[0m make_pipeline, Pipeline
[35mfrom[0m sklearn.preprocessing [35mimport[0m StandardScaler
[35mimport[0m datetime
[35mimport[0m time
[35mfrom[0m statistics [35mimport[0m mean
[35mimport[0m matplotlib.pyplot [33mas[0m plt
[35mfrom[0m matplotlib [35mimport[0m style
[35mfrom[0m sklearn.svm [35mimport[0m LinearSVR
[35mfrom[0m sklearn.svm [35mimport[0m SVR
[35mfrom[0m sklearn.model_selection [35mimport[0m train_test_split
[35mfrom[0m sklearn.model_selection [35mimport[0m RandomizedSearchCV
[35mfrom[0m sklearn.linear_model [35mimport[0m LinearRegression, Lasso, Ridge
[34m#import pickle #use dill instead below[0m
[35mfrom[0m sklearn.ensemble [35mimport[0m RandomForestRegressor [34m#based in part on https://towardsdatascience.com/random-forest-in-python-24d0893d51c0[0m
[35mfrom[0m statistics [35mimport[0m mean
[35mimport[0m os
[35mimport[0m pandas [33mas[0m pd [34m# data processing, CSV file I/O (e.g. pd.read_csv)[0m[52;194H1,1[11CTop[1;1H[34h[?25h[?25l[52;194H2[2;1H[34h[?25h[?25l[52;194H3[3;1H[34h[?25h[?25l[52;194H4[4;1H[34h[?25h[?25l[52;194H5[5;1H[34h[?25h[?25l[52;194H6[6;1H[34h[?25h[?25l[52;194H7[7;1H[34h[?25h[?25l[52;194H8[8;1H[34h[?25h[?25l[52;194H9,0-1[9;1H[34h[?25h[?25l[52;194H10,1 [10;1H[34h[?25h[?25l[52;195H1[11;1H[34h[?25h[?25l[52;195H2[12;1H[34h[?25h[?25l[52;195H3[13;1H[34h[?25h[?25l[52;195H4[14;1H[34h[?25h[?25l[52;195H5[15;1H[34h[?25h[?25l[52;195H6[16;1H[34h[?25h[?25l[52;195H7[17;1H[34h[?25h[?25l[52;195H8[18;1H[34h[?25h[?25l[52;195H9,0-1[19;1H[34h[?25h[?25l[52;194H20,1  [20;1H[34h[?25h[?25l[52;195H1[21;1H[34h[?25h[?25l[52;195H2[22;1H[34h[?25h[?25l[52;195H3[23;1H[34h[?25h[?25l[52;195H4[24;1H[34h[?25h[?25l[52;195H5[25;1H[34h[?25h[?25l[52;195H6[26;1H[34h[?25h[?25l[52;195H7[27;1H[34h[?25h[?25l[52;195H8[28;1H[34h[?25h[?25l[52;195H9[29;1H[34h[?25h[?25l[52;194H30[30;1H[34h[?25h[?25l[52;195H1[31;1H[34h[?25h[?25l[52;195H2[32;1H[34h[?25h[?25l[52;195H3[33;1H[34h[?25h[?25l[52;195H4[34;1H[34h[?25h[?25l[52;195H5[35;1H[34h[?25h[?25l[52;195H6[36;1H[34h[?25h[?25l[52;195H7[37;1H[34h[?25h[?25l[52;195H8[38;1H[34h[?25h[?25l[52;195H9[39;1H[34h[?25h[?25l[52;194H40[40;1H[34h[?25h[?25l[52;195H1[41;1H[34h[?25h[?25l[52;195H2[42;1H[34h[?25h[?25l[52;195H3[43;1H[34h[?25h[?25l[52;195H4[44;1H[34h[?25h[?25l[52;195H5[45;1H[34h[?25h[?25l[52;195H6[46;1H[34h[?25h[?25l[52;195H7[47;1H[34h[?25h[?25l[52;195H8[48;1H[34h[?25h[?25l[52;195H9[49;1H[34h[?25h[?25l[52;194H50[50;1H[34h[?25h[?25l[52;195H1[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m sklearn.metrics [35mimport[0m make_scorer[52;1H[K[52;194H52,1[11C0%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mimport[0m tensorflow [33mas[0m tf[52;194H[K[52;194H53,1[11C0%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.python.keras [35mimport[0m backend [33mas[0m K[52;194H[K[52;194H54,1[11C0%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.layers [35mimport[0m Dense, Dropout, Flatten [34m#.core[0m[52;194H[K[52;194H55,1[11C1%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.optimizers [35mimport[0m SGD[52;194H[K[52;194H56,1[11C1%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.layers [35mimport[0m Dropout[52;194H[K[52;194H57,1[11C1%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m sklearn.externals [35mimport[0m joblib[52;194H[K[52;194H58,1[11C2%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.models [35mimport[0m Sequential, Model[52;194H[K[52;194H59,1[11C2%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.layers [35mimport[0m Dense[52;194H[K[52;194H60,1[11C2%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.wrappers.scikit_learn [35mimport[0m KerasRegressor [34m# or Classifier[0m[52;194H[K[52;194H61,1[11C2%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m sklearn.model_selection [35mimport[0m KFold[52;194H[K[52;194H62,1[11C3%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.optimizers [35mimport[0m SGD[52;194H[K[52;194H63,1[11C3%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.layers [35mimport[0m Dropout[52;194H[K[52;194H64,1[11C3%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mimport[0m random[52;194H[K[52;194H65,1[11C4%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb[0m[52;194H[K[52;194H66,1[11C4%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorboard.plugins.hparams [35mimport[0m api [33mas[0m hp[52;194H[K[52;194H67,1[11C4%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mimport[0m random[52;194H[K[52;194H68,1[11C5%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.python.keras.layers [35mimport[0m deserialize, serialize[52;194H[K[52;194H69,1[11C5%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.python.keras.saving [35mimport[0m saving_utils[52;194H[K[52;194H70,1[11C5%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H71,0-1[9C5%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H72,0-1[9C6%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H73,0-1[9C6%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#if snps == 'shuf' :[0m[52;194H[K[52;194H74,1[11C6%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#       print("Shuf nestedCV in usage")[0m[52;194H[K[52;194H75,1[11C7%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#       from nested_cv_shuf import NestedCV[0m[52;194H[K[52;194H76,1[11C7%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#elif snps == 'top':[0m[52;194H[K[52;194H77,1[11C7%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#       from nested_cv import NestedCV[0m[52;194H[K[52;194H78,1[11C7%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#else:[0m[52;194H[K[52;194H79,1[11C8%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#       print("snnps must be top or shuf")[0m[52;194H[K[52;194H80,1[11C8%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H81,1-8[9C8%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H82,0-1[9C9%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hsys.path.insert([31m1[0m, [31m'/external_storage/ciaran/Library/Python/3.7/python/site-packages/'[0m)[52;194H[K[52;194H83,1[11C9%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mimport[0m dill [33mas[0m pickle[52;194H[K[52;194H84,1[11C9%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mfor[0m i [33min[0m [36mrange[0m([31m1[0m,[36mlen[0m(sys.argv)):[52;194H[K[52;194H85,1[10C10%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m(sys.argv[i])[52;194H[K[52;194H86,1-8[8C10%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H87,0-1[8C10%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mif[0m [33mnot[0m os.path.exists([31m'/external_storage/ciaran/arabadopsis/'[0m + phenotype+ [31m'/'[0m + snps):[52;194H[K[52;194H88,1[10C10%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5Hos.makedirs([31m'/external_storage/ciaran/arabadopsis/'[0m + phenotype+ [31m'/'[0m + snps)[52;194H[K[52;194H89,1[10C11%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H90,0-1[8C11%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hos.chdir([31m'/external_storage/ciaran/arabadopsis/'[0m + phenotype+ [31m'/'[0m + snps)[52;194H[K[52;194H91,1[10C11%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hdate_object = datetime.datetime.now().replace(second=[31m0[0m,microsecond=[31m0[0m)[52;194H[K[52;194H92,1[10C12%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[36mprint[0m(date_object)[52;194H[K[52;194H93,1[10C12%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H94,0-1[8C12%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mcoeff_determination[0m(y_true, y_pred):[52;194H[K[52;194H95,1[10C12%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9HSS_res = K.sum(K.square( y_true-y_pred ))[52;194H[K[52;194H96,1[10C13%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9HSS_tot = K.sum(K.square( y_true - K.mean(y_true)))[52;194H[K[52;194H97,1[10C13%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mreturn[0m ([31m1[0m-SS_res/SS_tot)[52;194H[K[52;194H98,1[10C13%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H99,0-1[8C14%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H100,0-1[7C14%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mload_data[0m(data):[52;194H[K[52;194H101,1[9C14%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hdataset = np.loadtxt(data, skiprows=[31m1[0m)[52;194H[K[52;194H102,1[9C15%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hx = dataset[: , [31m6[0m:set_size+[31m6[0m]/[31m2[0m[52;194H[K[52;194H103,1[9C15%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hy = dataset[: , [31m5[0m ][52;194H[K[52;194H104,1[9C15%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hy = y.reshape(-[31m1[0m,[31m1[0m)[52;194H[K[52;194H105,1[9C15%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[34m#print("Performing split of raw data....")[0m[52;194H[K[52;194H106,1[9C16%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[34m#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.8, random_state=42)[0m[52;194H[K[52;194H107,1[9C16%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mreturn[0m x, y [34m#x_train, y_train, x_test, y_test[0m[52;194H[K[52;194H108,1[9C16%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H109,0-1[7C17%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H110,0-1[7C17%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H111,0-1[7C17%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mwith[0m [36mopen[0m(([31m'validation_results_'[0m+ [36mstr[0m(snps) +[36mstr[0m(num) + phenotype + [36mstr[0m([31m"{:%Y_%m_%d}"[0m.format(datetime.datetime.now())) + [31m'.vallog'[0m ), [31m'a'[0m) [33mas[0m f:[52;194H[K[52;194H112,1[9C17%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Horiginal_stdout = sys.stdout [34m# Save a reference to the original standard output[0m[52;194H[K[52;194H113,1[9C18%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hsys.stdout = f [34m# Change the standard output to the file we created.[0m[52;194H[K[52;194H114,1[9C18%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m(datetime.datetime.now())[52;194H[K[52;194H115,1[9C18%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hsys.stdout = original_stdout[52;194H[K[52;194H116,1[9C19%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H117,0-1[7C19%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mbaseline[0m(x, y):[52;194H[K[52;194H118,1[9C19%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hmodel = LinearRegression()[52;194H[K[52;194H119,1[9C20%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hmodel.fit(x, y)[52;194H[K[52;194H120,1[9C20%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mreturn[0m model[52;194H[K[52;194H121,1[9C20%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H122,0-1[7C20%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H123,0-1[7C21%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mavg_cv_result[0m(measure,cv_result):[52;194H[K[52;194H124,1[9C21%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hmy_var_name = [k [33mfor[0m k,v [33min[0m [36mlocals[0m().items() [33mif[0m v == measure][[31m0[0m] [34m#just to print out the name[0m[52;194H[K[52;194H125,1-8[7C21%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m(my_var_name)[52;194H[K[52;194H126,1-8[7C22%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hn_combos = [36mlen[0m(cv_result.cv_results_[[31m'split0_test_neg_mean_absolute_error'[0m])[52;194H[K[52;194H127,1-8[7C22%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hnamed_dict = {} [34m#dictonary will have a list of results PER grid combination across all CV results and this will return the average result. [0m[52;194H[K[52;194H128,1-8[7C22%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Havg_list = [][52;194H[K[52;194H129,1-8[7C22%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mfor[0m combo [33min[0m [36mrange[0m([31m0[0m, n_combos):[52;194H[K[52;194H130,1-8[7C23%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hnamed_dict[[36mstr[0m(combo)] = [][52;194H[K[52;194H131,1-8[7C23%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17H[33mfor[0m split [33min[0m measure:[52;194H[K[52;194H132,1-8[7C23%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;25Hnamed_dict[[36mstr[0m(combo)].append(cv_result.cv_results_[split][combo])[52;194H[K[52;194H133,1-8[7C24%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Havg_list.append(statistics.mean(named_dict[[36mstr[0m(combo)]))[52;194H[K[52;194H134,1-8[7C24%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17H[36mprint[0m(combo, statistics.mean(named_dict[[36mstr[0m(combo)]))[52;194H[K[52;194H135,1-8[7C24%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H136,0-1[7C25%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m'Max'[0m, np.nanmax(avg_list), np.where(avg_list == np.nanmax(avg_list)))[52;194H[K[52;194H137,1-8[7C25%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m'Min'[0m, np.nanmin(avg_list), np.where(avg_list == np.nanmin(avg_list)))[52;194H[K[52;194H138,1-8[7C25%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mreturn[0m avg_list[52;194H[K[52;194H139,1-8[7C25%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H140,0-1[7C26%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H141,0-1[7C26%[51;1H[34h[?25h[?25l[1;51r[1;1H[2M[1;52r[50;1H[33mdef[0m [36munpack[0m(model, training_config, weights): [34m##https://github.com/tensorflow/tensorflow/issues/34697 #fixes an error that the early stopping callback throws up in the nested cv #something about the parralele fitt[51;1Hting step needing everything to be pickle-able and the callback isnt [0m[52;194H[K[52;194H142,1[9C26%[50;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5Hrestored_model = deserialize(model)[52;194H[K[52;194H143,1[9C27%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5H[33mif[0m training_config [33mis[0m [33mnot[0m [36mNone[0m:[52;194H[K[52;194H144,1[9C27%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hrestored_model.compile([52;194H[K[52;194H145,1[9C27%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;13H**saving_utils.compile_args_from_training_config([52;194H[K[52;194H146,1[9C28%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Htraining_config[52;194H[K[52;194H147,1[9C28%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;13H)[52;194H[K[52;194H148,1[9C28%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H)[52;194H[K[52;194H149,1[9C29%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5Hrestored_model.set_weights(weights)[52;194H[K[52;194H150,1[9C29%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5H[33mreturn[0m restored_model[52;194H[K[52;194H151,1[9C29%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H152,0-1[7C29%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m# Hotfix function[0m[52;194H[K[52;194H153,1[9C30%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mmake_keras_picklable[0m():[52;194H[K[52;194H154,1[9C30%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H155,0-1[7C30%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5H[33mdef[0m [36m__reduce__[0m(self):[52;194H[K[52;194H156,1[9C31%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hmodel_metadata = saving_utils.model_metadata(self)[52;194H[K[52;194H157,1[9C31%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Htraining_config = model_metadata.get([31m"training_config"[0m, [36mNone[0m)[52;194H[K[52;194H158,1[9C31%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hmodel = serialize(self)[52;194H[K[52;194H159,1[9C31%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hweights = self.get_weights()[52;194H[K[52;194H160,1[9C32%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mreturn[0m (unpack, (model, training_config, weights))[52;194H[K[52;194H161,1[9C32%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H162,0-1[7C32%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5Hcls = Model[52;194H[K[52;194H163,1[9C33%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5Hcls.__reduce__ = __reduce__[52;194H[K[52;194H164,1[9C33%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H165,0-1[7C33%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H166,0-1[7C34%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mmake_param_box_plot[0m(goal_dict, time_dict):[52;194H[K[52;194H167,1[9C34%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mfor[0m param [33min[0m goal_dict:[52;194H[K[52;194H168,1-8[7C34%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.subplots([31m1[0m,[31m2[0m,figsize=([31m12[0m,[31m8[0m))[52;194H[K[52;194H169,1[9C34%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.subplot([31m121[0m)[52;194H[K[52;194H170,1[9C35%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.boxplot(goal_dict[param].values(), bootstrap=[36mNone[0m,showmeans=[36mFalse[0m, meanline=[36mFalse[0m, notch=[36mTrue[0m,labels=goal_dict[param].keys()) [34m#orange line is median, green dotted line is mean[0m[52;194H[K[52;194H171,1[9C35%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.xlabel([36mstr[0m(param).upper(), fontsize=[31m10[0m, fontweight=[31m'bold'[0m)[52;194H[K[52;194H172,1[9C35%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.ylabel([31m'R^2'[0m, fontsize=[31m10[0m,fontweight=[31m'bold'[0m)[52;194H[K[52;194H173,1[9C36%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.title([31m'R^2 Score vs %s'[0m % param, fontsize=[31m14[0m, fontweight=[31m'bold'[0m)[52;194H[K[52;194H174,1[9C36%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17H[33mif[0m param == [31m'initialization'[0m:[52;194H[K[52;194H175,1[9C36%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;25Hplt.xticks(fontsize=[31m6[0m)[52;194H[K[52;194H176,1[9C36%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.subplot([31m122[0m)[52;194H[K[52;194H177,1[9C37%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.boxplot(time_dict[param].values(), bootstrap=[36mNone[0m,showmeans=[36mFalse[0m, meanline=[36mFalse[0m, notch=[36mFalse[0m,labels=time_dict[param].keys())[52;194H[K[52;194H178,1[9C37%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.xlabel([36mstr[0m(param).upper(), fontsize=[31m10[0m, fontweight=[31m'bold'[0m)[52;194H[K[52;194H179,1[9C37%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.ylabel([31m'Training Time'[0m, fontsize=[31m10[0m,fontweight=[31m'bold'[0m)[52;194H[K[52;194H180,1[9C38%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.title([31m'Training Time vs %s'[0m % param, fontsize=[31m14[0m, fontweight=[31m'bold'[0m)[52;194H[K[52;194H181,1[9C38%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.tight_layout(pad=[31m4[0m)[52;194H[K[52;194H182,1[9C38%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17H[33mif[0m param == [31m'initialization'[0m:[52;194H[K[52;194H183,1[9C39%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;25Hplt.xticks(fontsize=[31m6[0m)[52;194H[K[52;194H184,1[9C39%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hmy_fig_name = [31m"plots_of_"[0m + [36mstr[0m(param) + [31m'_'[0m + [36mstr[0m([31m"{:%Y_%m_%d}"[0m.format(datetime.datetime.now())) + [31m'_'[0m +[36mstr[0m(snps) +[36mstr[0m(num)+ [31m".png"[0m[52;194H[K[52;194H185,1[9C39%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.savefig(my_fig_name, dpi=[31m300[0m)[52;194H[K[52;194H186,1[9C39%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.show()[52;194H[K[52;194H187,1[9C40%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.clf()[52;194H[K[52;194H188,1[9C40%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.close()[52;194H[K[52;194H189,1[9C40%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H190,1-8[7C41%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mmake_goal_dict[0m(whole_dict):[52;194H[K[52;194H191,1[9C41%[51;1H[34h[?25h[?25l[1;51r[1;1H[2M[1;52r[50;9H[36mprint[0m(whole_dict)[51;9Hgoal_dict = {key:{} [33mfor[0m key [33min[0m whole_dict}[52;194H[K[52;194H192,1-8[7C41%[50;8H[34h[?25h[?25l[52;196H3[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mfor[0m key [33min[0m whole_dict:[52;194H[K[52;194H194,1-8[7C42%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17H[33mfor[0m item [33min[0m whole_dict[key]:[52;194H[K[52;194H195,1-8[7C42%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;25Hgoal_dict[key][item] = [][52;194H[K[52;194H196,1-8[7C42%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Htime_dict = {key:{} [33mfor[0m key [33min[0m whole_dict} [34m#both empty[0m[52;194H[K[52;194H197,1-8[7C42%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mfor[0m key [33min[0m whole_dict:[52;194H[K[52;194H198,1-8[7C43%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17H[33mfor[0m item [33min[0m whole_dict[key]:[52;194H[K[52;194H199,1[9C43%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;25Htime_dict[key][item] = [][52;194H[K[52;194H200,1[9C43%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mreturn[0m goal_dict, time_dict[52;194H[K[52;194H201,1-8[7C44%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H202,0-1[7C44%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hx_train, y_train = load_data(data)[52;194H[K[52;194H203,1[9C44%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hname_list = np.loadtxt(data, skiprows=[31m1[0m, usecols=([31m0[0m,), dtype=[31m'str'[0m)[52;194H[K[52;194H204,1[9C45%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H205,0-1[7C45%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hscaler = preprocessing.StandardScaler().fit(y_train)[52;194H[K[52;194H206,1[9C45%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#pickle.dump(scaler, open('scaler.pkl', 'wb'))[0m[52;194H[K[52;194H207,1[9C45%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#scaler = pickle.load(open('scaler.pkl', 'rb'))[0m[52;194H[K[52;194H208,1[9C46%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H209,0-1[7C46%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hy_train = scaler.transform(y_train)[52;194H[K[52;194H210,1[9C46%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H211,0-1[7C47%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hn_snps = x_train.shape[[31m1[0m][52;194H[K[52;194H212,1[9C47%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hmy_cv = sklearn.model_selection.KFold(n_splits=[31m10[0m, shuffle=[36mTrue[0m, random_state=[31m42[0m)[52;194H[K[52;194H213,1[9C47%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#################################################SVM####SVM#####SVM####################################################################[0m[52;194H[K[52;194H214,1[9C47%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mncv_results[0m(analysis, ncv_object):[52;194H[K[52;194H215,1[9C48%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Best Params of %s is %s "[0m % (analysis, ncv_object.best_params))[52;194H[K[52;194H216,1[9C48%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Outer scores of %s is %s and mean is %s"[0m % (analysis, ncv_object.outer_scores, np.mean(ncv_object.outer_scores)))[52;194H[K[52;194H217,1[9C48%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Variance of %s is %s "[0m % (analysis, ncv_object.variance))[52;194H[K[52;194H218,1[9C49%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Goal dict of %s is %s "[0m % (analysis, ncv_object.goal_dict))[52;194H[K[52;194H219,1[9C49%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hmake_param_box_plot(ncv_object.goal_dict, ncv_object.time_dict)[52;194H[K[52;194H220,1[9C49%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mwith[0m [36mopen[0m([31m'NCV_'[0m + [36mstr[0m(analysis) + [31m'_'[0m +  [36mstr[0m(snps) + [31m'_'[0m + [36mstr[0m(phenotype) + [31m'_'[0m + [36mstr[0m(num) + [31m'.pkl'[0m, [31m'wb'[0m) [33mas[0m ncvfile: [34m#with open("fname.pkl", 'rb') as ncvfile:[0m[52;194H[K[52;194H221,1[9C50%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hpickle.dump(ncv_object, ncvfile) [34m#ncv_object = pickle.load(ncvfile)[0m[52;194H[K[52;194H222,1[9C50%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H223,0-1[7C50%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mnn_results[0m(analysis, ncv_object):[52;194H[K[52;194H224,1[9C50%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Best Params of %s is %s "[0m % (analysis, ncv_object.best_params))[52;194H[K[52;194H225,1[9C51%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Outer scores of %s is %s and mean is %s"[0m % (analysis, ncv_object.outer_scores, np.mean(ncv_object.outer_scores)))[52;194H[K[52;194H226,1[9C51%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Variance of %s is %s "[0m % (analysis, ncv_object.variance))[52;194H[K[52;194H227,1[9C51%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hnn_list = [ncv_object.best_inner_params_list, ncv_object.best_inner_score_list, ncv_object.best_params, ncv_object.metric, ncv_object.outer_scores, ncv_object.variance][52;194H[K[52;194H228,1[9C52%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mwith[0m [36mopen[0m([31m'NCV_'[0m + [36mstr[0m(analysis) + [31m'_'[0m +  [36mstr[0m(snps) + [31m'_'[0m + [36mstr[0m(phenotype) + [31m'_'[0m + [36mstr[0m(num) + [31m'.pkl'[0m, [31m'wb'[0m) [33mas[0m ncvfile:[52;194H[K[52;194H229,1[9C52%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hpickle.dump(nn_list, ncvfile) [34m#ncv_object = pickle.load(ncvfile)[0m[52;194H[K[52;194H230,1[9C52%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hncv_object.model.model.save([31m"model_"[0m + [36mstr[0m(analysis) + [31m'_'[0m +  [36mstr[0m(snps) + [31m'_'[0m + [36mstr[0m(phenotype) + [31m'_'[0m + [36mstr[0m(num) + [31m".h5"[0m)[52;194H[K[52;194H231,1[9C52%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H232,0-1[7C53%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H233,0-1[7C53%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[36mprint[0m([31m"Performing SVM"[0m)[52;194H[K[52;194H234,1[9C53%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hc_param = [[31m1[0m,[31m2[0m, [31m10[0m, [31m100[0m][52;194H[K[52;194H235,1[9C54%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hgamma_param = [[36mfloat[0m(x) [33mfor[0m x [33min[0m np.linspace([31m0.1[0m, [31m1[0m, [31m4[0m)][52;194H[K[52;194H236,1[9C54%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H237,0-1[7C54%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H238,0-1[7C55%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hepsilon_param = [[36mfloat[0m(x) [33mfor[0m x [33min[0m np.linspace([31m0.1[0m, [31m1[0m, [31m4[0m)][52;194H[K[52;194H239,1[9C55%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hloss_param = [[31m'epsilon_insensitive'[0m, [31m'squared_epsilon_insensitive'[0m][52;194H[K[52;194H240,1[9C55%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hkernel_param = [[31m'poly'[0m][52;194H[K[52;194H241,1[9C55%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hdegree = [[31m1[0m,[31m2[0m,[31m3[0m][52;194H[K[52;194H242,1[9C56%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hsvm_random_grid = {[31m'gamma'[0m:gamma_param, [31m'C'[0m:c_param,[31m'kernel'[0m:kernel_param, [31m"degree"[0m:degree}[52;194H[K[52;194H243,1[9C56%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[36mprint[0m(svm_random_grid)[52;194H[K[52;194H244,1[9C56%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hsvm_random_grid2 = {[31m'C'[0m : c_param, [31m'loss'[0m:loss_param}[52;194H[K[52;194H245,1[9C57%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[36mprint[0m(svm_random_grid2)[52;194H[K[52;194H246,1[9C57%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hrbg_goal_dict, rbg_time_dict = make_goal_dict(svm_random_grid)[52;194H[K[52;194H247,1[9C57%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hsvm_goal_dict, svm_time_dict = make_goal_dict(svm_random_grid2)[52;194H[K[52;194H248,1[9C57%[51;1H[34h[?25h[?25l[1;51r[1;1H[2M[1;52r[50;1HSVM_NCV = NestedCV(model_name=[31m'LinearSVR'[0m, name_list = name_list, model=LinearSVR(), goal_dict=svm_goal_dict, time_dict=svm_time_dict, params_grid=svm_random_grid2, outer_kfolds=[31m2[0m, inner_kfolds=[31m2[0m, n_jobs = [31m8[0m,cv__[51;1Hoptions={[31m'randomized_search'[0m:[36mTrue[0m, [31m'randomized_search_iter'[0m:[31m50[0m, [31m'sqrt_of_score'[0m:[36mFalse[0m,[31m'recursive_feature_elimination'[0m:[36mFalse[0m, [31m'metric'[0m:sklearn.metrics.r2_score, [31m'metric_score_indicator_lower'[0m:[36mFalse[0m})[52;194H[K[52;194H249,1[9C58%[50;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1HSVM_NCV.fit(x_train, y_train.ravel(), name_list=name_list, phenfile=phenfile, set_size=set_size, snps=snps, model_name=[31m'SVM'[0m, goal_dict=svm_goal_dict, time_dict=svm_time_dict)[52;194H[K[52;194H250,1[9C58%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#make_param_box_plot(goal_dict, time_dict)[0m[52;194H[K[52;194H251,1[9C58%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H252,0-1[7C59%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hncv_results([31m'SVM'[0m, SVM_NCV)[52;194H[K[52;194H253,1[9C59%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31m'''[0m[52;194H[K[52;194H254,1[9C59%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mprint("Performing RBG")[0m[52;194H[K[52;194H255,1[9C60%[51;1H[34h[?25h[?25l[1;51r[1;1H[2M[1;52r[50;1H[31mRBG_NCV = NestedCV(model_name='RBG', name_list=name_list, model=SVR(), params_grid=svm_random_grid, outer_kfolds=4, inner_kfolds=4, n_jobs = 8,cv_options={'randomized_search':True, 'randomized_search_iter':50, ''[51;1Hsqrt_of_score':False,'recursive_feature_elimination':False, 'metric':sklearn.metrics.r2_score, 'metric_score_indicator_lower':False})[0m[52;194H[K[52;194H256,1[9C60%[50;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mRBG_NCV.fit(x_train, y_train.ravel(), name_list=name_list, phenfile=phenfile, set_size=set_size, snps=snps, model_name='RBG')[0m[52;194H[K[52;194H257,1[9C60%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mncv_results('RBG', RBG_NCV)[0m[52;194H[K[52;194H258,1[9C61%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H259,0-1[7C61%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mprint("Performing LASSO")[0m[52;194H[K[52;194H260,1[9C61%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31malpha = [0.0001, 0.001, 0.01, 0.1, 0.5, 1, 10, 100][0m[52;194H[K[52;194H261,1[9C61%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31malpha_dict = {'alpha':alpha}[0m[52;194H[K[52;194H262,1[9C62%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mprint(alpha_dict)[0m[52;194H[K[52;194H263,1[9C62%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31malpha_name_dict = {'alpha':"Alpha"}[0m[52;194H[K[52;194H264,1[9C62%[51;1H[34h[?25h[?25l[1;51r[1;1H[2M[1;52r[50;1H[31mLASS_NCV = NestedCV(model_name='LASS', name_list=name_list, model=Lasso(), params_grid=alpha_dict, outer_kfolds=4, inner_kfolds=4, n_jobs = 8,cv_options={'randomized_search':True, 'randomized_search_iter':50, 'ss[51;1Hqrt_of_score':False,'recursive_feature_elimination':False, 'metric':sklearn.metrics.r2_score, 'metric_score_indicator_lower':False})[0m[52;194H[K[52;194H265,1[9C63%[50;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mLASS_NCV.fit(x_train, y_train.ravel(), name_list=name_list, phenfile=phenfile, set_size=set_size, snps=snps, model_name='LASS')[0m[52;194H[K[52;194H266,1[9C63%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mncv_results('LASS', LASS_NCV)[0m[52;194H[K[52;194H267,1[9C63%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[31mprint("Performing Ridge")[0m[52;194H[K[52;194H268,1[9C64%[51;1H[34h[?25h[?25l[52;196H7[50;1H[34h[?25h[?25l[52;196H6[49;1H[34h[?25h[?25l[52;196H5[47;1H[34h[?25h[?25l[52;196H4[46;1H[34h[?25h[?25l[52;196H3[45;1H[34h[?25h[?25l[52;196H2[44;1H[34h[?25h[?25l[52;196H1[43;1H[34h[?25h[?25l[52;196H0[42;1H[34h[?25h[?25l[52;195H59,0-1[41;1H[34h[?25h[?25l[52;196H8,1  [40;1H[34h[?25h[?25l[52;196H7[39;1H[34h[?25h[?25l[52;196H6[37;1H[34h[?25h[?25l[52;196H5[36;1H[34h[?25h[?25l[52;196H4[35;1H[34h[?25h[?25l[52;196H3[34;1H[34h[?25h[?25l[52;196H2,0-1[33;1H[34h[?25h[?25l[52;196H1,1  [32;1H[34h[?25h[?25l[52;196H0[31;1H[34h[?25h[?25l[52;195H49[29;1H[34h[?25h[?25l[52;196H8[28;1H[34h[?25h[?25l[52;196H7[27;1H[34h[?25h[?25l[52;196H6[26;1H[34h[?25h[?25l[52;196H5[25;1H[34h[?25h[?25l[52;196H4[24;1H[34h[?25h[?25l[52;196H3[23;1H[34h[?25h[?25l[52;196H2[22;1H[34h[?25h[?25l[52;196H1[21;1H[34h[?25h[?25l[52;196H0[20;1H[34h[?25h[?25l[52;195H39[19;1H[34h[?25h[?25l[52;196H8,0-1[18;1H[34h[?25h[?25l[52;196H7[17;1H[34h[?25h[?25l[52;196H6,1  [16;1H[34h[?25h[?25l[52;196H5[15;1H[34h[?25h[?25l[52;198H2[15;2H[34h[?25h[?25l[52;198H3[15;3H[34h[?25h[?25l[52;198H4[15;4H[34h[?25h[?25l[52;198H5[15;5H[34h[?25h[?25l[52;198H6[15;6H[34h[?25h[?25l[52;198H7[15;7H[34h[?25h[?25l[52;198H8[15;8H[34h[?25h[?25l[52;198H9[15;9H[34h[?25h[?25l[52;198H10[15;10H[34h[?25h[?25l [46m[[12C][0m[52;199H1[15;11H[34h[?25h[?25l[[12C][52;199H2[15;12H[34h[?25h[?25l[52;199H3[15;13H[34h[?25h[?25l[52;199H4[15;14H[34h[?25h[?25l[52;199H5[15;15H[34h[?25h[?25l[52;199H6[15;16H[34h[?25h[?25l[52;199H7[15;17H[34h[?25h[?25l[52;199H8[15;18H[34h[?25h[?25l[52;199H9[15;19H[34h[?25h[?25l[52;198H20[15;20H[34h[?25h[?25l[52;199H1[15;21H[34h[?25h[?25l[52;199H2[15;22H[34h[?25h[?25l[52;199H3[15;23H[34h[?25h[?25l[15;11H[46m[[12C][0m[52;199H4[15;24H[34h[?25h[?25l[52;1H[1m-- INSERT --[0m[52;194H[K[52;194H235,24[8C64%[15;24H[34h[?25h[?25l[46m,[0m],[46m][0m[52;199H5[15;25H[34h[?25h[?25l[46m [0m] [46m][0m[52;199H6[15;26H[34h[?25h[?25l[31m[46m0[0m][31m0[0m[46m][0m[52;199H7[15;27H[34h[?25h[?25l[31m[46m.[0m][31m.[0m[46m][0m[52;199H8[15;28H[34h[?25h[?25l[31m[46m1[0m][31m1[0m[46m][0m[52;199H9[15;29H[34h[?25h[?25l[46m,[0m],[46m][0m[52;198H30[15;30H[34h[?25h[?25l[46m [0m] [46m][0m[52;199H1[15;31H[34h[?25h[?25l[31m[46m0[0m][31m0[0m[46m][0m[52;199H2[15;32H[34h[?25h[?25l[31m[46m.[0m][31m.[0m[46m][0m[52;199H3[15;33H[34h[?25h[?25l[31m[46m0[0m][31m0[0m[46m][0m[52;199H4[15;34H[34h[?25h[?25l[31m[46m0[0m][31m0[0m[46m][0m[52;199H5[15;35H[34h[?25h[?25l[31m[46m1[0m][31m1[0m[46m][0m[52;199H6[15;36H[34h[?25h[?25l[46m,[0m],[46m][0m[52;199H7[15;37H[34h[?25h[?25l[46m [0m] [46m][0m[52;199H8[15;38H[34h[?25h[?25l[31m[46m0[0m][31m0[0m[46m][0m[52;199H9[15;39H[34h[?25h[?25l[31m[46m.[0m][31m.[0m[46m][0m[52;198H40[15;40H[34h[?25h[?25l[31m[46m0[0m][31m0[0m[46m][0m[52;199H1[15;41H[34h[?25h[?25l[31m[46m0[0m][31m0[0m[46m][0m[52;199H2[15;42H[34h[?25h[?25l[31m[46m0[0m][31m0[0m[46m][0m[52;199H3[15;43H[34h[?25h[?25l[31m[46m1[0m][31m1[0m[46m][0m[52;199H4[15;44H[34h[?25h[?25l[46m,[0m],[46m][0m[52;199H5[15;45H[34h[?25h[?25l[46m [0m] [46m][0m[52;199H6[15;46H[34h[?25h[?25l[31m[46m1[0m][31m1[0m[46m][0m[52;199H7[15;47H[34h[?25h[?25l[31m[46m0[0m][31m0[0m[46m][0m[52;199H8[15;48H[34h[?25h[?25l[31m[46m0[0m][31m0[0m[46m][0m[52;199H9[15;49H[34h[?25h[?25l[31m[46m0[0m][31m0[0m[46m][0m[52;198H50[15;50H[34h[?25h[?25l[15;11H[[38C][52;196H6[16;50H[34h[?25h[?25l[52;196H7,1 [17;1H[34h[?25h[?25l[52;196H8[18;1H[34h[?25h[?25l[52;196H9,50[19;50H[34h[?25h[?25l[52;195H40[20;50H[34h[?25h[?25l[21;16H[46m[[6C][0m[52;196H1,24[21;24H[34h[?25h[?25l[21;16H[[6C][22;10H[46m[[5C][0m[52;196H2,17[22;17H[34h[?25h[?25l[22;10H[[5C][52;196H3,50[23;50H[34h[?25h[?25l[24;6H[46m([15C)[0m[52;196H4,23[24;23H[34h[?25h[?25l[24;6H([15C)[52;196H5,50[25;50H[34h[?25h[?25l[26;6H[46m([16C)[0m[52;196H6,24[26;24H[34h[?25h[?25l[26;6H([16C)[52;196H7,50[27;50H[34h[?25h[?25l[52;196H8[28;50H[34h[?25h[?25l[52;196H9[29;50H[34h[?25h[?25l[52;195H50[31;50H[34h[?25h[?25l[52;195H49[29;50H[34h[?25h[?25l[52;199H1[29;51H[34h[?25h[?25l[52;199H2[29;52H[34h[?25h[?25l[52;199H3[29;53H[34h[?25h[?25l[52;199H4[29;54H[34h[?25h[?25l[52;199H5[29;55H[34h[?25h[?25l[52;199H6[29;56H[34h[?25h[?25l[52;199H7[29;57H[34h[?25h[?25l[52;199H8[29;58H[34h[?25h[?25l[52;199H9[29;59H[34h[?25h[?25l[52;198H60[29;60H[34h[?25h[?25l[52;199H1[29;61H[34h[?25h[?25l[52;199H2[29;62H[34h[?25h[?25l[52;199H3[29;63H[34h[?25h[?25l[52;199H4[29;64H[34h[?25h[?25l[52;199H5[29;65H[34h[?25h[?25l[52;199H6[29;66H[34h[?25h[?25l[52;199H7[29;67H[34h[?25h[?25l[52;199H8[29;68H[34h[?25h[?25l[52;199H9[29;69H[34h[?25h[?25l[52;198H70[29;70H[34h[?25h[?25l[52;199H1[29;71H[34h[?25h[?25l[52;199H2[29;72H[34h[?25h[?25l[52;199H3[29;73H[34h[?25h[?25l[52;199H4[29;74H[34h[?25h[?25l[52;199H5[29;75H[34h[?25h[?25l[52;199H6[29;76H[34h[?25h[?25l[52;199H7[29;77H[34h[?25h[?25l[52;199H8[29;78H[34h[?25h[?25l[52;199H9[29;79H[34h[?25h[?25l[52;198H80[29;80H[34h[?25h[?25l[52;199H1[29;81H[34h[?25h[?25lR[46m()[0m[127C__[30;1Ho[52;199H2[29;82H[34h[?25h[?25l[129C__[30;1Ho[52;199H3[29;83H[34h[?25h[?25l[128C__[30;1Ho[52;199H4[29;84H[34h[?25h[?25l()[127C__[30;1Ho[52;199H5[29;85H[34h[?25h[?25l[52;199H6[29;86H[34h[?25h[?25l[52;199H7[29;87H[34h[?25h[?25l[52;199H8[29;88H[34h[?25h[?25l[52;199H9[29;89H[34h[?25h[?25l[52;198H90[29;90H[34h[?25h[?25l[52;199H1[29;91H[34h[?25h[?25l[52;199H2[29;92H[34h[?25h[?25l[52;199H3[29;93H[34h[?25h[?25l[52;199H4[29;94H[34h[?25h[?25l[52;199H5[29;95H[34h[?25h[?25l[52;199H6[29;96H[34h[?25h[?25l[52;199H7[29;97H[34h[?25h[?25l[52;199H8[29;98H[34h[?25h[?25l[52;199H9[29;99H[34h[?25h[?25l[52;198H100[29;100H[34h[?25h[?25l[52;200H1[29;101H[34h[?25h[?25l[52;200H2[29;102H[34h[?25h[?25l[52;200H3[29;103H[34h[?25h[?25l[52;200H4[29;104H[34h[?25h[?25l[52;200H5[29;105H[34h[?25h[?25l[52;200H6[29;106H[34h[?25h[?25l[52;200H7[29;107H[34h[?25h[?25l[52;200H8[29;108H[34h[?25h[?25l[52;200H9[29;109H[34h[?25h[?25l[52;199H10[29;110H[34h[?25h[?25l[52;200H1[29;111H[34h[?25h[?25l[52;200H2[29;112H[34h[?25h[?25l[52;200H3[29;113H[34h[?25h[?25l[52;200H4[29;114H[34h[?25h[?25l[52;200H5[29;115H[34h[?25h[?25l[52;200H6[29;116H[34h[?25h[?25l[52;200H7[29;117H[34h[?25h[?25l[52;200H8[29;118H[34h[?25h[?25l[52;200H9[29;119H[34h[?25h[?25l[52;199H20[29;120H[34h[?25h[?25l[52;200H1[29;121H[34h[?25h[?25l[52;200H2[29;122H[34h[?25h[?25l[52;200H3[29;123H[34h[?25h[?25l[52;200H4[29;124H[34h[?25h[?25l[52;200H5[29;125H[34h[?25h[?25l[52;200H6[29;126H[34h[?25h[?25l[52;200H7[29;127H[34h[?25h[?25l[52;200H8[29;128H[34h[?25h[?25l[52;200H9[29;129H[34h[?25h[?25l[52;199H30[29;130H[34h[?25h[?25l[52;200H1[29;131H[34h[?25h[?25l[52;200H2[29;132H[34h[?25h[?25l[52;200H3[29;133H[34h[?25h[?25l[52;200H4[29;134H[34h[?25h[?25l[52;200H5[29;135H[34h[?25h[?25l[52;200H6[29;136H[34h[?25h[?25l[52;200H7[29;137H[34h[?25h[?25l[52;200H8[29;138H[34h[?25h[?25l[52;200H9[29;139H[34h[?25h[?25l[52;199H40[29;140H[34h[?25h[?25l[52;200H1[29;141H[34h[?25h[?25l[52;200H2[29;142H[34h[?25h[?25l[52;200H3[29;143H[34h[?25h[?25l[52;200H4[29;144H[34h[?25h[?25l[52;200H5[29;145H[34h[?25h[?25l[52;200H6[29;146H[34h[?25h[?25l[52;200H7[29;147H[34h[?25h[?25l[52;200H8[29;148H[34h[?25h[?25l[52;200H9[29;149H[34h[?25h[?25l[52;199H50[29;150H[34h[?25h[?25l[52;200H1[29;151H[34h[?25h[?25l[52;200H2[29;152H[34h[?25h[?25l[52;200H3[29;153H[34h[?25h[?25l[52;200H4[29;154H[34h[?25h[?25l[52;200H5[29;155H[34h[?25h[?25l[52;200H6[29;156H[34h[?25h[?25l[52;200H7[29;157H[34h[?25h[?25l[52;200H8[29;158H[34h[?25h[?25l[52;200H9[29;159H[34h[?25h[?25l[52;199H60[29;160H[34h[?25h[?25l[52;200H1[29;161H[34h[?25h[?25l[52;200H2[29;162H[34h[?25h[?25l[52;200H3[29;163H[34h[?25h[?25l[52;200H4[29;164H[34h[?25h[?25l[52;200H5[29;165H[34h[?25h[?25l[52;200H6[29;166H[34h[?25h[?25l[52;200H7[29;167H[34h[?25h[?25l[52;200H8[29;168H[34h[?25h[?25l[52;200H9[29;169H[34h[?25h[?25l[52;199H70[29;170H[34h[?25h[?25l[52;200H1[29;171H[34h[?25h[?25l[52;200H2[29;172H[34h[?25h[?25l[52;200H3[29;173H[34h[?25h[?25l[52;200H4[29;174H[34h[?25h[?25l[52;200H5[29;175H[34h[?25h[?25l[52;200H6[29;176H[34h[?25h[?25l[52;200H7[29;177H[34h[?25h[?25l[52;200H8[29;178H[34h[?25h[?25l[52;200H9[29;179H[34h[?25h[?25l[52;199H80[29;180H[34h[?25h[?25l[52;200H1[29;181H[34h[?25h[?25l[52;200H2[29;182H[34h[?25h[?25l[52;200H3[29;183H[34h[?25h[?25l[52;200H4[29;184H[34h[?25h[?25l[52;200H5[29;185H[34h[?25h[?25l[52;200H6[29;186H[34h[?25h[?25l[52;200H7[29;187H[34h[?25h[?25l[52;200H8[29;188H[34h[?25h[?25l[52;200H9[29;189H[34h[?25h[?25l[52;199H90[29;190H[34h[?25h[?25l[52;200H1[29;191H[34h[?25h[?25l[52;200H2[29;192H[34h[?25h[?25l[52;200H3[29;193H[34h[?25h[?25l[52;200H4[29;194H[34h[?25h[?25l[52;200H5[29;195H[34h[?25h[?25l[52;200H6[29;196H[34h[?25h[?25l[52;200H7[29;197H[34h[?25h[?25l[52;200H8[29;198H[34h[?25h[?25l[52;200H9[29;199H[34h[?25h[?25l[52;198H200[29;200H[34h[?25h[?25l[52;200H1[29;201H[34h[?25h[?25l[52;200H2[29;202H[34h[?25h[?25l[52;200H3[29;203H[34h[?25h[?25l[52;200H4[29;204H[34h[?25h[?25l[52;200H5[29;205H[34h[?25h[?25l[52;200H6[29;206H[34h[?25h[?25l[52;200H7[29;207H[34h[?25h[?25l[52;200H8[29;208H[34h[?25h[?25l[52;200H9[29;209H[34h[?25h[?25l[52;199H10[29;210H[34h[?25h[?25l[52;200H1[29;211H[34h[?25h[?25l[52;200H2[30;1H[34h[?25h[?25l[52;200H3[30;2H[34h[?25h[?25l[52;200H4[30;3H[34h[?25h[?25l[52;200H5[30;4H[34h[?25h[?25l[52;200H6[30;5H[34h[?25h[?25l[52;200H7[30;6H[34h[?25h[?25l[52;200H8[30;7H[34h[?25h[?25l[52;200H9[30;8H[34h[?25h[?25l=[46m{[187C}[0m[52;199H20[30;9H[34h[?25h[?25l[52;200H1[30;10H[34h[?25h[?25l{[187C}[52;200H2[30;11H[34h[?25h[?25l[52;200H3[30;12H[34h[?25h[?25l[52;200H4[30;13H[34h[?25h[?25l[52;200H5[30;14H[34h[?25h[?25l[52;200H6[30;15H[34h[?25h[?25l[52;200H7[30;16H[34h[?25h[?25l[52;200H8[30;17H[34h[?25h[?25l[52;200H9[30;18H[34h[?25h[?25l[52;199H30[30;19H[34h[?25h[?25l[52;200H1[30;20H[34h[?25h[?25l[52;200H2[30;21H[34h[?25h[?25l[52;200H3[30;22H[34h[?25h[?25l[52;200H4[30;23H[34h[?25h[?25l[52;200H5[30;24H[34h[?25h[?25l[52;200H6[30;25H[34h[?25h[?25l[52;200H7[30;26H[34h[?25h[?25l[52;200H8[30;27H[34h[?25h[?25l[52;200H9[30;28H[34h[?25h[?25l[52;199H40[30;29H[34h[?25h[?25l[52;200H1[30;30H[34h[?25h[?25l[52;200H2[30;31H[34h[?25h[?25l[52;200H3[30;32H[34h[?25h[?25l[52;200H4[30;33H[34h[?25h[?25l[52;200H5[30;34H[34h[?25h[?25l[52;200H6[30;35H[34h[?25h[?25l[52;200H7[30;36H[34h[?25h[?25l[52;200H8[30;37H[34h[?25h[?25l[52;200H9[30;38H[34h[?25h[?25l[52;199H50[30;39H[34h[?25h[?25l[52;200H1[30;40H[34h[?25h[?25l[52;200H2[30;41H[34h[?25h[?25l[52;200H3[30;42H[34h[?25h[?25l[52;200H4[30;43H[34h[?25h[?25l[52;200H5[30;44H[34h[?25h[?25l[52;200H6[30;45H[34h[?25h[?25l[52;200H7[30;46H[34h[?25h[?25l[52;200H8[30;47H[34h[?25h[?25l[52;200H9[30;48H[34h[?25h[?25l[52;199H60[30;49H[34h[?25h[?25l[52;200H1[30;50H[34h[?25h[?25l[52;200H2[30;51H[34h[?25h[?25l[52;200H3[30;52H[34h[?25h[?25l[52;200H4[30;53H[34h[?25h[?25l[52;200H5[30;54H[34h[?25h[?25l[52;200H6[30;55H[34h[?25h[?25l[52;200H7[30;56H[34h[?25h[?25l[52;200H8[30;57H[34h[?25h[?25l[52;200H9[30;58H[34h[?25h[?25l[52;199H70[30;59H[34h[?25h[?25l[52;200H1[30;60H[34h[?25h[?25l[52;200H2[30;61H[34h[?25h[?25l[52;200H3[30;62H[34h[?25h[?25l[52;200H2[30;61H[34h[?25h[?25li50, [31m'sqrt_of_score'[0m:[36mFalse[0m,[31m'recursive_feature_elimination'[0m:[36mFalse[0m, [31m'metric'[0m:sklearn.metrics.r2_score, [31m'metric_score_indicator_lower'[0m:[36mFalse[0m})[52;200H3[30;62H[34h[?25h[?25l[31m50[0m, [31m'sqrt_of_score'[0m:[36mFalse[0m,[31m'recursive_feature_elimination'[0m:[36mFalse[0m, [31m'metric'[0m:sklearn.metrics.r2_score, [31m'metric_score_indicator_lower'[0m:[36mFalse[0m})[30;199H[K[52;200H2[30;61H[34h[?25h[?25l[31m150[0m, [31m'sqrt_of_score'[0m:[36mFalse[0m,[31m'recursive_feature_elimination'[0m:[36mFalse[0m, [31m'metric'[0m:sklearn.metrics.r2_score, [31m'metric_score_indicator_lower'[0m:[36mFalse[0m})[52;200H3[30;62H[34h[?25h[52;1H[K[30;61H[?25l[52;194H249,272[7C64%[30;61H[34h[?25h[?25l[52;194H[K[52;1H:[34h[?25hx[?25l[34h[?25h[?25l"cv_grid_all_ml.py" 391L, 23181C written
[?1l>[34h[?25h[?1049l(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ vi cv_grid_all_ml.pypython cv_grid_all_ml.py 31 /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del /home/ciaran/completed_big_matrix_binary_new_snps_ids__first_10k.raw shuf FT10cv_new_10k 10000M[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C#chnage vack c level[K
[KM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cpython cv_grid_all_ml.py 31 /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del /home/ciaran/completed_big_matrix_binary_new_snps_ids__first_10k.raw shuf FT10cv_new_10k 10000
Please remember to set the right set size in the nested_cv code
/external_storage/ciaran/machine_learning2
WARNING THIS IS AN EDITED SCRIPT - Ciaran Kelly 2021 
 Edited with permission under liscence 
 Flex version
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
31
/home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del
/home/ciaran/completed_big_matrix_binary_new_snps_ids_first_10k.raw
shuf
FT10cv_new_10k
10000
2021-12-19 23:34:00
Performing SVM
{'gamma': [0.1, 0.4, 0.7, 1.0], 'degree': [1, 2, 3], 'kernel': ['poly'], 'C': [1, 2, 10, 100, 0.1, 0.001, 0.0001, 1000]}
{'loss': ['epsilon_insensitive', 'squared_epsilon_insensitive'], 'C': [1, 2, 10, 100, 0.1, 0.001, 0.0001, 1000]}
{'gamma': [0.1, 0.4, 0.7, 1.0], 'degree': [1, 2, 3], 'kernel': ['poly'], 'C': [1, 2, 10, 100, 0.1, 0.001, 0.0001, 1000]}
{'loss': ['epsilon_insensitive', 'squared_epsilon_insensitive'], 'C': [1, 2, 10, 100, 0.1, 0.001, 0.0001, 1000]}
ADBUCE
INNER COUNT NO.  1
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_1_in_1_in.raw
Set size set to 10000
Set size set to 10000
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 16 is smaller than n_iter=150. Running 16 iterations. For exhaustive searches, use GridSearchCV.
  % (grid_size, self.n_iter, grid_size), UserWarning)
{'loss': 'epsilon_insensitive', 'C': 1}
Blue
Red
0.18789199423874958
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 1}
Blue
Red
0.19039947328298668
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 2}
Blue
Red
0.18789481357402948
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 2}
Blue
Red
0.1891887618229413
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 10}
Blue
Red
0.18790340308175968
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 10}
Blue
Red
0.18816555505259003
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 100}
Blue
Red
0.18788685712668696
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 100}
Blue
Red
0.18791323113539615
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 0.1}
Blue
Red
0.18789572369321328
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 0.1}
Blue
Red
0.20948494128297057
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 0.001}
Blue
Red
0.34272808627211016
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 0.001}
Blue
Red
0.3340695589114048
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 0.0001}
Blue
Red
0.04908425620531909
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 0.0001}
Blue
Red
0.11983415866896152
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 1000}
Blue
Red
0.1879012684060023
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 1000}
Blue
Red
0.18785576872336807
Typer LinearSVR
INNER COUNT NO.  2
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_1_in_2_in.raw
Set size set to 10000
Set size set to 10000
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 16 is smaller than n_iter=150. Running 16 iterations. For exhaustive searches, use GridSearchCV.
  % (grid_size, self.n_iter, grid_size), UserWarning)
{'loss': 'epsilon_insensitive', 'C': 1}
Blue
Red
0.4000765839129262
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 1}
Blue
Red
0.40223121550334906
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 2}
Blue
Red
0.40006970931621266
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 2}
Blue
Red
0.40119118324457903
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 10}
Blue
Red
0.40007153019646413
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 10}
Blue
Red
0.40029340159935345
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 100}
Blue
Red
0.40007410721044756
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 100}
Blue
Red
0.40010395906891405
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 0.1}
Blue
Red
0.4000702578126394
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 0.1}
Blue
Red
0.4167936098413537
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 0.001}
Blue
Red
0.3628460757357632
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 0.001}
Blue
Red
0.40143150145965323
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 0.0001}
Blue
Red
0.08438061473570546
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 0.0001}
Blue
Red
0.15309068938633486
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 1000}
Blue
Red
0.4000663518529023
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 1000}
Blue
Red
0.40006675138661896
Typer LinearSVR
OUTER COUNT NO.  1
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_1_in_2_out.raw
Set size set to 10000
Set size set to 10000
0.40628378053075886
INNER COUNT NO.  1
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_2_in_1_in.raw
Set size set to 10000
Set size set to 10000
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 16 is smaller than n_iter=150. Running 16 iterations. For exhaustive searches, use GridSearchCV.
  % (grid_size, self.n_iter, grid_size), UserWarning)
{'loss': 'epsilon_insensitive', 'C': 1}
Blue
Red
0.40100928296949845
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 1}
Blue
Red
0.4027920121120476
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 2}
Blue
Red
0.40101185943689854
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 2}
Blue
Red
0.40190910402067237
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 10}
Blue
Red
0.40100973530306283
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 10}
Blue
Red
0.401182496062946
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 100}
Blue
Red
0.4010014810204129
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 100}
Blue
Red
0.40101822641463847
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 0.1}
Blue
Red
0.40100786947342737
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 0.1}
Blue
Red
0.4147600850637577
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 0.001}
Blue
Red
0.3306005040706921
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 0.001}
Blue
Red
0.38104436819361365
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 0.0001}
Blue
Red
0.10756509990164675
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 0.0001}
Blue
Red
0.15607592685162297
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 1000}
Blue
Red
0.401008438138055
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 1000}
Blue
Red
0.40101656563278776
Typer LinearSVR
INNER COUNT NO.  2
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_2_in_2_in.raw
Set size set to 10000
Set size set to 10000
/home/ciaran/venv/local/lib/python3.5/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 16 is smaller than n_iter=150. Running 16 iterations. For exhaustive searches, use GridSearchCV.
  % (grid_size, self.n_iter, grid_size), UserWarning)
{'loss': 'epsilon_insensitive', 'C': 1}
Blue
Red
0.42084273633679325
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 1}
Blue
Red
0.4229017770929343
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 2}
Blue
Red
0.42083857858230667
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 2}
Blue
Red
0.42189717508964597
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 10}
Blue
Red
0.42083384609464725
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 10}
Blue
Red
0.4210636794869417
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 100}
Blue
Red
0.42084134457327727
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 100}
Blue
Red
0.420868382978809
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 0.1}
Blue
Red
0.42083574233415333
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 0.1}
Blue
Red
0.43585531074585215
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 0.001}
Blue
Red
0.3579365922201795
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 0.001}
Blue
Red
0.40058749612372824
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 0.0001}
Blue
Red
0.09061850640833535
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 0.0001}
Blue
Red
0.150230349464066
Typer LinearSVR
{'loss': 'epsilon_insensitive', 'C': 1000}
Blue
Red
0.4208423228186118
Typer LinearSVR
{'loss': 'squared_epsilon_insensitive', 'C': 1000}
Blue
Red
0.42084329083778627
Typer LinearSVR
OUTER COUNT NO.  2
/external_storage/ciaran/arabadopsis/FT10cv_new_10k/shuf
test_raw_plink_shuf_2_in_2_out.raw
Set size set to 10000
Set size set to 10000
0.36133518037324863
Best Params of SVM is {'loss': ['squared_epsilon_insensitive'], 'C': [0.1]} 
Outer scores of SVM is [0.40628378053075886, 0.36133518037324863] and mean is 0.38380948045200375
Variance of SVM is [0.5145179709277082, 0.3981409336532966] 
Goal dict of SVM is {'loss': {'epsilon_insensitive': [0.18789199423874958, 0.18789481357402948, 0.18790340308175968, 0.18788685712668696, 0.18789572369321328, 0.34272808627211016, 0.04908425620531909, 0.1879012684060023, 0.4000765839129262, 0.40006970931621266, 0.40007153019646413, 0.40007410721044756, 0.4000702578126394, 0.3628460757357632, 0.08438061473570546, 0.4000663518529023, 0.40100928296949845, 0.40101185943689854, 0.40100973530306283, 0.4010014810204129, 0.40100786947342737, 0.3306005040706921, 0.10756509990164675, 0.401008438138055, 0.42084273633679325, 0.42083857858230667, 0.42083384609464725, 0.42084134457327727, 0.42083574233415333, 0.3579365922201795, 0.09061850640833535, 0.4208423228186118], 'squared_epsilon_insensitive': [0.19039947328298668, 0.1891887618229413, 0.18816555505259003, 0.18791323113539615, 0.20948494128297057, 0.3340695589114048, 0.11983415866896152, 0.18785576872336807, 0.40223121550334906, 0.40119118324457903, 0.40029340159935345, 0.40010395906891405, 0.4167936098413537, 0.40143150145965323, 0.15309068938633486, 0.40006675138661896, 0.4027920121120476, 0.40190910402067237, 0.401182496062946, 0.40101822641463847, 0.4147600850637577, 0.38104436819361365, 0.15607592685162297, 0.40101656563278776, 0.4229017770929343, 0.42189717508964597, 0.4210636794869417, 0.420868382978809, 0.43585531074585215, 0.40058749612372824, 0.150230349464066, 0.42084329083778627]}, 'C': {0.1: [0.18789572369321328, 0.20948494128297057, 0.4000702578126394, 0.4167936098413537, 0.40100786947342737, 0.4147600850637577, 0.42083574233415333, 0.43585531074585215], 1: [0.18789199423874958, 0.19039947328298668, 0.4000765839129262, 0.40223121550334906, 0.40100928296949845, 0.4027920121120476, 0.42084273633679325, 0.4229017770929343], 2: [0.18789481357402948, 0.1891887618229413, 0.40006970931621266, 0.40119118324457903, 0.40101185943689854, 0.40190910402067237, 0.42083857858230667, 0.42189717508964597], 100: [0.18788685712668696, 0.18791323113539615, 0.40007410721044756, 0.40010395906891405, 0.4010014810204129, 0.40101822641463847, 0.42084134457327727, 0.420868382978809], 1000: [0.1879012684060023, 0.18785576872336807, 0.4000663518529023, 0.40006675138661896, 0.401008438138055, 0.40101656563278776, 0.4208423228186118, 0.42084329083778627], 0.0001: [0.04908425620531909, 0.11983415866896152, 0.08438061473570546, 0.15309068938633486, 0.10756509990164675, 0.15607592685162297, 0.09061850640833535, 0.150230349464066], 10: [0.18790340308175968, 0.18816555505259003, 0.40007153019646413, 0.40029340159935345, 0.40100973530306283, 0.401182496062946, 0.42083384609464725, 0.4210636794869417], 0.001: [0.34272808627211016, 0.3340695589114048, 0.3628460757357632, 0.40143150145965323, 0.3306005040706921, 0.38104436819361365, 0.3579365922201795, 0.40058749612372824]}} 
Performing Neural Network
Traceback (most recent call last):
  File "cv_grid_all_ml.py", line 343, in <module>
    NN_NCV = NestedCV(model_name='nn_model', name_list = name_list, model=nn_model, params_grid=param_grid, outer_kfolds=4, inner_kfolds=4, n_jobs = 32,cv_options={'randomized_search':True, 'randomized_search_iter':100, 'sqrt_of_score':False,'recursive_feature_elimination':False, 'metric':sklearn.metrics.r2_score, 'metric_score_indicator_lower':False})
TypeError: __init__() missing 2 required positional arguments: 'goal_dict' and 'time_dict'
(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ ###chnage c 
(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ ###chnage c python cv_grid_all_ml.py 31 /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del /home/ciaran/completed_big_matrix_binary_new_snps_ids__first_10k.raw shuf FT10cv_new_10k 10000M[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cvi cv_grid_all_ml.py[K
[KM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cpython cv_grid_all_ml.py 31 /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del /home/ciaran/completed_big_matrix_binary_new_snps_ids__first_10k.raw shuf FT10cv_new_10k 10000M[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C#chnage vack c level[K
[KM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cvi cv_grid_all_ml.pygit commit -m "boxplotting seems to be working"add . [Kpython cv_grid_all_ml.py 31 /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del /home/ciaran/completed_big_matrix_binary_new_snps_ids__first_10k.raw shuf FT10cv_new_10k 10000M[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cvi cv_grid_all_ml.py[K
[KM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cpython cv_grid_all_ml.py 31 /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del /home/ciaran/completed_big_matrix_binary_new_snps_ids__first_10k.raw shuf FT10cv_new_10k 10000M[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cvi cv_grid_all_ml.py[K
[KM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C~/j[Kpython cv_grid_all_ml.py 31 /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del /home/ciaran/completed_big_matrix_binary_new_snps_ids__first_10k.raw shuf FT10cv_new_10k 10000M[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cgit add . [K
[KM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cvi cv_grid_all_ml.pynested_cv_new_name.py~/j[Kvi nested_cv_new_name.py[4Pcv_grid_all_ml.py[10Pgit add . python cv_grid_all_ml.py 31 /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del /home/ciaran/completed_big_matrix_binary_new_snps_ids__first_10k.raw shuf FT10cv_new_10k 10000M[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C~/j[K
[KM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cvi cv_grid_all_ml.pypython cv_grid_all_ml.py 31 /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del /home/ciaran/completed_big_matrix_binary_new_snps_ids__first_10k.raw shuf FT10cv_new_10k 10000M[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cvi cv_grid_all_ml.py[K
[KM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cpython cv_grid_all_ml.py 31 /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del /home/ciaran/completed_big_matrix_binary_new_snps_ids__first_10k.raw shuf FT10cv_new_10k 10000M[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cgit add . [K
[KM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Ccommit -m "boxplotting seems to be working"[27Pvi cv_grid_all_ml.py#chnage vack c levelpython cv_grid_all_ml.py 31 /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del /home/ciaran/completed_big_matrix_binary_new_snps_ids__first_10k.raw shuf FT10cv_new_10k 10000M[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cvi cv_grid_all_ml.py[K
[KM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cpython cv_grid_all_ml.py 31 /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del /home/ciaran/completed_big_matrix_binary_new_snps_ids__first_10k.raw shuf FT10cv_new_10k 10000M[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C###chnage c [K
[KM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[K###chnage c python cv_grid_all_ml.py 31 /home/ciaran/arabadopsis/phenotypes/values_FT10.8424.dup.del /home/ciaran/completed_big_matrix_binary_new_snps_ids__first_10k.raw shuf FT10cv_new_10k 10000M[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cvi cv_grid_all_ml.py[K
[KM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C
[?1049h[?1h=[1;52r[34l[34h[?25h[23m[24m[0m[H[J[?25l[52;1H"cv_grid_all_ml.py" 391L, 23181C[1;1H[34m#Warning : best model selected by NMAE and R2 might not be the same
#performs linear regression, linear regression, neural network, svm and random forest, LASSO, RIDGE, CNN
#source ~/venv/bin/activate #in python 3.5.2
#print a log to a .txt file!
#model = pickle.load(open('FILEPATH', 'rb')) 
#dependencies = {'coeff_determination':coeff_determination}
#model = tf.keras.models.load_model('FILEPATH', custom_objects=dependencies)
#

#import tensorflow
#import numpy as np; import scipy #need to do this before path insert
#import sys
#sys.path.insert(1, '/external_storage/ciaran/Library/Python/3.7/python/site-packages/')
#import dill as pickle
#sys.path.insert(1, '/external_storage/ciaran/Library/Python/3.7/python/site-packages/nested_cv')
#from nested_cv import NestedCV
#with open('NCV_NN.pkl', 'rb') as f:
#     red = pickle.load(f)[0m

[36mprint[0m([31m"Please remember to set the right set size in the nested_cv code"[0m)
[35mimport[0m sys
sys.path.insert([31m1[0m, [31m'/external_storage/ciaran/Library/Python/3.7/python/site-packages/nested_cv'[0m)
num = sys.argv[[31m1[0m] [34m#script number for saving out[0m
phenfile = [36mstr[0m(sys.argv[[31m2[0m]) [34m#txt file with phenotypes[0m
data = [36mstr[0m(sys.argv[[31m3[0m]) [34m#needs to be same size as set_size[0m
snps = [36mstr[0m(sys.argv[[31m4[0m]) [34m#top or shuf[0m
phenotype = [36mstr[0m(sys.argv[[31m5[0m]) [34m#make a directory for the results[0m
set_size = [36mint[0m(sys.argv[[31m6[0m]) [34m#how many SNPs[0m
[35mfrom[0m nested_cv [35mimport[0m NestedCV
[35mimport[0m statistics
[35mimport[0m numpy [33mas[0m np
[35mimport[0m sklearn
[35mimport[0m seaborn [33mas[0m sns
[35mfrom[0m sklearn [35mimport[0m preprocessing
[35mfrom[0m sklearn.pipeline [35mimport[0m make_pipeline, Pipeline
[35mfrom[0m sklearn.preprocessing [35mimport[0m StandardScaler
[35mimport[0m datetime
[35mimport[0m time
[35mfrom[0m statistics [35mimport[0m mean
[35mimport[0m matplotlib.pyplot [33mas[0m plt
[35mfrom[0m matplotlib [35mimport[0m style
[35mfrom[0m sklearn.svm [35mimport[0m LinearSVR
[35mfrom[0m sklearn.svm [35mimport[0m SVR
[35mfrom[0m sklearn.model_selection [35mimport[0m train_test_split
[35mfrom[0m sklearn.model_selection [35mimport[0m RandomizedSearchCV
[35mfrom[0m sklearn.linear_model [35mimport[0m LinearRegression, Lasso, Ridge
[34m#import pickle #use dill instead below[0m
[35mfrom[0m sklearn.ensemble [35mimport[0m RandomForestRegressor [34m#based in part on https://towardsdatascience.com/random-forest-in-python-24d0893d51c0[0m
[35mfrom[0m statistics [35mimport[0m mean
[35mimport[0m os
[35mimport[0m pandas [33mas[0m pd [34m# data processing, CSV file I/O (e.g. pd.read_csv)[0m[52;194H1,1[11CTop[1;1H[34h[?25h[?25l[52;194H2[2;1H[34h[?25h[?25l[52;194H3[3;1H[34h[?25h[?25l[52;194H4[4;1H[34h[?25h[?25l[52;194H5[5;1H[34h[?25h[?25l[52;194H6[6;1H[34h[?25h[?25l[52;194H7[7;1H[34h[?25h[?25l[52;194H8[8;1H[34h[?25h[?25l[52;194H9,0-1[9;1H[34h[?25h[?25l[52;194H10,1 [10;1H[34h[?25h[?25l[52;195H1[11;1H[34h[?25h[?25l[52;195H2[12;1H[34h[?25h[?25l[52;195H3[13;1H[34h[?25h[?25l[52;195H4[14;1H[34h[?25h[?25l[52;195H5[15;1H[34h[?25h[?25l[52;195H6[16;1H[34h[?25h[?25l[52;195H7[17;1H[34h[?25h[?25l[52;195H8[18;1H[34h[?25h[?25l[52;195H9,0-1[19;1H[34h[?25h[?25l[52;194H20,1  [20;1H[34h[?25h[?25l[52;195H1[21;1H[34h[?25h[?25l[52;195H2[22;1H[34h[?25h[?25l[52;195H3[23;1H[34h[?25h[?25l[52;195H4[24;1H[34h[?25h[?25l[52;195H5[25;1H[34h[?25h[?25l[52;195H6[26;1H[34h[?25h[?25l[52;195H7[27;1H[34h[?25h[?25l[52;195H8[28;1H[34h[?25h[?25l[52;195H9[29;1H[34h[?25h[?25l[52;194H30[30;1H[34h[?25h[?25l[52;195H1[31;1H[34h[?25h[?25l[52;195H2[32;1H[34h[?25h[?25l[52;195H3[33;1H[34h[?25h[?25l[52;195H4[34;1H[34h[?25h[?25l[52;195H5[35;1H[34h[?25h[?25l[52;195H6[36;1H[34h[?25h[?25l[52;195H7[37;1H[34h[?25h[?25l[52;195H8[38;1H[34h[?25h[?25l[52;195H9[39;1H[34h[?25h[?25l[52;194H40[40;1H[34h[?25h[?25l[52;195H1[41;1H[34h[?25h[?25l[52;195H2[42;1H[34h[?25h[?25l[52;195H3[43;1H[34h[?25h[?25l[52;195H4[44;1H[34h[?25h[?25l[52;195H5[45;1H[34h[?25h[?25l[52;195H6[46;1H[34h[?25h[?25l[52;195H7[47;1H[34h[?25h[?25l[52;195H8[48;1H[34h[?25h[?25l[52;195H9[49;1H[34h[?25h[?25l[52;194H50[50;1H[34h[?25h[?25l[52;195H1[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m sklearn.metrics [35mimport[0m make_scorer[52;1H[K[52;194H52,1[11C0%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mimport[0m tensorflow [33mas[0m tf[52;194H[K[52;194H53,1[11C0%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.python.keras [35mimport[0m backend [33mas[0m K[52;194H[K[52;194H54,1[11C0%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.layers [35mimport[0m Dense, Dropout, Flatten [34m#.core[0m[52;194H[K[52;194H55,1[11C1%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.optimizers [35mimport[0m SGD[52;194H[K[52;194H56,1[11C1%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.layers [35mimport[0m Dropout[52;194H[K[52;194H57,1[11C1%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m sklearn.externals [35mimport[0m joblib[52;194H[K[52;194H58,1[11C2%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.models [35mimport[0m Sequential, Model[52;194H[K[52;194H59,1[11C2%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.layers [35mimport[0m Dense[52;194H[K[52;194H60,1[11C2%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.wrappers.scikit_learn [35mimport[0m KerasRegressor [34m# or Classifier[0m[52;194H[K[52;194H61,1[11C2%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m sklearn.model_selection [35mimport[0m KFold[52;194H[K[52;194H62,1[11C3%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.optimizers [35mimport[0m SGD[52;194H[K[52;194H63,1[11C3%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.keras.layers [35mimport[0m Dropout[52;194H[K[52;194H64,1[11C3%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mimport[0m random[52;194H[K[52;194H65,1[11C4%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb[0m[52;194H[K[52;194H66,1[11C4%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorboard.plugins.hparams [35mimport[0m api [33mas[0m hp[52;194H[K[52;194H67,1[11C4%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mimport[0m random[52;194H[K[52;194H68,1[11C5%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.python.keras.layers [35mimport[0m deserialize, serialize[52;194H[K[52;194H69,1[11C5%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mfrom[0m tensorflow.python.keras.saving [35mimport[0m saving_utils[52;194H[K[52;194H70,1[11C5%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H71,0-1[9C5%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H72,0-1[9C6%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H73,0-1[9C6%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#if snps == 'shuf' :[0m[52;194H[K[52;194H74,1[11C6%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#       print("Shuf nestedCV in usage")[0m[52;194H[K[52;194H75,1[11C7%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#       from nested_cv_shuf import NestedCV[0m[52;194H[K[52;194H76,1[11C7%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#elif snps == 'top':[0m[52;194H[K[52;194H77,1[11C7%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#       from nested_cv import NestedCV[0m[52;194H[K[52;194H78,1[11C7%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#else:[0m[52;194H[K[52;194H79,1[11C8%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#       print("snnps must be top or shuf")[0m[52;194H[K[52;194H80,1[11C8%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H81,1-8[9C8%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H82,0-1[9C9%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hsys.path.insert([31m1[0m, [31m'/external_storage/ciaran/Library/Python/3.7/python/site-packages/'[0m)[52;194H[K[52;194H83,1[11C9%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[35mimport[0m dill [33mas[0m pickle[52;194H[K[52;194H84,1[11C9%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mfor[0m i [33min[0m [36mrange[0m([31m1[0m,[36mlen[0m(sys.argv)):[52;194H[K[52;194H85,1[10C10%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m(sys.argv[i])[52;194H[K[52;194H86,1-8[8C10%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H87,0-1[8C10%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mif[0m [33mnot[0m os.path.exists([31m'/external_storage/ciaran/arabadopsis/'[0m + phenotype+ [31m'/'[0m + snps):[52;194H[K[52;194H88,1[10C10%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5Hos.makedirs([31m'/external_storage/ciaran/arabadopsis/'[0m + phenotype+ [31m'/'[0m + snps)[52;194H[K[52;194H89,1[10C11%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H90,0-1[8C11%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hos.chdir([31m'/external_storage/ciaran/arabadopsis/'[0m + phenotype+ [31m'/'[0m + snps)[52;194H[K[52;194H91,1[10C11%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hdate_object = datetime.datetime.now().replace(second=[31m0[0m,microsecond=[31m0[0m)[52;194H[K[52;194H92,1[10C12%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[36mprint[0m(date_object)[52;194H[K[52;194H93,1[10C12%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H94,0-1[8C12%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mcoeff_determination[0m(y_true, y_pred):[52;194H[K[52;194H95,1[10C12%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9HSS_res = K.sum(K.square( y_true-y_pred ))[52;194H[K[52;194H96,1[10C13%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9HSS_tot = K.sum(K.square( y_true - K.mean(y_true)))[52;194H[K[52;194H97,1[10C13%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mreturn[0m ([31m1[0m-SS_res/SS_tot)[52;194H[K[52;194H98,1[10C13%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H99,0-1[8C14%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H100,0-1[7C14%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mload_data[0m(data):[52;194H[K[52;194H101,1[9C14%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hdataset = np.loadtxt(data, skiprows=[31m1[0m)[52;194H[K[52;194H102,1[9C15%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hx = dataset[: , [31m6[0m:set_size+[31m6[0m]/[31m2[0m[52;194H[K[52;194H103,1[9C15%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hy = dataset[: , [31m5[0m ][52;194H[K[52;194H104,1[9C15%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hy = y.reshape(-[31m1[0m,[31m1[0m)[52;194H[K[52;194H105,1[9C15%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[34m#print("Performing split of raw data....")[0m[52;194H[K[52;194H106,1[9C16%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[34m#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.8, random_state=42)[0m[52;194H[K[52;194H107,1[9C16%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mreturn[0m x, y [34m#x_train, y_train, x_test, y_test[0m[52;194H[K[52;194H108,1[9C16%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H109,0-1[7C17%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H110,0-1[7C17%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H111,0-1[7C17%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mwith[0m [36mopen[0m(([31m'validation_results_'[0m+ [36mstr[0m(snps) +[36mstr[0m(num) + phenotype + [36mstr[0m([31m"{:%Y_%m_%d}"[0m.format(datetime.datetime.now())) + [31m'.vallog'[0m ), [31m'a'[0m) [33mas[0m f:[52;194H[K[52;194H112,1[9C17%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Horiginal_stdout = sys.stdout [34m# Save a reference to the original standard output[0m[52;194H[K[52;194H113,1[9C18%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hsys.stdout = f [34m# Change the standard output to the file we created.[0m[52;194H[K[52;194H114,1[9C18%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m(datetime.datetime.now())[52;194H[K[52;194H115,1[9C18%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hsys.stdout = original_stdout[52;194H[K[52;194H116,1[9C19%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H117,0-1[7C19%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mbaseline[0m(x, y):[52;194H[K[52;194H118,1[9C19%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hmodel = LinearRegression()[52;194H[K[52;194H119,1[9C20%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hmodel.fit(x, y)[52;194H[K[52;194H120,1[9C20%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mreturn[0m model[52;194H[K[52;194H121,1[9C20%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H122,0-1[7C20%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H123,0-1[7C21%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mavg_cv_result[0m(measure,cv_result):[52;194H[K[52;194H124,1[9C21%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hmy_var_name = [k [33mfor[0m k,v [33min[0m [36mlocals[0m().items() [33mif[0m v == measure][[31m0[0m] [34m#just to print out the name[0m[52;194H[K[52;194H125,1-8[7C21%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m(my_var_name)[52;194H[K[52;194H126,1-8[7C22%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hn_combos = [36mlen[0m(cv_result.cv_results_[[31m'split0_test_neg_mean_absolute_error'[0m])[52;194H[K[52;194H127,1-8[7C22%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hnamed_dict = {} [34m#dictonary will have a list of results PER grid combination across all CV results and this will return the average result. [0m[52;194H[K[52;194H128,1-8[7C22%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Havg_list = [][52;194H[K[52;194H129,1-8[7C22%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mfor[0m combo [33min[0m [36mrange[0m([31m0[0m, n_combos):[52;194H[K[52;194H130,1-8[7C23%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hnamed_dict[[36mstr[0m(combo)] = [][52;194H[K[52;194H131,1-8[7C23%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17H[33mfor[0m split [33min[0m measure:[52;194H[K[52;194H132,1-8[7C23%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;25Hnamed_dict[[36mstr[0m(combo)].append(cv_result.cv_results_[split][combo])[52;194H[K[52;194H133,1-8[7C24%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Havg_list.append(statistics.mean(named_dict[[36mstr[0m(combo)]))[52;194H[K[52;194H134,1-8[7C24%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17H[36mprint[0m(combo, statistics.mean(named_dict[[36mstr[0m(combo)]))[52;194H[K[52;194H135,1-8[7C24%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H136,0-1[7C25%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m'Max'[0m, np.nanmax(avg_list), np.where(avg_list == np.nanmax(avg_list)))[52;194H[K[52;194H137,1-8[7C25%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m'Min'[0m, np.nanmin(avg_list), np.where(avg_list == np.nanmin(avg_list)))[52;194H[K[52;194H138,1-8[7C25%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mreturn[0m avg_list[52;194H[K[52;194H139,1-8[7C25%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H140,0-1[7C26%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H141,0-1[7C26%[51;1H[34h[?25h[?25l[1;51r[1;1H[2M[1;52r[50;1H[33mdef[0m [36munpack[0m(model, training_config, weights): [34m##https://github.com/tensorflow/tensorflow/issues/34697 #fixes an error that the early stopping callback throws up in the nested cv #something about the parralele fitt[51;1Hting step needing everything to be pickle-able and the callback isnt [0m[52;194H[K[52;194H142,1[9C26%[50;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5Hrestored_model = deserialize(model)[52;194H[K[52;194H143,1[9C27%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5H[33mif[0m training_config [33mis[0m [33mnot[0m [36mNone[0m:[52;194H[K[52;194H144,1[9C27%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hrestored_model.compile([52;194H[K[52;194H145,1[9C27%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;13H**saving_utils.compile_args_from_training_config([52;194H[K[52;194H146,1[9C28%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Htraining_config[52;194H[K[52;194H147,1[9C28%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;13H)[52;194H[K[52;194H148,1[9C28%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H)[52;194H[K[52;194H149,1[9C29%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5Hrestored_model.set_weights(weights)[52;194H[K[52;194H150,1[9C29%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5H[33mreturn[0m restored_model[52;194H[K[52;194H151,1[9C29%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H152,0-1[7C29%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m# Hotfix function[0m[52;194H[K[52;194H153,1[9C30%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mmake_keras_picklable[0m():[52;194H[K[52;194H154,1[9C30%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H155,0-1[7C30%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5H[33mdef[0m [36m__reduce__[0m(self):[52;194H[K[52;194H156,1[9C31%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hmodel_metadata = saving_utils.model_metadata(self)[52;194H[K[52;194H157,1[9C31%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Htraining_config = model_metadata.get([31m"training_config"[0m, [36mNone[0m)[52;194H[K[52;194H158,1[9C31%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hmodel = serialize(self)[52;194H[K[52;194H159,1[9C31%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hweights = self.get_weights()[52;194H[K[52;194H160,1[9C32%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mreturn[0m (unpack, (model, training_config, weights))[52;194H[K[52;194H161,1[9C32%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H162,0-1[7C32%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5Hcls = Model[52;194H[K[52;194H163,1[9C33%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;5Hcls.__reduce__ = __reduce__[52;194H[K[52;194H164,1[9C33%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H165,0-1[7C33%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H166,0-1[7C34%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mmake_param_box_plot[0m(goal_dict, time_dict):[52;194H[K[52;194H167,1[9C34%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mfor[0m param [33min[0m goal_dict:[52;194H[K[52;194H168,1-8[7C34%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.subplots([31m1[0m,[31m2[0m,figsize=([31m12[0m,[31m8[0m))[52;194H[K[52;194H169,1[9C34%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.subplot([31m121[0m)[52;194H[K[52;194H170,1[9C35%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.boxplot(goal_dict[param].values(), bootstrap=[36mNone[0m,showmeans=[36mFalse[0m, meanline=[36mFalse[0m, notch=[36mTrue[0m,labels=goal_dict[param].keys()) [34m#orange line is median, green dotted line is mean[0m[52;194H[K[52;194H171,1[9C35%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.xlabel([36mstr[0m(param).upper(), fontsize=[31m10[0m, fontweight=[31m'bold'[0m)[52;194H[K[52;194H172,1[9C35%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.ylabel([31m'R^2'[0m, fontsize=[31m10[0m,fontweight=[31m'bold'[0m)[52;194H[K[52;194H173,1[9C36%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.title([31m'R^2 Score vs %s'[0m % param, fontsize=[31m14[0m, fontweight=[31m'bold'[0m)[52;194H[K[52;194H174,1[9C36%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17H[33mif[0m param == [31m'initialization'[0m:[52;194H[K[52;194H175,1[9C36%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;25Hplt.xticks(fontsize=[31m6[0m)[52;194H[K[52;194H176,1[9C36%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.subplot([31m122[0m)[52;194H[K[52;194H177,1[9C37%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.boxplot(time_dict[param].values(), bootstrap=[36mNone[0m,showmeans=[36mFalse[0m, meanline=[36mFalse[0m, notch=[36mFalse[0m,labels=time_dict[param].keys())[52;194H[K[52;194H178,1[9C37%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.xlabel([36mstr[0m(param).upper(), fontsize=[31m10[0m, fontweight=[31m'bold'[0m)[52;194H[K[52;194H179,1[9C37%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.ylabel([31m'Training Time'[0m, fontsize=[31m10[0m,fontweight=[31m'bold'[0m)[52;194H[K[52;194H180,1[9C38%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.title([31m'Training Time vs %s'[0m % param, fontsize=[31m14[0m, fontweight=[31m'bold'[0m)[52;194H[K[52;194H181,1[9C38%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.tight_layout(pad=[31m4[0m)[52;194H[K[52;194H182,1[9C38%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17H[33mif[0m param == [31m'initialization'[0m:[52;194H[K[52;194H183,1[9C39%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;25Hplt.xticks(fontsize=[31m6[0m)[52;194H[K[52;194H184,1[9C39%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hmy_fig_name = [31m"plots_of_"[0m + [36mstr[0m(param) + [31m'_'[0m + [36mstr[0m([31m"{:%Y_%m_%d}"[0m.format(datetime.datetime.now())) + [31m'_'[0m +[36mstr[0m(snps) +[36mstr[0m(num)+ [31m".png"[0m[52;194H[K[52;194H185,1[9C39%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.savefig(my_fig_name, dpi=[31m300[0m)[52;194H[K[52;194H186,1[9C39%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.show()[52;194H[K[52;194H187,1[9C40%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.clf()[52;194H[K[52;194H188,1[9C40%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hplt.close()[52;194H[K[52;194H189,1[9C40%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H190,1-8[7C41%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mmake_goal_dict[0m(whole_dict):[52;194H[K[52;194H191,1[9C41%[51;1H[34h[?25h[?25l[1;51r[1;1H[2M[1;52r[50;9H[36mprint[0m(whole_dict)[51;9Hgoal_dict = {key:{} [33mfor[0m key [33min[0m whole_dict}[52;194H[K[52;194H192,1-8[7C41%[50;8H[34h[?25h[?25l[52;196H3[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mfor[0m key [33min[0m whole_dict:[52;194H[K[52;194H194,1-8[7C42%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17H[33mfor[0m item [33min[0m whole_dict[key]:[52;194H[K[52;194H195,1-8[7C42%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;25Hgoal_dict[key][item] = [][52;194H[K[52;194H196,1-8[7C42%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Htime_dict = {key:{} [33mfor[0m key [33min[0m whole_dict} [34m#both empty[0m[52;194H[K[52;194H197,1-8[7C42%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mfor[0m key [33min[0m whole_dict:[52;194H[K[52;194H198,1-8[7C43%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17H[33mfor[0m item [33min[0m whole_dict[key]:[52;194H[K[52;194H199,1[9C43%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;25Htime_dict[key][item] = [][52;194H[K[52;194H200,1[9C43%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mreturn[0m goal_dict, time_dict[52;194H[K[52;194H201,1-8[7C44%[51;8H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H202,0-1[7C44%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hx_train, y_train = load_data(data)[52;194H[K[52;194H203,1[9C44%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hname_list = np.loadtxt(data, skiprows=[31m1[0m, usecols=([31m0[0m,), dtype=[31m'str'[0m)[52;194H[K[52;194H204,1[9C45%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H205,0-1[7C45%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hscaler = preprocessing.StandardScaler().fit(y_train)[52;194H[K[52;194H206,1[9C45%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#pickle.dump(scaler, open('scaler.pkl', 'wb'))[0m[52;194H[K[52;194H207,1[9C45%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#scaler = pickle.load(open('scaler.pkl', 'rb'))[0m[52;194H[K[52;194H208,1[9C46%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H209,0-1[7C46%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hy_train = scaler.transform(y_train)[52;194H[K[52;194H210,1[9C46%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H211,0-1[7C47%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hn_snps = x_train.shape[[31m1[0m][52;194H[K[52;194H212,1[9C47%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hmy_cv = sklearn.model_selection.KFold(n_splits=[31m10[0m, shuffle=[36mTrue[0m, random_state=[31m42[0m)[52;194H[K[52;194H213,1[9C47%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#################################################SVM####SVM#####SVM####################################################################[0m[52;194H[K[52;194H214,1[9C47%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mncv_results[0m(analysis, ncv_object):[52;194H[K[52;194H215,1[9C48%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Best Params of %s is %s "[0m % (analysis, ncv_object.best_params))[52;194H[K[52;194H216,1[9C48%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Outer scores of %s is %s and mean is %s"[0m % (analysis, ncv_object.outer_scores, np.mean(ncv_object.outer_scores)))[52;194H[K[52;194H217,1[9C48%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Variance of %s is %s "[0m % (analysis, ncv_object.variance))[52;194H[K[52;194H218,1[9C49%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Goal dict of %s is %s "[0m % (analysis, ncv_object.goal_dict))[52;194H[K[52;194H219,1[9C49%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hmake_param_box_plot(ncv_object.goal_dict, ncv_object.time_dict)[52;194H[K[52;194H220,1[9C49%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mwith[0m [36mopen[0m([31m'NCV_'[0m + [36mstr[0m(analysis) + [31m'_'[0m +  [36mstr[0m(snps) + [31m'_'[0m + [36mstr[0m(phenotype) + [31m'_'[0m + [36mstr[0m(num) + [31m'.pkl'[0m, [31m'wb'[0m) [33mas[0m ncvfile: [34m#with open("fname.pkl", 'rb') as ncvfile:[0m[52;194H[K[52;194H221,1[9C50%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hpickle.dump(ncv_object, ncvfile) [34m#ncv_object = pickle.load(ncvfile)[0m[52;194H[K[52;194H222,1[9C50%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H223,0-1[7C50%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[33mdef[0m [36mnn_results[0m(analysis, ncv_object):[52;194H[K[52;194H224,1[9C50%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Best Params of %s is %s "[0m % (analysis, ncv_object.best_params))[52;194H[K[52;194H225,1[9C51%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Outer scores of %s is %s and mean is %s"[0m % (analysis, ncv_object.outer_scores, np.mean(ncv_object.outer_scores)))[52;194H[K[52;194H226,1[9C51%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[36mprint[0m([31m"Variance of %s is %s "[0m % (analysis, ncv_object.variance))[52;194H[K[52;194H227,1[9C51%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hnn_list = [ncv_object.best_inner_params_list, ncv_object.best_inner_score_list, ncv_object.best_params, ncv_object.metric, ncv_object.outer_scores, ncv_object.variance][52;194H[K[52;194H228,1[9C52%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9H[33mwith[0m [36mopen[0m([31m'NCV_'[0m + [36mstr[0m(analysis) + [31m'_'[0m +  [36mstr[0m(snps) + [31m'_'[0m + [36mstr[0m(phenotype) + [31m'_'[0m + [36mstr[0m(num) + [31m'.pkl'[0m, [31m'wb'[0m) [33mas[0m ncvfile:[52;194H[K[52;194H229,1[9C52%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;17Hpickle.dump(nn_list, ncvfile) [34m#ncv_object = pickle.load(ncvfile)[0m[52;194H[K[52;194H230,1[9C52%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;9Hncv_object.model.model.save([31m"model_"[0m + [36mstr[0m(analysis) + [31m'_'[0m +  [36mstr[0m(snps) + [31m'_'[0m + [36mstr[0m(phenotype) + [31m'_'[0m + [36mstr[0m(num) + [31m".h5"[0m)[52;194H[K[52;194H231,1[9C52%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H232,0-1[7C53%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H233,0-1[7C53%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[36mprint[0m([31m"Performing SVM"[0m)[52;194H[K[52;194H234,1[9C53%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hc_param = [[31m1[0m,[31m2[0m, [31m10[0m, [31m100[0m, [31m0.1[0m, [31m0.001[0m, [31m0.0001[0m, [31m1000[0m][52;194H[K[52;194H235,1[9C54%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hgamma_param = [[36mfloat[0m(x) [33mfor[0m x [33min[0m np.linspace([31m0.1[0m, [31m1[0m, [31m4[0m)][52;194H[K[52;194H236,1[9C54%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H237,0-1[7C54%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H238,0-1[7C55%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hepsilon_param = [[36mfloat[0m(x) [33mfor[0m x [33min[0m np.linspace([31m0.1[0m, [31m1[0m, [31m4[0m)][52;194H[K[52;194H239,1[9C55%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hloss_param = [[31m'epsilon_insensitive'[0m, [31m'squared_epsilon_insensitive'[0m][52;194H[K[52;194H240,1[9C55%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hkernel_param = [[31m'poly'[0m][52;194H[K[52;194H241,1[9C55%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hdegree = [[31m1[0m,[31m2[0m,[31m3[0m][52;194H[K[52;194H242,1[9C56%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hsvm_random_grid = {[31m'gamma'[0m:gamma_param, [31m'C'[0m:c_param,[31m'kernel'[0m:kernel_param, [31m"degree"[0m:degree}[52;194H[K[52;194H243,1[9C56%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[36mprint[0m(svm_random_grid)[52;194H[K[52;194H244,1[9C56%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hsvm_random_grid2 = {[31m'C'[0m : c_param, [31m'loss'[0m:loss_param}[52;194H[K[52;194H245,1[9C57%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[36mprint[0m(svm_random_grid2)[52;194H[K[52;194H246,1[9C57%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hrbg_goal_dict, rbg_time_dict = make_goal_dict(svm_random_grid)[52;194H[K[52;194H247,1[9C57%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1Hsvm_goal_dict, svm_time_dict = make_goal_dict(svm_random_grid2)[52;194H[K[52;194H248,1[9C57%[51;1H[34h[?25h[?25l[1;51r[1;1H[2M[1;52r[50;1HSVM_NCV = NestedCV(model_name=[31m'LinearSVR'[0m, name_list = name_list, model=LinearSVR(), goal_dict=svm_goal_dict, time_dict=svm_time_dict, params_grid=svm_random_grid2, outer_kfolds=[31m2[0m, inner_kfolds=[31m2[0m, n_jobs = [31m8[0m,cv__[51;1Hoptions={[31m'randomized_search'[0m:[36mTrue[0m, [31m'randomized_search_iter'[0m:[31m150[0m, [31m'sqrt_of_score'[0m:[36mFalse[0m,[31m'recursive_feature_elimination'[0m:[36mFalse[0m, [31m'metric'[0m:sklearn.metrics.r2_score, [31m'metric_score_indicator_lower'[0m:[36mFalse[0m})[52;194H[K[52;194H249,1[9C58%[50;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1HSVM_NCV.fit(x_train, y_train.ravel(), name_list=name_list, phenfile=phenfile, set_size=set_size, snps=snps, model_name=[31m'SVM'[0m, goal_dict=svm_goal_dict, time_dict=svm_time_dict)[52;194H[K[52;194H250,1[9C58%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[51;1H[34m#make_param_box_plot(goal_dict, time_dict)[0m[52;194H[K[52;194H251,1[9C58%[51;1H[34h[?25h[?25l[1;51r[51;1H
[1;52r[52;194H[K[52;194H252,0-1[7C59%[51;1H[34h[?25h[?25l[52;196H1,1  [50;1H[34h[?25h[?25l[52;196H0[49;1H[34h[?25h[?25l[52;195H49[47;1H[34h[?25h[?25l[52;196H8[46;1H[34h[?25h[?25l[52;196H7[45;1H[34h[?25h[?25l[52;196H6[44;1H[34h[?25h[?25l[52;196H5[43;1H[34h[?25h[?25l[52;196H4[42;1H[34h[?25h[?25l[52;196H3[41;1H[34h[?25h[?25l[52;196H2[40;1H[34h[?25h[?25l[52;196H1[39;1H[34h[?25h[?25l[52;196H0[38;1H[34h[?25h[?25l[52;195H39[37;1H[34h[?25h[?25l[52;196H8,0-1[36;1H[34h[?25h[?25l[52;196H7[35;1H[34h[?25h[?25l[52;196H6,1  [34;1H[34h[?25h[?25l[52;196H5[33;1H[34h[?25h[?25l[52;196H4[32;1H[34h[?25h[?25l[52;196H5[33;1H[34h[?25h[?25l[52;198H2[33;2H[34h[?25h[?25l[52;198H3[33;3H[34h[?25h[?25l[52;198H4[33;4H[34h[?25h[?25l[52;198H5[33;5H[34h[?25h[?25l[52;198H6[33;6H[34h[?25h[?25l[52;198H7[33;7H[34h[?25h[?25l[52;198H8[33;8H[34h[?25h[?25l[52;198H9[33;9H[34h[?25h[?25l[52;198H10[33;10H[34h[?25h[?25l [46m[[38C][0m[52;199H1[33;11H[34h[?25h[?25l[[38C][52;199H2[33;12H[34h[?25h[?25l[52;199H3[33;13H[34h[?25h[?25l[52;199H4[33;14H[34h[?25h[?25l[52;199H5[33;15H[34h[?25h[?25l[52;199H6[33;16H[34h[?25h[?25l[52;199H7[33;17H[34h[?25h[?25l[52;199H8[33;18H[34h[?25h[?25l[52;199H9[33;19H[34h[?25h[?25l[52;198H20[33;20H[34h[?25h[?25l[52;199H1[33;21H[34h[?25h[?25l[52;199H2[33;22H[34h[?25h[?25l[52;199H3[33;23H[34h[?25h[?25l[52;199H4[33;24H[34h[?25h[?25l[52;199H5[33;25H[34h[?25h[?25l[52;199H6[33;26H[34h[?25h[?25l[52;199H7[33;27H[34h[?25h[?25l[52;199H8[33;28H[34h[?25h[?25l[52;199H9[33;29H[34h[?25h[?25l[52;198H30[33;30H[34h[?25h[?25l[52;199H1[33;31H[34h[?25h[?25l[52;199H2[33;32H[34h[?25h[?25l[52;199H3[33;33H[34h[?25h[?25l[52;199H4[33;34H[34h[?25h[?25l[52;199H5[33;35H[34h[?25h[?25l[52;199H6[33;36H[34h[?25h[?25l[52;199H7[33;37H[34h[?25h[?25l[52;199H8[33;38H[34h[?25h[?25l[52;199H9[33;39H[34h[?25h[?25l[52;198H40[33;40H[34h[?25h[?25l[52;199H1[33;41H[34h[?25h[?25l[52;199H2[33;42H[34h[?25h[?25l[52;199H3[33;43H[34h[?25h[?25l[52;199H4[33;44H[34h[?25h[?25l[52;199H5[33;45H[34h[?25h[?25l[52;199H6[33;46H[34h[?25h[?25l[52;199H7[33;47H[34h[?25h[?25l[52;199H8[33;48H[34h[?25h[?25l[52;199H9[33;49H[34h[?25h[?25l[33;11H[46m[[38C][0m[52;198H50[33;50H[34h[?25h[?25l[52;1H[1m-- INSERT --[0m[52;194H[K[52;194H235,50[8C59%[33;50H[34h[?25h[?25l][46m ][0m[33;50H[K[52;198H49[33;49H[34h[?25h[?25l][46m ][0m[33;49H[K[52;199H8[33;48H[34h[?25h[?25l][46m ][0m[33;48H[K[52;199H7[33;47H[34h[?25h[?25l][46m ][0m[33;47H[K[52;199H6[33;46H[34h[?25h[?25l][46m ][0m[33;46H[K[52;199H5[33;45H[34h[?25h[?25l][46m ][0m[33;45H[K[52;199H4[33;44H[34h[?25h[?25l][46m ][0m[33;44H[K[52;199H3[33;43H[34h[?25h[?25l][46m ][0m[33;43H[K[52;199H2[33;42H[34h[?25h[?25l][46m ][0m[33;42H[K[52;199H1[33;41H[34h[?25h[?25l][46m ][0m[33;41H[K[52;199H0[33;40H[34h[?25h[?25l][46m ][0m[33;40H[K[52;198H39[33;39H[34h[?25h[?25l][46m ][0m[33;39H[K[52;199H8[33;38H[34h[?25h[?25l][46m ][0m[33;38H[K[52;199H7[33;37H[34h[?25h[?25l][46m ][0m[33;37H[K[52;199H6[33;36H[34h[?25h[?25l][46m ][0m[33;36H[K[52;199H5[33;35H[34h[?25h[?25l][46m ][0m[33;35H[K[52;199H4[33;34H[34h[?25h[?25l][46m ][0m[33;34H[K[52;199H3[33;33H[34h[?25h[?25l][46m ][0m[33;33H[K[52;199H2[33;32H[34h[?25h[?25l][46m ][0m[33;32H[K[52;199H1[33;31H[34h[?25h[?25l][46m ][0m[33;31H[K[52;199H0[33;30H[34h[?25h[?25l][46m ][0m[33;30H[K[52;198H29[33;29H[34h[?25h[?25l][46m ][0m[33;29H[K[52;199H8[33;28H[34h[?25h[?25l][46m ][0m[33;28H[K[52;199H7[33;27H[34h[?25h[?25l][46m ][0m[33;27H[K[52;199H6[33;26H[34h[?25h[?25l][46m ][0m[33;26H[K[52;199H5[33;25H[34h[?25h[?25l][46m ][0m[33;25H[K[52;199H4[33;24H[34h[?25h[?25l][46m ][0m[33;24H[K[52;199H3[33;23H[34h[?25h[?25l][46m ][0m[33;23H[K[52;199H2[33;22H[34h[?25h[?25l][46m ][0m[33;22H[K[52;199H1[33;21H[34h[?25h[?25l][46m ][0m[33;21H[K[52;199H0[33;20H[34h[?25h[?25l][46m ][0m[33;20H[K[52;198H19[33;19H[34h[?25h[?25l][46m ][0m[33;19H[K[52;199H8[33;18H[34h[?25h[?25l][46m ][0m[33;18H[K[52;199H7[33;17H[34h[?25h[?25l][46m ][0m[33;17H[K[52;199H6[33;16H[34h[?25h[?25l][46m ][0m[33;16H[K[52;199H5[33;15H[34h[?25h[?25l][46m ][0m[33;15H[K[52;199H4[33;14H[34h[?25h[?25l][46m ][0m[33;14H[K[52;199H3[33;13H[34h[?25h[?25l[46m,[0m],[46m][0m[52;199H4[33;14H[34h[?25h[?25l[31m[46m2[0m][31m2[0m[46m][0m[52;199H5[33;15H[34h[?25h[52;1H[K[33;14H[?25l[[3C][52;194H235,14[8C59%[33;14H[34h[?25h[?25l[52;194H[K[52;1H:[34h[?25hx[?25l[34h[?25h[?25l"cv_grid_all_ml.py" 391L, 23146C written
[?1l>[34h[?25h[?1049l(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ git add . 
(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ git commir[Kt -m "[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K###
(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ #32 
(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ ~[K#31
(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ grep '0.42084329083778627' [1@#
(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ script done
Script started, file is done
bash: alias: NF} $1 | sort -nu | tail -n 1 : not found
(base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ exit script
exit
bash: exit: script: numeric argument required
Script done, file is done
(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ exit script
exit
There are stopped jobs.
(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ #'loss': {'epsilon_insensitive': [0.18789199423874958, 0.18789481357402948, 0.18790340308175968, 0.18788685712668696, 0.18789572369321328, 0.3 4272808627211016, 0.04908425620531909, 0.1879012684060023, 0.4000765839129262, 0.40006970931621266, 0.40007153019646413, 0.40007410721044756, 0.4000702578126394, 0.3628460757357632, 0.08438061473570546, 0.400066 3518529023, 0.40100928296949845, 0.40101185943689854, 0.40100973530306283, 0.4010014810204129, 0.40100786947342737, 0.3306005040706921, 0.10756509990164675, 0.401008438138055, 0.42084273633679325, 0.420838578582 30667, 0.42083384609464725, 0.42084134457327727, 0.42083574233415333, 0.3579365922201795, 0.09061850640833535, 0.4208423228186118], 'squared_epsilon_insensitive': [0.19039947328298668, 0.1891887618229413, 0.1881 6555505259003, 0.18791323113539615, 0.20948494128297057, 0.3340695589114048, 0.11983415866896152, 0.18785576872336807, 0.40223121550334906, 0.40119118324457903, 0.40029340159935345, 0.40010395906891405, 0.416793 6098413537, 0.40143150145965323, 0.15309068938633486, 0.40006675138661896, 0.4027920121120476, 0.40190910402067237, 0.401182496062946, 0.40101822641463847, 0.4147600850637577, 0.38104436819361365, 0.156075926851 62297, 0.40101656563278776, 0.4229017770929343, 0.42189717508964597, 0.4210636794869417, 0.420868382978809, 0.43585531074585215, 0.40058749612372824, 0.150230349464066, 0.42084329083778627]},
(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ #0.40628378053075886,
(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ ###
(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ #scaling?
(venv) (base) ciaran@pg3:/external_storage/ciaran/machine_learning2$ script[K[K[K[K[K[Kexit script
exit
bash: exit: script: numeric argument required

Script done on Mon 20 Dec 2021 13:58:41 GMT
